[
  {
    "year": "2013",
    "abstract": "In an aircraft electric power system, one or more supervisory control units actuate a set of electromechanical switches to dynamically distribute power from generators to loads, while satisfying safety, reliability, and real-time performance requirements. To reduce expensive redesign steps, this control problem is generally addressed by minor incremental changes on top of consolidated solutions. A more systematic approach is hindered by a lack of rigorous design methodologies that allow estimating the impact of earlier design decisions on the final implementation. To achieve an optimal implementation that satisfies a set of requirements, we propose a platform-based methodology for electric power system design, which enables independent implementation of system topology (i.e., interconnection among elements) and control protocol by using a compositional approach. In our flow, design space exploration is carried out as a sequence of refinement steps from the initial specification toward a final implementation by mapping higher level behavioral and performance models into a set of either existing or virtual library components at the lower level of abstraction. Specifications are first expressed using the formalisms of linear temporal logic, signal temporal logic, and arithmetic constraints on Boolean variables. To reason about different requirements, we use specialized analysis and synthesis frameworks and formulate assume guarantee contracts at the articulation points in the design flow. We show the effectiveness of our approach on a proof-of-concept electric power system design."
  },
  {
    "year": "2013",
    "abstract": "One of the main features of adaptive systems is an oscillatory convergence that exacerbates with the speed of adaptation. Recently, it has been shown that closed-loop reference models (CRMs) can result in improved transient performance over their open-loop counterparts in model reference adaptive control. In this paper, we quantify both the transient performance in the classical adaptive systems and their improvement with CRMs. In addition to deriving bounds on L-2 norms of the derivatives of the adaptive parameters that are shown to be smaller, an optimal design of CRMs is proposed that minimizes an underlying peaking phenomenon. The analytical tools proposed are shown to be applicable for a range of adaptive control problems including direct control and composite control with observer feedback. The presence of CRMs in adaptive backstepping and adaptive robot control is also discussed. Simulation results are presented throughout this paper to support the theoretical derivations."
  },
  {
    "year": "2013",
    "abstract": "This paper explores the effective temporal surface-illuminated properties of two-component composites consisting of inclusions of regularly-and irregularly-shaped crystals in a matrix. Time-domain electromagnetic modeling using the finite integration technique is used to calculate scattering (S-) parameters, and from these, the effective relative permittivities are calculated. It is shown that the orientation of inclusions with high permittivity contrast affects the effective electrical permittivity of a composite mixture. For both low and high contrast inclusions, fields localize at edges and corners of the irregular inclusion in a manner not dependent on boundary conditions used in simulation."
  },
  {
    "year": "2013",
    "abstract": "The multilevel generalized assignment problem (MGAP) consists of minimizing the assignment cost of a set of jobs to machines, each having associated therewith a capacity constraint. Each machine can perform a job with different efficiency levels that entail different costs and amount of resources required. The MGAP was introduced in the context of large manufacturing systems as a more general variant of the well-known generalized assignment problem, where a single efficiency level is associated with each machine. In this paper, we propose a branch-and-cut algorithm whose core is an exact separation procedure for the multiple-choice knapsack polytope induced by the capacity constraints and single-level execution constraints. A computational experience on a set of benchmark instances is reported, showing the effectiveness of the proposed approach."
  },
  {
    "year": "2013",
    "abstract": "The Kinect system is arguably the most popular 3-D camera technology currently on the market. Its application domain is vast and has been deployed in scenarios where accurate geometric measurements are needed. Regarding the PrimeSense technology, a limited amount of work has been devoted to calibrating the Kinect, especially the depth data. The Kinect is, however, inevitably prone to distortions, as independently confirmed by numerous users. An effective method for improving the quality of the Kinect system is by modeling the sensor's systematic errors using bundle adjustment. In this paper, a method for modeling the intrinsic and extrinsic parameters of the infrared and colour cameras, and more importantly the distortions in the depth image, is presented. Through an integrated marker-and feature-based self-calibration, two Kinects were calibrated. A novel approach for modeling the depth systematic errors as a function of lens distortion and relative orientation parameters is shown to be effective. The results show improvements in geometric accuracy up to 53% compared with uncalibrated point clouds captured using the popular software RGBDemo. Systematic depth discontinuities were also reduced and in the check-plane analysis the noise of the Kinect point cloud was reduced by 17%."
  },
  {
    "year": "2013",
    "abstract": "Biometric template aging is defined as an increase in recognition error rate with increased time since enrollment. It is believed that template aging does not occur for iris recognition. Several research groups, however, have recently reported experimental results showing that iris template aging does occur. This template aging effect manifests as a shift in the authentic distribution, resulting in an increased false non-match rate. Analyzing results from a three-year time-lapse data set, we find∼150%increase in the false non-match rate at a decision threshold representing a one in two million false match rate. We summarize several known elements of eye aging that could contribute to template aging, including age-related change in pupil dilation. Finally, we discuss various steps that can control the template aging effect in typical identity verification applications."
  },
  {
    "year": "2013",
    "abstract": "Presents the associate editors for IEEE Access."
  },
  {
    "year": "2013",
    "abstract": "In this paper, a new approach toward the design of a memristor based nonvolatile static random-access memory (SRAM) cell using a combination of memristor and metal-oxide semiconductor devices is proposed. Memristor and MOSFETs of the Taiwan Semiconductor Manufacturing Company's 180-nm technology are used to form a single cell. The predicted area of this cell is significantly less and the average read–write power is∼25timesless than a conventional 6-T SRAM cell of the same complementary metal-oxide semiconductor technology. Read time is much less than the 6-T SRAM cell. However, write time is a bit higher, and can be improved by increasing the mobility of the memristor. The nonvolatile characteristic of the cell makes it attractive for nonvolatile random access memory design."
  },
  {
    "year": "2013",
    "abstract": "This paper suggests a real-valued sparse representation method using a unitary transformation that can convert complex-valued manifold matrices from uniform circular array into real ones. Because of this transformation, the computational complexity is modified. Simulation results confirmed the effectiveness of the proposed method with a circular array radar."
  },
  {
    "year": "2013",
    "abstract": "The influences of the intermediate band (IB) filling, the absorption coefficient constants, and the IB position on the efficiency of a quantum dot intermediate band solar cell (QD-IBSC) are investigated considering the spatial variation of subbandgap generation rates. A new definition of optimal intermediate band filling is proposed. A mathematical model is developed to optimize the intermediate band solar cell (IBSC) structure under idealized conditions, which calculates the optimal ratio of the subbandgap absorption coefficient constants and the optimal position of IB."
  },
  {
    "year": "2013",
    "abstract": "In this paper, a new ordinary differential equation numerical integration method is successfully applied to various mathematical branches such as partial differential equation (PDE) boundary problems, PDE initial-boundary problems, tough nonlinear equations, and so forth. The new method does not use Jacobian, so it can handle very large systems, say the dimension N=1 000 000, or even larger. In addition, we give a very simple accelerating convergence approach for the linear algebraic equations arising from linear PDE boundary problems. All the numerical results show that the new method is very promising for super large scale systems."
  },
  {
    "year": "2013",
    "abstract": "The recently proposed dual mode logic (DML) gates family enables a very high level of energy-delay optimization flexibility at the gate level. In this paper, this flexibility is utilized to improve energy efficiency and performance of combinatorial circuits by manipulating their critical and noncritical paths. An approach that locates the design's critical paths and operates these paths in the boosted performance mode is proposed. The noncritical paths are operated in the low energy DML mode, which does not affect the performance of the design, but allows significant energy consumption reduction. The proposed approach is analyzed on a 128 bit carry skip adder. Simulations, carried out in a standard 40 nm digital CMOS process with , show that the proposed approach allows performance improvement of X2 along with reduction of energy consumption of X2.5, as compared with a standard CMOS implementation. At , improvements of 1.3X and 1.5X in performance and energy are achieved, respectively."
  },
  {
    "year": "2013",
    "abstract": "As data sets continue to grow in size, visualization has become a vitally important tool for extracting meaningful knowledge. Scattered point data, which are unordered sets of point coordinates with associated measured values, arise in many contexts, such as scientific experiments, sensor networks, and numerical simulations. In this paper, we present a method for visualizing such scattered point data sets. Our method is based on volume ray casting, and distinguishes itself by operating directly on the unstructured samples, rather than resampling them to form voxels. We estimate the intensity of the volume at points along the rays by interpolation using nearby samples, taking advantage of an octree to facilitate efficient range search. The method has been implemented on multi-core CPUs, GPUs as well as multi-GPU systems. Our source code is available under a BSD license athttps://github.com/acelster/scatter-pt-vizTo test our method, actual X-ray diffraction data sets have been used, consisting of up to 240 million data points. We are able to generate images of good quality and achieve interactive frame rates in favorable cases. The GPU implementation (Nvidia Tesla K20) achieves speedups of 8–14 compared with our parallelized CPU version (4-core, hyperthreaded Intel i7 3770 K)."
  },
  {
    "year": "2013",
    "abstract": "Two turn-key surface potential-based compact models are developed to simulate multigate transistors for integrated circuit (IC) designs. The BSIM-CMG (common-multigate) model is developed to simulate double-, triple-, and all-around-gate FinFETs and it is selected as the world's first industry-standard compact model for the FinFET. The BSIM-IMG (independent-multigate) model is developed for independent double-gate, ultrathin body (UTB) transistors, capturing the dynamic threshold voltage adjustment with back gate bias. Starting from long-channel devices, the basic models are first obtained using a Poisson-carrier transport approach. The basic models agree with the results of numerical two-dimensional device simulators. The real-device effects then augment the basic models. All the important real-device effects, such as short-channel effects (SCEs), quantum mechanical confinement effects, mobility degradation, and parasitics are included in the models. BSIM-CMG and BSIM-IMG have been validated with hardware silicon-based data from multiple technologies. The developed models also meet the stringent quality assurance tests expected of production level models."
  },
  {
    "year": "2013",
    "abstract": "The manual creation of complex 3D structures for use in engineering analysis is a major obstacle to analyzing physically realistic structures. A bias is invariably imposed when a mixture is manually composed, and the structure is rarely representative of the process by which composites are fabricated. Properties such as packing density and anisotropies that seem to easily occur in nature are very difficult to obtain with manual arrangements. This paper addresses the creation of complex 3D mixtures, comprising crystals embedded in a matrix, for subsequent electromagnetic (EM) analysis. The physically realistic arrangement of the crystals is facilitated by the use of physics engine software, specifically the Bullet physics library, which renders the realistic effects in advanced computer games. A composite mixture of crystals is created by pouring a series of random crystals into a box with the crystals bouncing against each other and aligning just as they do in the real world. Higher packing densities are obtained than can be reasonably obtained with manual construction. The arrangement of the obtained crystals reflects the real world alignment of asymmetric crystals. A composite is created here and used with EM simulation software to investigate energy localization in materials."
  },
  {
    "year": "2013",
    "abstract": "A comprehensive equation for natural and physical systems including electric and magnetic, mass, and wave interactions has long been an objective of engineers and scientists. Dynamic energy appears to be one possible technique. A method of mathematics is used with curved space vectors which greatly reduces the calculus. The vector relationship between cyclic time and reference time is identified. A diffusion relationship is developed which has the potential for describing the Higgs field. The diffusion is defined in terms of a volume gradient varying with reference time. By judicious definition of dynamic energy, the diffusion is also a direct relationship of electrical charge and magnetism with waves or photons, and mass."
  },
  {
    "year": "2013",
    "abstract": "In software development, stakeholders of the same project often collaborate asynchronously through shared artifacts. A traceability system links a project's artifacts and therefore provides support for collaboration among stakeholders. Different stakeholders are interested in different types of traceability links. The literature often states that traceability is useful but expensive to build and maintain. However, there is no study showing reduction in effort when traceability links among various software artifacts are provided and used during the maintenance phase. This paper presents a study evaluating the benefits of using traceability among requirements, design, code, code inspections, builds, defects, and tests artifacts in the maintenance phase. Before the study, a survey was conducted at a large industrial firm to determine the type of links that different stakeholders are interested in. Twenty-five stakeholders from this firm participated in a survey to define the type of traceability links that were of interest to them. With this result, a traceability link model is proposed that categorizes different types of traceability links based on stakeholders' roles. This link model was used in the study. Twenty-eight subjects from industry and academia participated in the empirical study that was conducted after the survey. A prototype link-tracing tool, TraceLink, was developed and used in the study to present traceability links to the experimental group, whereas the control group was not given any links to solve the tasks. Five maintenance tasks were used in the study. The results show a significant improvement in task accuracy (86.06%) when traceability links were given to the subjects. In conclusion, a traceability model based on an industrial survey provided traceability views that are based on stakeholders' roles. This empirical study provides evidence that traceability links are effective in solving maintenance tasks with higher accuracy."
  },
  {
    "year": "2013",
    "abstract": "This paper is concerned with modeling of networks with an extremely large number of components using partial differential equations (PDEs). This modeling method is based on the convergence of a sequence of underlying Markov chains of the network indexed byN, the number of components in the network. AsNgoes to infinity, the sequence converges to a continuum limit, which is the solution of a certain PDE. We provide sufficient conditions for the convergence and characterize the rate of convergence. As an application, we model large wireless sensor networks by PDEs. While traditional Monte Carlo simulation for extremely large networks is practically infeasible, PDEs can be solved with reasonable computation overhead using well-established mathematical tools."
  },
  {
    "year": "2013",
    "abstract": "This paper deals with the design and evaluation of novel dynamic random access memory (DRAM) cells that have an oxide-based resistive element added for non-volatile operation. Two existing DRAM cells (namely the 3T1D and B3T cells) are utilized as volatile cores; a RRAM circuitry (consisting of an access control transistor and an oxide resistive RAM) is added to the core to extend its operation for non-volatile operation; two NVDRAM cells are then proposed. Considerations, such as the threshold voltage for the refresh operation and output read circuitry, are also considered. The impacts of the non-volatile circuitry as well as the DRAM core selection are assessed by HSPICE simulation. Figures of merit as related to performance, process variability, power consumption, and circuit design (critical charge and area of cell layout) are established for both volatile and non-volatile DRAM cells as well as memory arrays."
  },
  {
    "year": "2013",
    "abstract": "A better understanding of the degradation modes and rates for photovoltaic (PV) modules is necessary to optimize and extend the lifetime of these modules. Lifetime and degradation science (L&DS) is used to understand degradation modes, mechanisms and rates of materials, components and systems to predict lifetime of PV modules. A PV module lifetime and degradation science (PVM L&DS) model is an essential component to predict lifetime and mitigate degradation of PV modules using reproducible open data science. Previously published accelerated testing data from Underwriter Laboratories on PV modules with fluorinated polyester backsheets, which included eight modules that were exposed up to 4000 hrs of damp heat (85% relative humidity at 85∘C) and eight exposed up to 4000 hrs of ultraviolet light (80W/m2of 280–400 nm wavelengths at 60∘C) (UV preconditioning) were used to determine statistically significant relationships between the applied stresses and measured responses. There were 15 different variables tracking aspects of system performance, degradation mechanisms, component metrics and time. Modules were analyzed for three system performance metrics (fill factor, peak power, and wet insulation). The results were statistically analyzed to identify variable transformations, statistically significant relationships (SSRs) and to develop the PVM L&DS model informed by a generalization of structural equation modeling techniques. The SSRs and significant model coefficients, combined with domain analytics, incorporating materials science, chemistry, and physics expertise, produced a pathway diagram ranking the variables' impact on the system performance, which were iteratively examined using sound statistical analysis and diagnostics. The SSRs determined from the damp heat exposure for the system response of Pmax corresponded to the degradation pathway of polyester terephthalate (PET) and ethylene vinyl acetate (EVA) hydroly..."
  },
  {
    "year": "2013",
    "abstract": "Multi-objective robot exploration constitutes one of the most challenging tasks for autonomous robots performing in various operations and different environments. However, the optimal exploration path depends heavily on the objectives and constraints that both these operations and environments introduce. Typical environment constraints include partially known or completely unknown workspaces, limited-bandwidth communications, and sparse or dense clattered spaces. In such environments, the exploration robots must satisfy additional operational constraints, including time-critical goals, kinematic modeling, and resource limitations. Finding the optimal exploration path under these multiple constraints and objectives constitutes a challenging non-convex optimization problem. In our approach, we model the environment constraints in cost functions and utilize the cognitive-based adaptive optimization algorithm to meet time-critical objectives. The exploration path produced is optimal in the sense of globally minimizing the required time as well as maximizing the explored area of a partially unknown workspace. Since obstacles are sensed during operation, initial paths are possible to be blocked leading to a robot entrapment. A supervisor is triggered to signal a blocked passage and subsequently escape from the basin of cost function local minimum. Extensive simulations and comparisons in typical scenarios are presented to show the efficiency of the proposed approach."
  },
  {
    "year": "2013",
    "abstract": "We present DooDB, a doodle database containing data from 100 users captured with a touch screen-enabled mobile device under realistic conditions following a systematic protocol. The database contains two corpora: 1) doodles and 2) pseudo-signatures, which are simplified finger-drawn versions of the handwritten signature. The dataset includes genuine samples and forgeries, produced under worst-case conditions, where attackers have visual access to the drawing process. Statistical and qualitative analyzes of the data are presented, comparing doodles and pseudo-signatures to handwritten signatures. Time variability, learning curves, and discriminative power of different features are also studied. Verification performance against forgeries is analyzed using state-of-the-art algorithms and benchmark results are provided."
  },
  {
    "year": "2013",
    "abstract": "A nanoslit is capable of performing versatile functions in nanophotonic applications. In this invited paper, we discuss the physics and manipulation methods of surface plasmon excitation such as directional switching at a nanoslit. Furthermore, enhancing the light intensity passing through a nanoslit by employing embedded metallic nanoislands is experimentally presented and numerically analyzed."
  },
  {
    "year": "2013",
    "abstract": "Enhancements of the directivity and front-to-back ratio (FTBR) of a metamaterial-inspired electrically small, linearly polarized, coaxially-fed Egyptian axe dipole antenna are considered. They are accomplished with a particular augmentation of the original near-field-resonant-parasitic (NFRP) antenna with an additional NFRP element, a small disc conductor modified with two meanderline-shaped slots. The entire system is evaluated numerically with two independent computational electromagnetics simulators. The optimized results demonstrate an electrically small antenna (i.e., ka <; 1.0) with a reasonably low profile (i.e., height ~ λ/10) that improves the directivity from 1.77 to 6.32 dB, increases the FTBR from 0 to > 20 dB, and maintains large half-power beamwidths, while having a radiation efficiency over 80% with nearly complete matching to a 50 Ω source."
  },
  {
    "year": "2013",
    "abstract": "In open and dynamic multiagent systems (MASs), agents often need to rely on resources or services provided by other agents to accomplish their goals. During this process, agents are exposed to the risk of being exploited by others. These risks, if not mitigated, can cause serious breakdowns in the operation of MASs and threaten their long-term wellbeing. To protect agents from the uncertainty in the behavior of their interaction partners, the age-old mechanism of trust between human beings is re-contexted into MASs. The basic idea is to let agents self-police the MAS by rating each other on the basis of their observed behavior and basing future interaction decisions on such information. Over the past decade, a large number of trust management models were proposed. However, there is a lack of research effort in several key areas, which are critical to the success of trust management in MASs where human beings and agents coexist. The purpose of this paper is to give an overview of existing research in trust management in MASs. We analyze existing trust models from a game theoretic perspective to highlight the special implications of including human beings in an MAS, and propose a possible research agenda to advance the state of the art in this field."
  },
  {
    "year": "2013",
    "abstract": "Important progress has been made by many researchers in extracting fundamental design principles from patterns in design parameters along the nondominated front generated by evolutionary algorithms in biobjective optimization problems. However, to the best of our knowledge, no attention has been given to discovering design principles from the wealth of additional information available from patterns in dominated solutions. To explore the same, we use heatmaps of dominated solutions to visualize how relevant variables self-organize with respect to the objectives throughout the feasible region. We overlay ceteris paribus lines on these heatmaps to show how the objective values change when a given design variable is varied while all others are held constant. We use three biobjective optimization problems to demonstrate various ways in which these visualization techniques can provide additional useful information beyond that which can be determined from the nondominated front. Specifically, we investigate a simple two-member truss design problem, a simple welded beam design problem, and a real-world watershed management design problem to illustrate: 1) how principles derived from the nondominated front alone can be misleading; 2) how new principles can be derived from the dominated solutions; and 3) how nondominated solutions can often be fragile with respect to assumptions about uncertain external forcing conditions, whereas solutions a short distance inside the front are often much more robust."
  },
  {
    "year": "2013",
    "abstract": "The high complexity of numerous optimal classic communication schemes, such as the maximum likelihood (ML) multiuser detector (MUD), often prevents their practical implementation. In this paper, we present an extensive review and tutorial on quantum search algorithms (QSA) and their potential applications, and we employ a QSA that finds the minimum of a function in order to perform optimal hard MUD with a quadratic reduction in the computational complexity when compared to that of the ML MUD. Furthermore, we follow a quantum approach to achieve the same performance as the optimal soft-input soft-output classic detectors by replacing them with a quantum algorithm, which estimates the weighted sum of a function's evaluations. We propose a soft-input soft-output quantum-assisted MUD (QMUD) scheme, which is the quantum-domain equivalent of the ML MUD. We then demonstrate its application using the design example of a direct-sequence code division multiple access system employing bit-interleaved coded modulation relying on iterative decoding, and compare it with the optimal ML MUD in terms of its performance and complexity. Both our extrinsic information transfer charts and bit error ratio curves show that the performance of the proposed QMUD and that of the optimal classic MUD are equivalent, but the QMUD's computational complexity is significantly lower."
  },
  {
    "year": "2013",
    "abstract": "Presents the editorial board for IEEE Access."
  },
  {
    "year": "2013",
    "abstract": "The adaptive Runge–Kutta (ARK) method on multi-general-purpose graphical processing units (GPUs) is used for solving large nonlinear systems of first-order ordinary differential equations (ODEs) with over∼10000variables describing a large genetic network in systems biology for the biological clock. To carry out the computation of the trajectory of the system, a hierarchical structure of the ODEs is exploited, and an ARK solver is implemented in compute unified device architecture/C++ (CUDA/C++) on GPUs. The result is a 75-fold speedup for calculations of 2436 independent modules within the genetic network describing clock function relative to a comparable CPU architecture. These 2436 modules span one-quarter of the entire genome of a model fungal system, Neurospora crassa. The power of a GPU can in principle be harnessed by using warp-level parallelism, instruction level parallelism or both of them. Since the ARK ODE solver is entirely sequential, we propose a new parallel processing algorithm using warp-level parallelism for solving∼10000ODEs that belong to a large genetic network describing clock genome-level dynamics. A video is attached illustrating the general idea of the method on GPUs that can be used to provide new insights into the biological clock through single cell measurements on the clock."
  },
  {
    "year": "2013",
    "abstract": "The global bandwidth shortage facing wireless carriers has motivated the exploration of the underutilized millimeter wave (mm-wave) frequency spectrum for future broadband cellular communication networks. There is, however, little knowledge about cellular mm-wave propagation in densely populated indoor and outdoor environments. Obtaining this information is vital for the design and operation of future fifth generation cellular networks that use the mm-wave spectrum. In this paper, we present the motivation for new mm-wave cellular systems, methodology, and hardware for measurements and offer a variety of measurement results that show 28 and 38 GHz frequencies can be used when employing steerable directional antennas at base stations and mobile devices."
  },
  {
    "year": "2013",
    "abstract": "In organized healthcare quality improvement collaboratives (QICs), teams of practitioners from different hospitals exchange information on clinical practices with the aim of improving health outcomes at their own institutions. However, what works in one hospital may not work in others with different local contexts because of nonlinear interactions among various demographics, treatments, and practices. In previous studies of collaborations where the goal is a collective problem solving, teams of diverse individuals have been shown to outperform teams of similar individuals. However, when the purpose of collaboration is knowledge diffusion in complex environments, it is not clear whether team diversity will help or hinder effective learning. In this paper, we first use an agent-based model of QICs to show that teams comprising similar individuals outperform those with more diverse individuals under nearly all conditions, and that this advantage increases with the complexity of the landscape and level of noise in assessing performance. Examination of data from a network of real hospitals provides encouraging evidence of a high degree of similarity in clinical practices, especially within teams of hospitals engaging in QIC teams. However, our model also suggests that groups of similar hospitals could benefit from larger teams and more open sharing of details on clinical outcomes than is currently the norm. To facilitate this, we propose a secure virtual collaboration system that would allow hospitals to efficiently identify potentially better practices in use at other institutions similar to theirs without any institutions having to sacrifice the privacy of their own data. Our results may also have implications for other types of data-driven diffusive learning such as in personalized medicine and evolutionary search in noisy, complex combinatorial optimization problems."
  },
  {
    "year": "2013",
    "abstract": "This paper provides an overview of the main features of several bibliometric indicators which were proposed in the last few decades. Their pros and cons are highlighted and compared with the features of the well-known impact factor (IF) to show how alternative metrics are specifically designed to address the flaws that the IF was shown to have, especially in the last few years. We also report the results of recent studies in the bibliometric literature showing how the scientific impact of journals as evaluated by bibliometrics is a very complicated matter and it is completely unrealistic to try to capture it by any single indicator, such as the IF or any other. As such, we conclude that the adoption of more metrics, with complementary features, to assess journal quality would be very beneficial as it would both offer a more comprehensive and balanced view of each journal in the space of scholarly publications, as well as eliminate the pressure on individuals and their incentive to do metric manipulation which is an unintended result of the current (mis)use of the IF as the gold standard for publication quality."
  },
  {
    "year": "2013",
    "abstract": "Incomplete specifications on new products are very common resulting in unreliable products and costly scope creeps and design changes. Designing out failures using lessons learned can result in very high reliability and significantly lower warranty costs. The quickest way to make use of the lessons learned is to learn from heuristics which are short statements based on wisdom from lessons learned over many products. The article covers examples to show how the heuristics were applied to successful products."
  },
  {
    "year": "2013",
    "abstract": "Presents an editorial for the initial publicaiton of IEEE Access, detailing the mission and scope of the publication."
  },
  {
    "year": "2013",
    "abstract": "Digital evidence is increasingly used in juridical proceedings. In some recent legal cases, the verdict has been strongly influenced by the digital evidence proffered by the defense. Digital traces can be left on computers, phones, digital cameras, and also on remote machines belonging to ISPs, telephone providers, companies that provide services via Internet such as YouTube, Facebook, Gmail, and so on. This paper presents a methodology for the automated production of predetermined digital evidence, which can be leveraged to forge a digital alibi. It is based on the use of an automation, a program meant to simulate any common user activity. In addition to wanted traces, the automation may produce a number of unwanted traces, which may be disclosed upon a digital forensic analysis. These include data remanence of suspicious files, as well as any kind of logs generated by the operating system modules and services. The proposed methodology describes a process to design, implement, and execute the automation on a target system, and to properly handle both wanted and unwanted evidence. Many experiments with different combinations of automation tools and operating systems are conducted. This paper presents an implementation of the methodology through VBScript on Windows 7. A forensic analysis on the target system is not sufficient to reveal that the alibi is forged by automation. These considerations emphasize the difference between digital and traditional evidence. Digital evidence is always circumstantial, and therefore it should be considered relevant only if supported by stronger evidence collected through traditional investigation techniques. Thus, a Court verdict should not be based solely on digital evidence."
  },
  {
    "year": "2013",
    "abstract": "This paper provides the design and implementation of anL1-optimal control of a quadrotor unmanned aerial vehicle (UAV). The quadrotor UAV is an underactuated rigid body with four propellers that generate forces along the rotor axes. These four forces are used to achieve asymptotic tracking of four outputs, namely the position of the center of mass of the UAV and the heading. With perfect knowledge of plant parameters and no measurement noise, the magnitudes of the errors are shown to exponentially converge to zero. In the case of parametric uncertainty and measurement noise, the controller yields an exponential decrease of the magnitude of the errors in anL1-optimal sense. In other words, the controller is designed so that it minimizes theL∞-gain of the plant with respect to disturbances. The performance of the controller is evaluated in experiments and compared with that of a related robust nonlinear controller in the literature. The experimental data shows that the proposed controller rejects persistent disturbances, which is quantified by a very small magnitude of the mean error."
  },
  {
    "year": "2013",
    "abstract": "Mixed models provide a novel approach to the analysis of radar tracking residuals by considering randomness from different sources. Through properly considering randomness, mixed models can provide greater power to determine the statistical significance of various parameters needed for radar calibration. This paper applies a mixed models approach to the analysis of radar tracking residuals from calibration satellites observed by the Cobra Dane radar and finds a time dependent bias in the azimuth residuals."
  },
  {
    "year": "2013",
    "abstract": "Cellular networks are currently experiencing a tremendous growth of data traffic. To cope with this demand, a close cooperation between academic researchers and industry/standardization experts is necessary, which hardly exists in practice. In this paper, we try to bridge this gap between researchers and engineers by providing a review of current standard-related research efforts in wireless communication systems. Furthermore, we give an overview about our attempt in facilitating the exchange of information and results between researchers and engineers, via a common simulation platform for 3GPP long term evolution (LTE) and a corresponding webforum for discussion. Often, especially in signal processing, reproducing results of other researcher is a tedious task, because assumptions and parameters are not clearly specified, which hamper the consideration of the state-of-the-art research in the standardization process. Also, practical constraints, impairments imposed by technological restrictions and well-known physical phenomena, e.g., signaling overhead, synchronization issues, channel fading, are often disregarded by researchers, because of simplicity and mathematical tractability. Hence, evaluating the relevance of research results under practical conditions is often difficult. To circumvent these problems, we developed a standard-compliant open-source simulation platform for LTE that enables reproducible research in a well-defined environment. We demonstrate that innovative research under the confined framework of a real-world standard is possible, sometimes even encouraged. With examples of our research work, we investigate on the potential of several important research areas under typical practical conditions, and highlight consistencies as well as differences between theory and practice."
  },
  {
    "year": "2013",
    "abstract": "Many operations, be they military, police, rescue, or other field operations, require localization services and online situation awareness to make them effective. Questions such as how many people are inside a building and their locations are essential. In this paper, an online localization and situation awareness system is presented, called Mobile Urban Situation Awareness System (MUSAS), for gathering and maintaining localization information, to form a common operational picture. The MUSAS provides multiple localization services, as well as visualization of other sensor data, in a common frame of reference. The information and common operational picture of the system is conveyed to all parties involved in the operation, the field team, and people in the command post. In this paper, a general system architecture for enabling localization based situation awareness is designed and the MUSAS system solution is presented. The developed subsystem components and forming of the common operational picture are summarized, and the future potential of the system for various scenarios is discussed. In the demonstration, the MUSAS is deployed to an unknown building, in an ad hoc fashion, to provide situation awareness in an urban indoor military operation."
  },
  {
    "year": "2013",
    "abstract": "Exploiting the parallelism in multiprocessor systems is a major challenge in modern computer science. Multicore programming demands a change in the way we design and use fundamental data structures. The standard collection of data structures and algorithms in C++11 is the sequential standard template library (STL). In this paper, we present their vision for the theory and practice for the design and implementation of a collection of highly concurrent fundamental data structures for multiprocessor application development with associated programming interface and advanced optimization support. Specifically, the proposed approach will provide a familiar, easy-to-use, and composable interface, similar to that of C++ STL. Each container type will be enhanced with internal support for nonblocking synchronization of its data access, thereby providing better safety and performance than traditional blocking synchronization by: 1) eliminating hazards such as deadlock, livelock, and priority inversion and 2) by being highly scalable in supporting large numbers of threads. The new library, lockless containers/data concurrency, will provide algorithms for handling fundamental computations in multithreaded contexts, and will incorporate these into libraries with familiar look and feel. The proposed approach will provide an immense boost in performance and software reuse, consequently productivity, for developers of scientific and systems applications, which are predominantly in C/C++. STL is widely used and a concurrent replacement library will have an immediate practical relevance and a significant impact on a variety of parallel programming domains including simulation, massive data mining, computational biology, financial engineering, and embedded control systems. As a proof-of-concept, this paper discusses the first design and implementation of a wait-free hash table."
  },
  {
    "year": "2013",
    "abstract": "This paper reports the latest technological advances made by the Industrial Technology Research Institute (ITRI) in flexible displays, especially the flexible substrate, thin-film transistor (TFT) backplane, and active matrix organic light-emitting diode display. Using the leading cholesteric liquid crystal technology of ITRI, we develop a rewritable, environmentally friendly thermal printable e-paper. The e-paper, devised to reduce traditional paper consumption, achieves a high resolution of 300 dpi with a memory function. In addition, we report on the ITRI's initial success in demonstrating a complete R2R process for multisensing touch panels on 100-μmthick flexible glass substrates provided by Corning."
  },
  {
    "year": "2013",
    "abstract": "The signal processing concept of signal-to-noise ratio (SNR), in its role as a performance measure, is recast within the more general context of information theory, leading to a series of useful insights. Establishing generalized SNR (GSNR) as a rigorous information theoretic measure inherent in any set of observations significantly strengthens its quantitative performance pedigree while simultaneously providing a specific definition under general conditions. In turn, this directly leads to consideration of the log likelihood ratio (LLR): first, as the simplest possible information-preserving transformation (i.e., signal processing algorithm) and subsequently, as an absolute, comparable measure of information for any specific observation exemplar. The information accounting methodology that results permits practical use of both GSNR and LLR as diagnostic scalar performance measurements, directly comparable across alternative system/algorithm designs, applicable at any tap point within any processing string, in a form that is also comparable with the inherent performance bounds due to information conservation."
  },
  {
    "year": "2013",
    "abstract": "A team of robots are deployed to accomplish a task while maintaining a viable ad-hoc network capable of supporting data transmissions necessary for task fulfillment. Solving this problem necessitates: 1) estimation of the wireless propagation environment to identify viable point-to-point communication links; 2) determination of end-to-end routes to support data traffic; and 3) motion control algorithms to navigate through spatial configurations that guarantee required minimum levels of service. Therefore, we present methods for: 1) estimation of point-to-point channels using pathloss and spatial Gaussian process models; 2) data routing so as to determine suitable end-to-end communication routes given estimates of point-to-point channel rates; and 3) motion planning to determine robot trajectories restricted to configurations that ensure survival of the communication network. Because of the inherent uncertainty of wireless channels, the model of links and routes is stochastic. The criteria for route selection is to maximize the probability of network survival—defined as the ability to support target communication rates—given achievable rates on local point-to-point links. Maximum survival probability routes for present and future positions are input into a mobility control module that determines robot trajectories restricted to configurations that ensure the probability of network survival stays above a minimum reliability level. Local trajectory planning is proposed for simple environments and global planning is proposed for complex surroundings. The three proposed components are integrated and tested in experiments run in two different environments. Experimental results show successful navigation with continuous end-to-end connectivity."
  },
  {
    "year": "2013",
    "abstract": "In this paper, we propose an efficient distortion-based privacy-preserving metering scheme that protects an individual customer's privacy and provides the complete power consumption distribution curve of a multitude of customers without privacy invasion. In the proposed scheme, a random noise is purposely introduced to distort customers' power consumption data at the smart meter so that data recovery becomes infeasible. Using the power consumption data and prior knowledge about added random noise, we develop an efficient algorithm for power consumption distribution reconstruction needed for power demand analysis and prediction. As a complete solution, our scheme also supports a privacy-preserving billing service. Using experimental results from real world single household power consumption data set and synthesized data of a large number of households, we demonstrate that the proposed scheme is robust against known attacks. Since it does not demand new facilities on existing smart grids, the proposed scheme offers a practical solution."
  },
  {
    "year": "2013",
    "abstract": "A 256 Gb NAND flash memory multi-chip package (MCP) includes eight stacked 32 Gb 2 bit/cell multi-level cell (MLC) die and an 11.6 mm2HyperLink NAND bridge chip providing four internal NAND channels for concurrent memory operations. The bridge chip provides an external 1.2 V unidirectional byte-wide point-to-point source-synchronous double data-rate (DDR) interface for low power 800 MB/s operation in a ring topology. Interface power is reduced by shutting down the phase-locked loop in every second MCP and alternating between edge aligned DDR clock and center aligned DDR clock for source-synchronous data transfer from MCP to MCP."
  },
  {
    "year": "2013",
    "abstract": "Taiwan's semiconductor industry has had a profound influence on both domestic economics and the global IT industry. To enhance its competitiveness and expand its global impact, the Taiwan government commenced the National Program for Intelligent Electronics (NPIE) in 2011 to promote technological innovation of medical electronics, green electronics, vehicular (car) electronics, conventional computer, communication, and consumer electronics, the so-called “MG+4C” applications. The government allocated a budget of $430 million over 5 years to facilitate technology development, advanced research, talent cultivation, industry promotion, and international collaboration. By coordinating with government agencies, research institutions, universities, and corporations, the NPIE develops an integrated framework of academia and industry in Taiwan. It also emphasizes the enhancement of product verification, regulations, and participation in international alliances and standards. By transferring academic results to industry in emerging MG+4C applications, it is expected that the NPIE will contribute to the global semiconductor community, energize Taiwan's IC industry, and create innovative products and intelligent systems for better life and better environment."
  },
  {
    "year": "2013",
    "abstract": "Decades of heavy investment in laboratory-based brain imaging and neuroscience have led to foundational insights into how humans sense, perceive, and interact with the external world. However, it is argued that fundamental differences between laboratory-based and naturalistic human behavior may exist. Thus, it remains unclear how well the current knowledge of human brain function translates into the highly dynamic real world. While some demonstrated successes in real-world neurotechnologies are observed, particularly in the area of brain-computer interaction technologies, innovations and developments to date are limited to a small science and technology community. We posit that advancements in real-world neuroimaging tools for use by a broad-based workforce will dramatically enhance neurotechnology applications that have the potential to radically alter human–system interactions across all aspects of everyday life. We discuss the efforts of a joint government-academic-industry team to take an integrative, interdisciplinary, and multi-aspect approach to translate current technologies into devices that are truly fieldable across a range of environments. Results from initial work, described here, show promise for dramatic advances in the field that will rapidly enhance our ability to assess brain activity in real-world scenarios."
  },
  {
    "year": "2013",
    "abstract": "A wireless video surveillance system consists of three major components: 1) the video capture and preprocessing; 2) the video compression and transmission in wireless sensor networks; and 3) the video analysis at the receiving end. A myriad of research works have been dedicated to this field due to its increasing popularity in surveillance applications. This survey provides a comprehensive overview of existing state-of-the-art technologies developed for wireless video surveillance, based on the in-depth analysis of the requirements and challenges in current systems. Specifically, the physical network infrastructure for video transmission over wireless channel is analyzed. The representative technologies for video capture and preliminary vision tasks are summarized. For video compression and transmission over the wireless networks, the ultimate goal is to maximize the received video quality under the resource limitation. This is also the main focus of this survey. We classify different schemes into categories including unequal error protection, error resilience, scalable video coding, distributed video coding, and cross-layer control. Cross-layer control proves to be a desirable measure for system-level optimal resource allocation. At the receiver's end, the received video is further processed for higher-level vision tasks, and the security and privacy issues in surveillance applications are also discussed."
  },
  {
    "year": "2013",
    "abstract": "This paper describes a compact high-performance orthomode transducer (OMT) with a circular waveguide input and two rectangular waveguide outputs based on the superimposition of three aluminum blocks. Several prototypes operating in the band 1 (31-45 GHz) of the atacama large millimeter array have been fabricated and measured. The design is based on the use of a turnstile junction that is machined in a single block, requiring neither alignment nor a high degree of mechanical tolerances. Thus, a high repeatability of the design is possible for mass production. Across the 31-45 GHz band, the isolation is better than 50 dB and the return losses at the input and outputs of the OMT are better than -25 dB."
  },
  {
    "year": "2013",
    "abstract": "Distributed optical fiber sensors have gained an increasingly prominent role in structural-health monitoring. These are composed of an optical fiber cable in which a light impulse is launched by an opto-electronic device. The scattered light is of interest in the spectral domain: the spontaneous Brillouin spectrum is centered on the Brillouin frequency, which is related to the local strain and temperature changes in the optical fiber. When coupled with an industrial Brillouin optical time-domain analyzer (B-OTDA), an optical fiber cable can provide distributed measurements of strain and/or temperature, with a spatial resolution over kilometers of 40 cm. This paper focuses on the functioning of a B-OTDA device, where we address the problem of the improvement of spatial resolution. We model a Brillouin spectrum measured within an integration base of 1 m as the superposition of the elementary spectra contained in the base. Then, the spectral distortion phenomenon can be mathematically explained: if the strain is not constant within the integration base, the Brillouin spectrum is composed of several elementary spectra that are centered on different local Brillouin frequencies. We propose a source separation methodology approach to decompose a measured Brillouin spectrum into its spectral components. The local Brillouin frequencies and amplitudes are related to a portion of the integration base where the strain is constant. A layout algorithm allows the estimation of a strain profile with new spatial resolution chosen by the user. Numerical tests enable the finding of the optimal parameters, which provides a reduction to 1 cm of the 40-cm spatial resolution of the B-OTDA device. These parameters are highlighted during a comparison with a reference strain profile acquired by a 5-cm-resolution Rayleigh scatter analyzer under controlled conditions. In comparison with the B-OTDA strain profile, our estimated strain profile has better accuracy, with centimeter spatial resolut..."
  },
  {
    "year": "2013",
    "abstract": "This paper considers a group of drogues whose objective is to estimate the physical parameters that determine the dynamics of ocean nonlinear internal waves. Internal waves are important in oceanography because, as they travel, they are capable of displacing small animals, such as plankton, larva, and fish. These waves are described by models that employ trigonometric functions parameterized by a set of constants such as amplitude, wavenumber, and temporal frequency. While underwater, individual drogues do not have access to absolute position information and only rely on inter-drogue measurements. Building on this data and the study of the drogue dynamics under the flow induced by the internal wave, we design two strategies, referred to as the Vanishing Derivative Method and the Passing Wave Method, that are able to determine the wavenumber and the speed ratio. Either of these strategies can be employed in the Parameter Determination Strategy to determine all the remaining wave parameters. We analyze the correctness of the proposed strategies and discuss their robustness against different sources of error. Simulations illustrate the algorithm performance under noisy measurements as well as the effect of different initial drogue configurations."
  },
  {
    "year": "2013",
    "abstract": "This paper introduces self-manipulation as a new formal design methodology for legged robots with varying ground interactions. The term denotes a set of modeling choices that permit a uniform and body-centric representation of the equations of motion—essentially a guide to the selection and configuration of coordinate frames. We present the hybrid system kinematics, dynamics, and transitions in the form of a consistently structured representation that simplifies and unites the account of these, otherwise bewilderingly diverse differential algebraic equations. Cleaving as closely as possible to the modeling strategies developed within the mature manipulation literature, self-manipulation models can leverage those insights and results where applicable, while clarifying the fundamental differences. Our primary motivation is not to facilitate numerical simulation but rather to promote design insight. We instantiate the abstract formalism for a simplified model of RHex, and illustrate its utility by applying a variety of analytical and computational techniques to derive new results bearing on behaviors, controllers, and platform design. For each example, we present empirical results documenting the specific benefits of the new insight into the robot's transitions from standing to moving in place and to leaping."
  },
  {
    "year": "2013",
    "abstract": "Taiwan has been a worldwide leader in computer hardware manufacturing for many years. Cloud computing, a major paradigm shift for the IT industry, threatens that leadership position. Cloud computing drives the value chain increasingly toward systems software and services. It has a major negative impact on the already marginal profit picture that threatens Taiwan's hardware manufacturing industry, and the Taiwan economy as a whole. The Institute for Information Industry (III) has devised a strategy and technology called cloud appliances for enterprises (CAFÉ) to address the effects of this technology shift. CAFÉ is a major initiative to help traditional Taiwanese OEM/ODM vendors transform themselves from primarily being commodity hardware providers into highly profitable, full-function cloud systems business that can reach worldwide markets. The III develops key system software components (CAFÉ technology components) that can be integrated with partners' hardware and other technology to create appliances that deliver cloud infrastructure and services enabling enterprises to establish private clouds (thus the name CAFÉ). Appliances that offer user-ready services and applications, provide significantly higher value to customers, support higher prices than hardware alone, and thereby increase the profit margin of Taiwanese IT vendors. Key Taiwan hardware vendors that have already adapted the CAFÉ strategy include ASUS, Delta Electronics, GIGABYTE, In Win, and Promise Technology. They are developing, integrating, and shipping cloud appliances that incorporate CAFÉ technology components, as well as their own technology. The CAFÉ strategy has turned the cloud computing challenge into a golden opportunity, moving the IT industry of Taiwan toward the next horizon. Video-Summary and Testimonialshttp://www.youtube.com/watch?v=p4jqeuAwF00&feature=youtu.be."
  },
  {
    "year": "2013",
    "abstract": "Future and current high-performance computing applications will have to change and adapt as node architectures evolve. The application of advanced architecture simulators will play a crucial role for the design and optimization of future data intensive applications. In this paper, we present our simulation-based framework for analyzing the scalability and performance of a number of critical optimizations of a massively parallel genomic search application, mpiBLAST, using an advanced macroscale simulator (SST/macro). We report the use of our framework for the evaluation of three potential improvements of mpiBLAST: 1) enabling high-performance parallel output; 2) an approach for caching database fragments in memory; and 3) a methodology for pre-distributing database segments. In our experimental setup, we performed query sequence matching on the genome of the yellow fever mosquito, Aedes aegypti."
  },
  {
    "year": "2013",
    "abstract": "We consider the problem of adaptively designing compressive measurement matrices for estimating time-varying sparse signals. We formulate this problem as a partially observable Markov decision process. This formulation allows us to use Bellman's principle of optimality in the implementation of multi-step lookahead designs of compressive measurements. We compare the performance of adaptive versus traditional non-adaptive designs and study the value of multi-step (non-myopic) versus one-step (myopic) lookahead adaptive schemes by introducing two variations of the compressive measurement design problem. In the first variation, we consider the problem of sequentially selecting measurement matrices with fixed dimensions from a prespecified library of measurement matrices. In the second variation, the number of compressive measurements, i.e., the number of rows of the measurement matrix, is adaptively chosen. Once the number of measurements is determined, the matrix entries are chosen according to a prespecified adaptive scheme. Each of these two problems is judged by a separate performance criterion. The gauge of efficiency in the first problem is the conditional mutual information between the sparse signal support and measurements. The second problem applies a linear combination of the number of measurements and conditional mutual information as the performance measure. Through several simulations, we study the effectiveness of different designs in various settings. The primary focus in these simulations is the application of a method known as rollout. However, the computational load required for using the rollout method has also inspired us to adapt two data association heuristics to the compressive sensing paradigm. These heuristics show promising decreases in the amount of computation for propagating distributions and searching for optimal solutions."
  },
  {
    "year": "2013",
    "abstract": "Due to the rapid growth of Internet broadband access and proliferation of modern mobile devices, various types of multimedia (e.g., text, images, audios, and videos) have become ubiquitously available anytime. Mobile device users usually store and use multimedia contents based on their personal interests and preferences. Mobile device challenges such as storage limitation have, however, introduced the problem of mobile multimedia overload to users. To tackle this problem, researchers have developed various techniques that recommend multimedia for mobile users. In this paper, we examine the importance of mobile multimedia recommendation systems from the perspective of three smart communities, namely mobile social learning, mobile event guide, and context-aware services. A cautious analysis of existing research reveals that the implementation of proactive, sensor-based and hybrid recommender systems can improve mobile multimedia recommendations. Nevertheless, there are still challenges and open issues such as the incorporation of context and social properties, which need to be tackled to generate accurate and trustworthy mobile multimedia recommendations."
  },
  {
    "year": "2013",
    "abstract": "In recent decades, we have witnessed the evolution of information technologies from the development of VLSI technologies, to communication and networking infrastructure, to the standardization of multimedia compression and coding schemes, to effective multimedia content search and retrieval. As a result, multimedia devices and digital content have become ubiquitous. This path of technological evolution has naturally led to a critical issue that must be addressed next, namely, to ensure that content, devices, and intellectual property are being used by authorized users for legitimate purposes, and to be able to forensically prove with high confidence when otherwise. When security is compromised, intellectual rights are violated, or authenticity is forged, forensic methodologies and tools are employed to reconstruct what has happened to digital content in order to answer who has done what, when, where, and how. The goal of this paper is to provide an overview on what has been done over the last decade in the new and emerging field of information forensics regarding theories, methodologies, state-of-the-art techniques, major applications, and to provide an outlook of the future."
  },
  {
    "year": "2013",
    "abstract": "Public open data access has a direct impact on future IT entrepreneurs' perception of ability to execute their business plans. Using high quality (50%-98% response rate) survey data from 138 Swedish IT-entrepreneurs, we find that access to public open data is considered very important for many IT-startups; 43% find open data essential for the realization of their business plan and 82% claim that access would support and strengthen the business plan. The survey also indicates a significant interest in, and willingness to pay for, public sector information data from companies that do not intend to commercialize data themselves but intend to use it to support or test other business models. From the survey, it is possible to infer that the previous discourse on open data, viewing it as a means for government accountability or e-government, or as the foundation for the commercialization of public sector information data is too limited. Open data should instead be seen as an enabler of innovation outside these traditional sectors. This also indicates that the previously calculated societal values of open data might be underestimated."
  },
  {
    "year": "2013",
    "abstract": "Technical progress in the open-source self replicating rapid prototyper (RepRap) community has enabled a distributed form of additive manufacturing to expand rapidly using polymer-based materials. However, the lack of an open-source metal alternative and the high capital costs and slow throughput of proprietary commercialized metal 3-D printers has severely restricted their deployment. The applications of commercialized metal 3-D printers are limited to only rapid prototyping and expensive finished products. This severely restricts the access of the technology for small and medium enterprises, the developing world and for use in laboratories. This paper reports on the development of a<$2000open-source metal 3-D printer. The metal 3-D printer is controlled with an open-source micro-controller and is a combination of a low-cost commercial gas-metal arc welder and a derivative of the Rostock, a deltabot RepRap. The bill of materials, electrical and mechanical design schematics, and basic construction and operating procedures are provided. A preliminary technical analysis of the properties of the 3-D printer and the resultant steel products are performed. The results of printing customized functional metal parts are discussed and conclusions are drawn about the potential for the technology and the future work necessary for the mass distribution of this technology."
  },
  {
    "year": "2013",
    "abstract": "A new numerical technique to solve nonlinear systems of initial value problems for nonlinear first-order differential equations (ODEs) that model genetic networks in systems biology is developed. This technique is based on finding local Galerkin approximations on each sub-interval at a given time grid of points using piecewise hat functions. Comparing the numerical solution of the new method for a single nonlinear ODE with an exact solution shows that this method gives accurate solutions with relative error1.88×10−11for a time step1×10−6. This new method is compared with the adaptive Runge Kutta (ARK) method for solving systems of ODEs, and the results are comparable for a time step2×10−4. It is shown that the relative error of the Galerkin method decreases approximately linearly with the log of the number of hat functions used. Unlike the ARK method, this new method has the potential to be parallelizable and to be useful for solving biological problems involving large genetic networks. An NSF commissioned video illustrating how systems biology helps us understand that a fundamental process in cells is included."
  },
  {
    "year": "2013",
    "abstract": "The electromagnetic (EM) properties of two-component mixtures involving many disordered regularly and irregularly shaped crystals are studied. The effective relative permittivities are calculated utilizing the time-domain finite integration technique. The effective permittivity of disordered mixtures deviates from established mixing theories especially in cases of high permittivity contrast between inclusions and matrix material, and is strongly correlated to the cross-sectional area of the inclusion crystals. Electric energy density localizes at the edges and corners of inclusions in a manner independent of inclusion shape and influenced by EM propagation direction and surrounding inclusions. For mixtures with both disordered irregular and more organized cube inclusions, energy localization increases as the EM signal travels through the mixture before decreasing due to attenuation of the propagating EM signal. With a large number of inclusion crystals (here in the hundreds), it is found that the impact on effective permittivity from differences in individual inclusion shapes is negligible."
  },
  {
    "year": "2013",
    "abstract": "The design of a sensitivity improved dual diaphragm based vibrating wire transducer for sensing pneumatic pressure is proposed. The improvement achieved in sensitivity of a dual diaphragm structure over the single diaphragm category is verified through experiments. An experimental setup for efficiently assessing the dynamic performances of the pressure transducers is presented. Eventually, the improved sensitivity concedes improvement in precision and resolution of the instrument. While the vibrating wire transducer based pressure sensors always show better dynamic performances over others, the present dual diaphragm structure offers improved sensitivity, which adds to their dynamic performances and overall performance characteristics."
  },
  {
    "year": "2013",
    "abstract": "This paper intends to elucidate challenges in some aspects of the hardware design of future generation computers. We use a system model, a stack of integrated circuit cards cooled by a dielectric coolant (FC77). A set of equations is developed to describe the relationships between the system throughput, the volume, the power consumption, and those concerning the details of internal organization such as signal and power line dimensions and coolant path width. The calculated values of throughput, volume, and power are projected on a state point in a graph of the figures-of-merit pair, the computational density, and the computational efficiency. By manipulating the empirical parameters imbedded in the model, the state point is steered to follow the evolutionary line that runs through the points corresponding to the existing supercomputers of several generations. Then, calculation is extended on state points for future prospective computers with target system throughputs. The results point to the needs for research and development effort on thermal management and materials development. As for thermal management of exa- and zeta-scale computers, we need to refocus heat transfer research. Coolant channels will have very large length-to-width ratios (several thousand), while the heat flux on the channel surface is quite low. Micro-fluidics to guarantee stable coolant flow in such long micro-channels will be of primary importance in place of the means to deal with high heat flux. We also need to develop novel materials for signal transmission lines and cooling, particularly in the development of zeta-scale computers."
  },
  {
    "year": "2013",
    "abstract": "The lack of good and up-to-date lab experiments form a major impediment in the domain of engineering education. Often, the lab experiments are outdated. The Virtual Labs project addresses the issue of lack of good lab facilities, as well as trained teachers, by making remote experimentation possible. The pedagogy is student-centric. The Virtual Labs project has also developed a novel methodology for field trials, outreach, and quality control. Virtual Labs also provide tremendous cost advantage. The Virtual Labs project is a wonderful example of an open educational resource developed by a multiinstitution multidiscipline project team. Over 100000 students are currently using the online labs under the Virtual Labs project. Many of these labs are being accessed outside the regular lab hours."
  },
  {
    "year": "2013",
    "abstract": "The success of designing software intensive systems (SISs) may be improved by incorporating experimentation to be part of the design process. This paper presents a scientific approach to experimentation on objects that are units of designers' behavior and is aimed at solving project tasks in conceptual design. The proposed approach is based on specifying the behavior units as precedents and pseudo-code programming of experiments' plans. Reasoning used by designers in the experiments is registered in a question-answer form. Experimenting is supported by a specialized toolkit."
  },
  {
    "year": "2013",
    "abstract": "At the request of the Domestic Nuclear Detection Office (DNDO), a Study Committee comprised of representatives from the American Physical Society, Panel on Public Affairs, the IEEE, and Nuclear and Plasma Sciences Society performed a technical review of the DNDO Transformational and Applied Research Directorate (TARD) R&D program. TARD's principal objective is to address gaps in the Global Nuclear Detection Architecture (GNDA) through improvements in the performance, cost, and operational burden of detectors and systems. The charge to the Study Committee was to investigate the existing TARD R&D plan and portfolio, recommend changes to the existing plan, and recommend possible new R&D areas and opportunities. This report is the result of an independent, detailed analysis of the current R&D plan and includes, for each application area, observations, and recommendations to focus future investments within the context of the TARD mission."
  }
]