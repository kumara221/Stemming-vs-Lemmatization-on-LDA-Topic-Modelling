[
  {
    "year": "2015",
    "abstract": "Over the last few decades, there has been considerable concern over the multifactory manufacturing environments owing to globalization. Numerous studies have indicated that flexible job-shop scheduling problems (FJSPs) and the distributed and FJSPs (DFJSPs) belong to NP-hard puzzle. The allocation of jobs to appropriate factories or flexible manufacturing units is an essential task in multifactory optimization scheduling, which involves the consideration of equipment performance, technology, capacity, and utilization level for each factory or manufacturing unit. Several variables and constraints should be considered in the encoding problem of DFJSPs when using genetic algorithms (GAs). In particular, it has been reported in the literature that the traditional GA encoding method may generate infeasible solutions or illegal solutions; thus, a specially designed evolution process is required. However, in such a process, the diversity of chromosomes is lost. To overcome this drawback, this paper proposes a refined encoding operator that integrates probability concepts into a real-parameter encoding method. In addition, the length of chromosomes can be substantially reduced using the proposed algorithm, thereby, saving computation space. The proposed refined GA algorithm was evaluated with satisfactory results through two-stage validation; in the first stage, a classical DFJSP was adopted to show the effectiveness of the algorithm, and in the second stage, the algorithm was used to solve a real-world case. The real-world case involved the use of historical data with 100 and 200 sets of work orders of a fastener manufacturer in Taiwan. The results were satisfactory and indicated that the proposed refined GA algorithm could effectively overcome the conflicts caused by GA encoding algorithms."
  },
  {
    "year": "2015",
    "abstract": "The Internet of Things (IoT) is a dynamic global information network consisting of Internet-connected objects, such as radio frequency identifications, sensors, and actuators, as well as other instruments and smart appliances that are becoming an integral component of the Internet. Over the last few years, we have seen a plethora of IoT solutions making their way into the industry marketplace. Context-aware communications and computing have played a critical role throughout the last few years of ubiquitous computing and are expected to play a significant role in the IoT paradigm as well. In this paper, we examine a variety of popular and innovative IoT solutions in terms of context-aware technology perspectives. More importantly, we evaluate these IoT solutions using a framework that we built around well-known context-aware computing theories. This survey is intended to serve as a guideline and a conceptual framework for context-aware product development and research in the IoT paradigm. It also provides a systematic exploration of existing IoT products in the marketplace and highlights a number of potentially significant research directions and trends."
  },
  {
    "year": "2015",
    "abstract": "Phasor measurement units (PMUs) are rapidly being deployed in electric power networks across the globe. Wide-area measurement system (WAMS), which builds upon PMUs and fast communication links, is consequently emerging as an advanced monitoring and control infrastructure. Rapid adaptation of such devices and technologies has led the researchers to investigate multitude of challenges and pursue opportunities in synchrophasor measurement technology, PMU structural design, PMU placement, miscellaneous applications of PMU from local perspectives, and various WAMS functionalities from the system perspective. Relevant research articles appeared in the IEEE and IET publications from 1983 through 2014 are rigorously surveyed in this paper to represent a panorama of research progress lines. This bibliography will aid academic researchers and practicing engineers in adopting appropriate topics and will stimulate utilities toward development and implementation of software packages."
  },
  {
    "year": "2015",
    "abstract": "To meet the requirements of 5G on enabling higher capacity, faster rates, more connectivity, higher reliability, lower latency, greater versatility, and application-domain specific topologies, new concepts and design approaches are in great need. Current standards for 4G may influence the introduction of radio features and network solutions for 5G systems. New network architectures extending beyond heterogeneous networks and exploiting new frequency spectra (e.g., millimeter wave) are emerging from research laboratories around the world. In addition to the network side, advanced terminals and receivers are being developed to optimize network performances. Splitting the control and data planes [currently studied in 3G Partnership Project (3GPP)] is an interesting paradigm for 5G, together with massive multi-input multi-output (MIMO), advanced antenna systems, software-defined networking (SDN), network functions virtualization (NFV), the Internet of Things, and cloud computing. Also, new radio protocols enabling heterogeneous traffics are required."
  },
  {
    "year": "2015",
    "abstract": "X-ray computed tomography (CT) has a central role in clinical imaging, often as the first and only imaging study before definitive intervention for a wide variety of conditions. X-ray micro-CT is similarly important in preclinical imaging with anatomically and physiologically relevant small animal models of human diseases. The field of X-ray CT is entering another Spring. Buds are blooming in the areas of X-ray sources such as those based on carbon nanotubes (CNTs) or free-electron lasers; detectors, especially photon-counting modules; X-ray gratings; contrast agents; compressive sensing (CS); and reconstruction methods. Synergistically, these innovations can be integrated to revolutionize the CT field, achieving a quantum leap in performance for biomedical applications."
  },
  {
    "year": "2015",
    "abstract": "The demand for high-speed data applications, such as high-quality wireless video streaming, social networking, and machine-to-machine communication, has been growing explosively over the past 20 years and it is envisioned that asymmetric digital subscriber line-like user experience will be supported in the next generation wireless networks. The fifth generation (5G) system deployed initially in 2020 is expected to provide about 1000 times higher wireless area capacity and save up to 90% of energy consumption per service compared with the current 4G system. Furthermore, more than 1000-Gb/s/km2area spectral capacity in dense urban environments, 10 times higher battery life time of connected devices, and 5 times reduced end-to-end latency are expected in 5G systems. To meet such challenging goals, there is an urgent need for a revolutionary approach involving new wireless network architectures, as well as advanced signal processing and networking technologies."
  },
  {
    "year": "2015",
    "abstract": "As children develop, they differ from adults in a number of important ways, including anatomy, metabolism, immune system, and the extent of myelination of the nervous system. As a consequence, equivalent exposures to radiation from mobile phones result in different doses to specific tissues in children compared with adults. Higher doses are likely to have more severe implications in the young. A young child's skull is not only smaller and thinner than an adult's, but also has dielectric characteristics closer to those of soft tissues, probably due to a higher water content. The young skull better matches the electromagnetic characteristics of the skin and brain. As a result, finite-difference time-domain (FDTD) simulations confirm field penetration and higher specific absorption rate (SAR) in deeper structures in the young brain. If the peak spatial SAR (psSAR) is modeled in the entire head, as current testing standards recommend, the results for adults and children are equivalent. Our anatomically based evaluations rely on FDTD simulations of different tissues within the brain and confirm that the psSAR in a child's brain is higher than in an adult's brain."
  },
  {
    "year": "2015",
    "abstract": "An ultrawideband (UWB) radar-based breast cancer detection system, which is composed of complementary metal-oxide-semiconductor (CMOS) integrated circuits, is presented. This system includes Gaussian monocycle pulse (GMP) generation circuits, switching (SW) matrix circuits, equivalent-time sampling circuits, and a compact UWB antenna array. During the detection process, the GMP signal with the center frequency of 6 GHz is first generated and transmitted with a repetition frequency of 100 MHz. The GMP signal is sent to a selected transmitter antenna by the SW matrix module, and the reflected signal is captured by the receiver antennas. Next, the high-speed equivalent-time sampling circuits are employed to retrieve the reflected GMP signal. A confocal algorithm is used to reconstruct the breast image. The total size for the prototype module is 45 cm×30 cm×14.5 cm in length, width, and height, respectively, which is dramatically smaller than the conventional detection systems. Using our proposed system, we demonstrate a successful detection of 1-cm cancer target in the breast phantom."
  },
  {
    "year": "2015",
    "abstract": "In this paper, we propose a solution to the problem of scheduling of a smart home appliance operation in a given time range. In addition to power-consuming appliances, we adopt a photovoltaic (PV) panel as a power-producing appliance that acts as a micro-grid. An appliance operation is modeled in terms of uninterruptible sequence phases, given in a load demand profile with a goal of minimizing electricity cost fulfilling duration, energy requirement, and user preference constraints. An optimization algorithm, which can provide a schedule for smart home appliance usage, is proposed based on the mixed-integer programming technique. Simulation results demonstrate the utility of our proposed solution for appliance scheduling. We further show that adding a PV system in the home results in the reduction of electricity bills and the export of energy to the national grid in times when solar energy production is more than the demand of the home."
  },
  {
    "year": "2015",
    "abstract": "The immune system in homo sapiens protects the body against diseases by identifying and attacking foreign pathogens. However, when the system misidentifies native cells as threats, it results in an auto-immune response. The auto-antibodies generated during this phenomenon may be identified through the indirect immunofluorescence test. An important constituent process of this test is the automated identification of antigen patterns in the cell images, which is the focus of this research. We perform a detailed literature review and present a framework to automate the identification of antigen patterns. The efficacy of the framework, demonstrated on the MIVIA ICPR 2012 HEp-2 Cell Contest and SNP HEp-2 Cell datasets, suggests that the algorithm is comparable with the state-of-the-art approaches."
  },
  {
    "year": "2015",
    "abstract": "The use of high-volume quantitative radiomics features extracted from multi-parametric magnetic resonance imaging (MP-MRI) is gaining attraction for the autodetection of prostate tumors, since it provides a plethora of mineable data, which can be used for both detection and prognosis of prostate cancer. While current voxel-resolution radiomics-driven prostate tumor detection approaches utilize quantitative radiomics features associated with individual voxels on an independent basis, the incorporation of additional information regarding the spatial and radiomics feature relationships between voxels has significant potential for achieving a more reliable detection performance. Motivated by this, we present a novel approach for automatic prostate cancer detection using a radiomics-driven conditional random field (RD-CRF) framework. In addition to the high-throughput extraction and utilization of a comprehensive set of voxel-level quantitative radiomics features, the proposed RD-CRF framework leverages inter-voxel spatial and radiomics feature relationships to ensure that the autodetected tumor candidates exhibit interconnected tissue characteristics reflective of cancerous tumors. We evaluated the performance of the proposed framework using clinical prostate MP-MRI data of 20 patients, and the results of RD-CRF framework demonstrated a clear improvement with respect to the state-of-the-art in quantitative radiomics for automatic voxel-resolution prostate cancer detection."
  },
  {
    "year": "2015",
    "abstract": "Smart worlds begin with smart things, such as smart objects, smart cities, smart manufacturing, and smart systems, are overlaid with sensing and actuation, many embedded in things, and eventually encompass all aspects of the cyber, physical, social, and thinking hyperspace. In the future, human beings will live in a smart environment where both life and work are well addressed by technology, whereas humans will be responsible only for providing creativity. For this purpose, we have organized the first 2015 Smart World Congress, including five IEEE International Conferences. Specifically, a variety of challenges are presented in the field of smart world. Therefore, an open forum on the top ten challenges (Top10Cs) for smart worlds was held under the congress to identify the main challenges through collecting the intelligence existing particularly in crowd wisdom. Top10Cs and related works, as a crowdsourcing approach, discuss and analyze the top challenges for smart worlds based on the selective results of the crowd and experts via an online platform. Moreover, we summarize the experiences obtained in organizing the Top10Cs forum."
  },
  {
    "year": "2015",
    "abstract": "With the integration of the advanced computing and communication technologies, smart grid system is dedicated to enhance the efficiency and the reliability of future power systems greatly through renewable energy resources, as well as distributed communication intelligence and demand response. Along with advanced features of smart grid, the reliability of smart grid communication system emerges to be a critical issue, since millions of smart devices are interconnected through communication networks throughout critical power facilities, which has an immediate and direct impact on the reliability of the entire power infrastructure. In this paper, we present a comprehensive survey of reliability issues posted by the smart grid with a focus on communications in support of neighborhood area networks (NAN). Specifically, we focus on network architecture, reliability requirements and challenges of both communication networks and systems, secure countermeasures, and case studies in smart grid NAN. We aim to provide a deep understanding of reliability challenges and effective solutions toward reliability issues in smart grid NAN."
  },
  {
    "year": "2015",
    "abstract": "In recent years, with the further adoption of the Internet of Things and sensor technology, all kinds of intelligent transportation system (ITS) applications based on a wide range of traffic sensor data have had rapid development. Traffic sensor data gathered by large amounts of sensors show some new features, such as massiveness, continuity, streaming, and spatio-temporality. ITS applications utilizing traffic sensor data can be divided into three main types: 1) offline processing of historical data; 2) online processing of streaming data; and 3) hybrid processing of both. Current research tends to solve these problems in separate solutions, such as stream computing and batch processing. In this paper, we propose a hybrid processing approach and present corresponding system implementation for both streaming and historical traffic sensor data, which combines spatio-temporal data partitioning, pipelined parallel processing, and stream computing techniques to support hybrid processing of traffic sensor data in real-time. Three types of real-world applications are explained in detail to show the usability and generality of our approach and system. Our experiments show that the system can achieve better performance than a popular open-source streaming system called Storm."
  },
  {
    "year": "2015",
    "abstract": "Even seemingly simple systems can produce complex dynamics, which leads management professionals to develop tools for training, monitoring, and improving performance. Management simulators provide useful insights about human behavior and interactions, while computational and informational decision support tools offer opportunities to reduce inconsistencies, errors, and non-optimal human choices, particularly for complex systems that involve multiple decision makers, uncertainty, variability, and time. We use the context of a popular management simulator that teaches students about the bullwhip effect (i.e., the beer distribution game) to explore an integrated decision analytic, control theory, and system dynamics approach to the game that recognizes the value of available (imperfect) information and considers the value of perfect information to provide the optimal strategy. Using a discrete event simulation, we characterize optimal decisions and overall team scores for the situation of actual available information and perfect information. We describe our implementation of the strategy in the field to win the 2007 beer game world championship played at the 25th conference of the International System Dynamics Society. This paper seeks to demonstrate that better understanding of the system and use of available information leads to significantly lower expected costs than identified in prior studies. Understanding complex systems and using information optimally may increase system stability and significantly improve performance, in some cases even without better information than already available."
  },
  {
    "year": "2015",
    "abstract": "The network connectivity of selfish wireless networks (SeWNs) constituted by selfish nodes (SeNs) is investigated. The SeN's degree of node-selfishness (DeNS) is used for characterizing the effects of its energy resources and the benefits of the incentives provided for enhancing its transmission willingness. Furthermore, the SeNs' signal to interference plus noise ratios are defined in terms of both their DeNSs and their interference factors. We then continue by quantifying the effect of node-selfishness on the grade of network connectivity and derive both the upper and lower bounds of the critical DeNS. Explicitly, the network is deemed to be connected when the DeNS is below the lower bound and unconnected for a DeNS above the upper bound. This allows us to quantify the asymptotic critical DeNSs for our SeWNs. In addition, we develop an energy-conscious node-selfishness model for characterizing the relationship between the SeN's residual energy and its DeNS. Based on this model and on the asymptotic critical DeNS derived, the critical amount of residual energy required for maintaining a specific grade of network connectivity is determined, which is verified by our simulation results."
  },
  {
    "year": "2015",
    "abstract": "Hierarchical modulation (HM), which is also known as layered modulation, has been widely adopted across the telecommunication industry. Its strict backward compatibility with single-layer modems and its low complexity facilitate the seamless upgrading of wireless communication services. The specific features of HM may be conveniently exploited for improving the throughput/information-rate of the system without requiring any extra bandwidth, while its complexity may even be lower than that of the equivalent system relying on conventional modulation schemes. As a recent research trend, the potential employment of HM in the context of cooperative communications has also attracted substantial research interests. Motivated by the lower complexity and higher flexibility of HM, we provide a comprehensive survey and conclude with a range of promising future research directions. Our contribution is the conception of a new cooperative communication paradigm relying on turbo trellis-coded modulation-aided twin-layer HM-16QAM and the analytical performance investigation of a four-node cooperative communication network employing a novel opportunistic routing algorithm. The specific performance characteristics evaluated include the distribution of delay, the outage probability, the transmit power of each node, the average packet power consumption, and the system throughput. The simulation results have demonstrated that when transmitting the packets formed by layered modulated symbol streams, our opportunistic routing algorithm is capable of reducing the transmit power required for each node in the network compared with that of the system using the traditional opportunistic routing algorithm. We have also illustrated that the minimum packet power consumption of our system using our opportunistic routing algorithm is also lower than that of the system using the traditional opportunistic routing algorithm."
  },
  {
    "year": "2015",
    "abstract": "Network lifetime (NL) maximization techniques have attracted a lot of research attention owing to their importance for extending the duration of the operations in the battery-constrained wireless sensor networks (WSNs). In this paper, we consider a two-stage NL maximization technique conceived for a fully-connected WSN, where the NL is strictly dependent on the source node’s (SN) battery level, since we can transmit information generated at the SN to the destination node (DN) via alternative routes, each having a specific route lifetime (RL) value. During the first stage, the RL of the alternative routes spanning from the SN to the DN is evaluated, where the RL is defined as the earliest time, at which a sensor node lying in the route fully drains its battery charge. The second stage involves the summation of these RL values, until the SN’s battery is fully depleted, which constitutes the lifetime of the WSN considered. Each alternative route is evaluated using cross-layer optimization of the power allocation, scheduling and routing operations for the sake of NL maximization for a predetermined per-link target signal-to-interference-plus-noise ratio values. Therefore, we propose the optimal but excessive-complexity algorithm, namely, the exhaustive search algorithm (ESA) and a near-optimal single objective genetic algorithm (SOGA) exhibiting a reduced complexity in a fully connected WSN. We demonstrate that in a high-complexity WSN, the SOGA is capable of approaching the ESA’s NL within a tiny margin of 3.02% at a 2.56 times reduced complexity. We also show that our NL maximization approach is powerful in terms of prolonging the NL while striking a tradeoff between the NL and the quality of service requirements."
  },
  {
    "year": "2015",
    "abstract": "A Taguchi-based genetic algorithm (TBGA) is adopted in an adaptive neuro-fuzzy inference system (ANFIS) to optimize the micro-structure parameters of backlight modules (BLMs) in liquid-crystal displays. The method reduces the number of experiments and accumulates the data that indicate performance quality of the modules. The TBGA selects appropriate membership functions and optimizes the premise and consequent parameters by minimizing the performance criterion of root-mean-squared error. The results indicate that the ANFIS with TBGA is significantly superior to ANFIS with particle swarm optimization, ANFIS with GA, and conventional ANFIS for designing the BLM model. Another role of the TBGA is optimizing micro-structure parameters for the backlight module. The results confirm excellent outcome of the TBGA-based ANFIS approach in terms of prediction accuracy, cost reduction, and luminance uniformity. Far more superior results were obtained when compared with those reported in the literature using conventional trial-and-error design methods and even Taguchi-based design methods. Fuzzy model in nature, our approach is applicable generally to industrial product designs and, thus, offers an effective route to solving problems in various industries."
  },
  {
    "year": "2015",
    "abstract": "Researchers have shown that the changes in face features due to plastic surgery can be modeled as a covariate that reduces the ability of algorithms to recognize a person's identity. Traditional dictionary learning methods learn a sparse representation using I0and I1norms that are computationally expensive. This paper presents a multiple projective dictionary learning (MPDL) framework that does not require the computation of I0and I1norms. We propose a novel solution to discriminate plastic surgery faces from regular faces by learning representations of local and global plastic surgery faces using multiple projective dictionaries and by using compact binary face descriptors. Experimental results on the plastic surgery database show that the proposed MPDL framework is able to detect plastic surgery faces with a high accuracy of 97.96%. To verify the identity of a person, the detected plastic surgery faces are divided into local regions of interest (ROIs) that are likely to be altered by a particular plastic surgery. The cosine distance between the compact binary face descriptors is computed for each ROI in the detected plastic surgery faces. In addition, we compute the human visual system feature similarity score based on phase congruency and gradient magnitude between the same ROIs. The cosine distance scores and the feature similarity scores are combined to learn a support vector machine model to verify if the faces belong to the same person. We integrate our proposed MPDL framework for face verification with two commercial systems to demonstrate an improvement in verification performance on a combined database of plastic surgery and regular face images."
  },
  {
    "year": "2015",
    "abstract": "The performance of cellular system significantly depends on its network topology, while cellular networks are undergoing a heterogeneous evolution. This promising trend introduces the unplanned deployment of smaller base stations (BSs), thus complicating the performance evaluation even further. In this paper, based on large amount of real BS locations data, we present a comprehensive analysis on the spatial modeling of a cellular network structure. Unlike the related works, we divide the BSs into different subsets according to geographical factor (e.g., urban or rural) and functional type (e.g., macrocells or microcells), and perform a detailed spatial analysis to each subset. After discovering the inaccuracy of the Poisson point process in BS locations modeling, we consider the Gibbs point processes as well as Neyman-Scott point processes and compare their performance in the view of a large-scale modeling test, and finally reveal the general clustering nature of BSs deployment. This paper carries out the first large-scale identification regarding available literature, and provides more realistic and general results to contribute to the performance analysis for the forthcoming heterogeneous cellular networks."
  },
  {
    "year": "2015",
    "abstract": "The collection of long-term health data is accelerating with the advent of portable/wearable medical devices including electrocardiograms (ECGs). This large corpus of data presents great opportunities to improve the quality of cardiac care. However, analyzing the data from these sensors is a challenge; the relevant information from ~120 000 heart beats per patient per day must be condensed into a human-readable form. Our goal is to facilitate the analysis of these unwieldy data sets. We have developed an open source tool for creating easy-to-interpret plots of cardiac information over long periods. We call these plots ECG clocks. The utility of our ECG clock library is demonstrated through multiple examples drawn from a database of 24-h Holter recordings. In these case studies, we focus on the visualization of heart rate and QT dynamics. The ECG clock concept is shown to be relevant for both physicians and researchers, for identifying healthy and abnormal values and patterns in ECG recordings. In this paper, we describe how to use the ECG clock library to analyze 24-h ECG recordings, and how to extend the source code for your own purposes. The tool is applicable to a wide range of cardiac monitoring tasks, such as heart rate variability or ST elevation. This library, which we have made freely available, can help provide new insights into circadian patterns of cardiac function in individuals and groups."
  },
  {
    "year": "2015",
    "abstract": "Non-orthogonal multiple access based on superposition coding (SC) for 5G cellular systems without relays is gaining increasing interest from both academia and industries. Since relay stations will be an integral part of future cellular networks, we propose evolved non-orthogonal multi-access schemes for both direct and relayed users. Our schedulers are built upon SC-relaying schemes with practical discrete hierarchical modulations (HMs), where the messages of two selected users are superposed into different HM layers, each layer being allocated an optimized amount of power and bearing a message flow to be decoded through either a direct or relayed link. As opposed to conventional schedulers that allocate orthogonal resources to each user in wireless relaying systems, our non-orthogonal schedulers allow a pair of selected users to simultaneously share their allocated resource unit. Moreover, unlike the SC-relaying schemes in the literature based on Gaussian codebooks, the proposed schemes are designed and analyzed under the practical constraints of discrete HMs. In spite of the complexity of the power optimization under discrete HMs, we provide a simple and near-optimal power allocation method for sum-rate maximization and proportional fairness. The simulation results show that the conventional orthogonal schedulers are outperformed by the proposed schedulers in terms of sum rate and fairness, even under the practical assumption of HMs."
  },
  {
    "year": "2015",
    "abstract": "As a potential technique to improve channel capacity, orbital angular momentum has been developed in the radio field. In this paper, a novel radio vortex-multiple-input multiple-output (RV-MIMO) system is proposed to provide high capacity in free space. In particular, the vortex channel of the proposed system is modeled. Based on this model, the optimal vortex phase is derived, which results in the optimal capacity of the proposed RV-MIMO system. Simulation results show that the proposed RV-MIMO system could achieve higher capacity than the MIMO system in free space."
  },
  {
    "year": "2015",
    "abstract": "Due to complexities of big video data management, such as massive processing of large amount of video data to do a video summary, it is challenging to effectively and efficiently store and process these video data in a user friendly way. Based on the parallel processing and flexible storage capabilities of cloud computing, in this paper, we propose a practical massive video management platform using Hadoop, which can achieve a fast video processing (such as video summary, encoding, and decoding) using MapReduce, with good usability, performance, and availability. Red5 streaming media server is used to get video stream from Hadoop distributed file system, and Flex is used to play video in browsers. A user-friendly interface is designed for managing the whole platform in a browser-server style using J2EE. In addition, we show our experiences on how to fine-tune the Hadoop to get optimized performance for different video processing tasks. The evaluations show that the proposed platform can satisfy the requirements of massive video data management."
  },
  {
    "year": "2015",
    "abstract": "The smart grid is concerned with energy efficiency and with the environment, being a countermeasure against the territory devastations that may originate by the fossil fuel mining industry feeding the conventional power grids. This paper deals with the integration between the electromobility and the urban power distribution network in a smart grid framework, i.e., a multi-stakeholder and multi-Internet ecosystem (Internet of Information, Internet of Energy, and Internet of Things) with edge computing capabilities supported by cloud-level services and with clean mapping between the logical and physical entities involved and their stakeholders. In particular, this paper presents some of the results obtained by us in several European projects that refer to the development of a traffic and power network co-simulation tool for electro mobility planning, platforms for recharging services, and communication and service management architectures supporting interoperability and other qualities required for the implementation of the smart grid framework. For each contribution, this paper describes the inter-disciplinary characteristics of the proposed approaches."
  },
  {
    "year": "2015",
    "abstract": "This paper examines how flexible cellular system architectures and efficient spectrum management techniques can be used to play a key role in accommodating the exponentially increasing demand for mobile data capacity in the near future. The efficiency of the use of radio spectrum for wireless communications can be dramatically increased by dynamic secondary spectrum sharing; an intelligent approach that allows unlicensed devices access to those parts of the spectrum that are otherwise underutilized by the incumbent users. In this paper, we propose a heuristically accelerated reinforcement learning (HARL)-based framework, designed for dynamic secondary spectrum sharing in Long Term Evolution cellular systems. It utilizes a radio environment map as external information for guiding the learning process of cognitive cellular systems. System level simulations of a stadium temporary event scenario show that the schemes based on the proposed HARL framework achieve high controllability of spectrum sharing patterns in a fully autonomous way. This results in a significant decrease in the primary system quality of service degradation due to interference from the secondary cognitive systems, compared with a state-of-the-art reinforcement learning solution and a purely heuristic typical LTE solution. The spectrum sharing patterns that emerge by using the proposed schemes also result in remarkable reliability of the cognitive eNodeB on the aerial platform. Furthermore, the novel principle and the general structure of heuristic functions proposed in the context of HARL are applicable to a wide range of self-organization problems beyond the wireless communications domain."
  },
  {
    "year": "2015",
    "abstract": "With the rapid development of wireless communication networks, the need of massive Multiple-Input Multiple-Output (MIMO) to offer sufficient network capacity has become evident. As a part of array signal processing, direction-of-arrival (DoA) estimation is of vital importance to obtain directional information of sources and to enable the 3-D beamforming. In this paper, the performance of DoA estimation for massive MIMO systems is analyzed and compared using a low complexity algorithm. To be specific, 2-D unitary estimation of signal parameters via rotational invariance techniques (U-ESPRIT) algorithm is studied to jointly estimate elevation and azimuth information, and uniform rectangular array is selected to represent massive MIMO systems. The simulation results indicate that the U-ESPRIT algorithm works well for the massive MIMO systems. The performance has been evaluated with various types of antenna configuration and source numbers. Finally, the array resolution is adopted to analyze the performance of elevation and azimuth estimation and how to choose the appropriate antenna array configuration."
  },
  {
    "year": "2015",
    "abstract": "The use of freely available online data is rapidly increasing, as companies have detected the possibilities and the value of these data in their businesses. In particular, data from social media are seen as interesting as they can, when properly treated, assist in achieving customer insight into business decision making. However, the unstructured and uncertain nature of this kind of big data presents a new kind of challenge: how to evaluate the quality of data and manage the value of data within a big data architecture? This paper contributes to addressing this challenge by introducing a new architectural solution to evaluate and manage the quality of social media data in each processing phase of the big data pipeline. The proposed solution improves business decision making by providing real-time, validated data for the user. The solution is validated with an industrial case example, in which the customer insight is extracted from social media data in order to determine the customer satisfaction regarding the quality of a product."
  },
  {
    "year": "2015",
    "abstract": "The near-capacity performance of classical low-density parity check (LDPC) codes and their efficient iterative decoding makes quantum LDPC (QLPDC) codes a promising candidate for quantum error correction. In this paper, we present a comprehensive survey of QLDPC codes from the perspective of code design as well as in terms of their decoding algorithms. We also conceive a modified non-binary decoding algorithm for homogeneous Calderbank-Shor-Steane-type QLDPC codes, which is capable of alleviating the problems imposed by the unavoidable length-four cycles. Our modified decoder outperforms the state-of-the-art decoders in terms of their word error rate performance, despite imposing a reduced decoding complexity. Finally, we intricately amalgamate our modified decoder with the classic uniformly reweighted belief propagation for the sake of achieving an improved performance."
  },
  {
    "year": "2015",
    "abstract": "While visual or tactile image data have been conventionally processed via filters or perceptron-like learning machines, the recent advances of computational topology may make it possible to successfully extract the global features from the local pixelwise data. In fact, some inventive algorithms have succeeded in computing the topological invariants, such as the number of objects or holes and irrespective of the shapes and positions of the touches. However, they are mostly offline algorithms aiming at big data. A real-time algorithm for computing topology is also needed for interactive applications such as touch sensors. Here, we propose a fast algorithm to compute the Euler characteristics of touch shapes by using the Poincare-Hopf index for each pixel. We demonstrate that our simple algorithm, implemented solely as logical operations in Arduino, correctly returns and updates the topological invariants of touches in real time."
  },
  {
    "year": "2015",
    "abstract": "This paper concerns with a wireless-energy-transfer (WET)-enabled massive multiple-input-multiple-output (MIMO) system with superimposed pilot (SP)-aided channel estimation. Unlike the conventional WET-enabled frame transmission schemes, with the aid of SP, both the uplink (UL) channel estimation and wireless information transmission (WIT) that powered by the downlink (DL) WET can be operated simultaneously, and thus provide the potential for improving the UL achievable throughput. The impact that the SP has on the performance of such a WET-enabled massive MIMO system is mathematically characterized, and the optimal solution, including the SP power-allocation and the ratio of time-allocation between the duration of UL WIT and DL WET, is derived with regard to maximize the UL achievable throughput. Numerical results demonstrate the proposed SP-aided WET technique yields a superior performance than the conventional pilot-only-based schemes."
  },
  {
    "year": "2015",
    "abstract": "Dynamic power management (DPM) plays a significant role to save power consumption effectively in both the design and operational phases of computer-based systems. It is well known that the state-dependent control policy by monitoring energy states in each component or the whole system is efficient for power saving in server systems whose system state, such as transaction request, can be completely observed. In this paper, we consider an optimal power-aware design in a cluster system and formulate the DPM problem by means of the Markov decision process. We derive the dynamic programming equation for the optimal control policy, which maximizes the expected reward per unit electrical power, which is called the power effectiveness, and give the policy iteration algorithm to determine the optimal control policy sequentially. In numerical experiments, we show the optimal control policy for an example of a cluster system with two service nodes, where the arrival stream of the transaction request is described as a Markov modulated Poisson process. In addition, based on the access data of an enterprise system, the optimal power-aware control for the cluster system and its effectiveness is examined."
  },
  {
    "year": "2015",
    "abstract": "Powering cellular base stations with renewable energy are one of the long-term strategies for achieving green networks and reducing their operational costs. As an energy provider, the power grid is evolving into a smarter one, which allows more energy-efficient cellular networks and enables cooperation and interaction with the smart grid. On one hand, cellular networks can use harvested renewable energy and on-site energy storage to reduce their energy costs. On the other hand, the price of electricity depends on the energy load, which will eventually contribute to decreasing the peak consumption and global energy cost. In this paper, we propose new integration architecture for renewable energy-powered cellular networks and the smart grid. The proposed architecture is designed based on the classification and the analysis of the existing proposals and the requirements of the smart grid, renewable energy systems, and cellular networks."
  },
  {
    "year": "2015",
    "abstract": "To guarantee the ubiquitous and fully autonomous Internet connections in our daily life, the new technical challenges of mobile communications lie on the efficient utilization of resource and social information. To facilitate the innovation of the fifth generation (5G) networks, the cloud radio access network (RAN) and fog network have been proposed to respond newly emerging traffic demands. The cloud RAN functions more toward centralized resource management to achieve optimal transmissions. The fog network takes advantage of social information and edge computing to efficiently alleviate the end-to-end latency. In this paper, we conduct a comprehensive survey of these two network structures, and then investigate possible harmonization to integrate both for the diverse needs of 5G mobile communications. We analytically study the harmonization of cloud RAN and fog network from various points of view, including the cache of Internet contents, mobility management, and radio access control. The performance of transition between the cloud RAN and the fog network has been presented and the subsequent switching strategy has been proposed to ensure engineering flexibility and success."
  },
  {
    "year": "2015",
    "abstract": "Aggregating fine-granular data measurements from smart meters presents an opportunity for utility companies to learn about consumers' power consumption patterns. Several research studies have shown that power consumption patterns can reveal a range of information about consumers, such as how many people are in the home, the types of appliances they use, their eating and sleeping routines, and even the TV programs they watch. As we move toward liberalized energy markets, many different parties are interested in gaining access to such data, which has enormous economical, societal, and environmental benefits. However, the main concern is that many such beneficial uses of smart meter big data would be severely curtailed if the data were excessively protected due to individuals' privacy. In this paper, we propose a game theoretic mechanism that balances between beneficial uses of data and individuals' privacy in deregulated smart grids. Our mechanism solves the problem of access control by fairly compensating consumers for their participation in the data market based on the concept of differential privacy. The results of our experiments show the importance of taking consumers' attitudes toward privacy as a crucial element in designing balanced markets for fair data sharing. Furthermore, the experiments provide a principled way to choose reasonable values for privacy levels that are more relevant to real-world scenarios."
  },
  {
    "year": "2015",
    "abstract": "Precise and accurate localization is one of the fundamental scientific and engineering technologies needed for the applications enabling the emergence of the Smart World. Localization techniques became popular with the global positioning system for outdoor applications, and in recent years, this has been followed by Wi-Fi localization for indoor applications. More recently, localization science and technology has progressed into in-body medical applications. Localization technologies have their own specific challenges depending on the application and environment, which are left for scientists and engineers to overcome. This paper presents the relation among different elements of the Smart World and corresponding localization technologies, classifies localization applications enabling smart devices and environments into logical categories, describes the complexity of the technologies used for localization, and introduces some of the open challenges for localization in the Smart World."
  },
  {
    "year": "2015",
    "abstract": "Cloud computing provides service for resource-constrained customers to perform large-scale scientific computation. However, it also brings some new challenges, which have to be considered in designing outsourcing protocols. In recent years, a few outsourcing protocols have been proposed for different kinds of problems. Quadratic programming (QP) is a class of mathematical optimization problem, and solving a large-scale QP problem requires a large amount of computation. Thus, there is a great need for customer to outsource large-scale QP problem to cloud. In this paper, we design a secure, verifiable, and efficient outsourcing protocol for QP problem. For security consideration, we encrypt the matrices and vectors contained in the QP problem in an efficient way. After cloud computing, we decrypt the result to get the ultimate solution. To ensure correctness, we verify the result returned by the cloud through Karush-Kuhn-Tucker conditions that are the necessary and sufficient conditions for the optimal solution. We also present complexity analysis and numerical simulations to illustrate the efficiency of our outsourcing protocol."
  },
  {
    "year": "2015",
    "abstract": "For task-scheduling problems in cloud computing, a multi-objective optimization method is proposed here. First, with an aim toward the biodiversity of resources and tasks in cloud computing, we propose a resource cost model that defines the demand of tasks on resources with more details. This model reflects the relationship between the user's resource costs and the budget costs. A multi-objective optimization scheduling method has been proposed based on this resource cost model. This method considers the makespan and the user's budget costs as constraints of the optimization problem, achieving multi-objective optimization of both performance and cost. An improved ant colony algorithm has been proposed to solve this problem. Two constraint functions were used to evaluate and provide feedback regarding the performance and budget cost. These two constraint functions made the algorithm adjust the quality of the solution in a timely manner based on feedback in order to achieve the optimal solution. Some simulation experiments were designed to evaluate this method's performance using four metrics: 1) the makespan; 2) cost; 3) deadline violation rate; and 4) resource utilization. Experimental results show that based on these four metrics, a multi-objective optimization method is better than other similar methods, especially as it increased 56.6% in the best case scenario."
  },
  {
    "year": "2015",
    "abstract": "Most existing multi-hop broadcast protocols for vehicular ad hoc networks do not consider the problem of how to adapt transmission parameters according to the network environment. Besides the propagation environment that determines the channel bit error rate, packet payload size has a significant effect on the packet loss rate. In this paper, we first discuss the effect of packet size on the packet reception ratio, and then propose a broadcast protocol that is able to specify the best relay node by taking into account the data payload size. The proposed protocol employs a fuzzy logic-based algorithm to jointly consider multiple metrics (link quality, inter-vehicle distance, and vehicle mobility) and uses a redundancy transmission approach to ensure high reliability. Since the fuzzy membership functions are tuned by using reinforcement learning, the protocol can adapt to various network scenarios. We use both real-world experiments and computer simulations to evaluate the proposed protocol."
  },
  {
    "year": "2015",
    "abstract": "Works on pico-satellites have gained momentum recently, especially those that consider pico-satellites as part of a much larger constellation or swarm. This feature allows pico-satellites to provide high temporal resolution of observational data and redundancy. In particular, it reduces the need for satellite-to-ground communications and, hence, helps save energy and allows the execution of distributed processing algorithms on the satellites themselves. Consequently, satellite-to-satellite or cross-link communication is critical. To realize these advantages, the cross-link antenna employed on pico-satellites must meet many criteria, namely, small size, lightweight, low-power consumption, high gain, wide bandwidth, circular polarization, and beam steerability. To date, no works have examined the suitability of existing planar antenna designs for the use on pico-satellites. To this end, this paper contributes to the literature by focusing on microstrip patch and slot antennas that have the ability to achieve high gain, beam steering, and wide bandwidth. This paper reviews 66 planar antenna designs, which includes 38-patch and 28-slot antennas. In addition, we provide an extensive qualitative comparison of these antennas in terms of their mass, size, gain, beam steerability, type of polarization, operating frequency band, and return loss. In addition, we have evaluated three antenna designs that best address the pico-satellite challenges on a common platform. We find that the asymmetric E-shaped patch antenna design is the most suitable for the use on 2U CubeSats. This is because of its small size (34 × 13 mm2) and high gain (7.3 dB). In addition, the E-shaped patch antenna yields a wide -10-dB bandwidth of 2300 MHz and a small return loss of -15.2 dB."
  },
  {
    "year": "2015",
    "abstract": "Incidents involving data breaches are ever-present in the media since several years. In order to overcome this threat, organizations apply enterprise content-aware data leakage prevention (DLP) solutions to monitor and control data access and usage. However, this paper argues that current solutions are not able to reliably protect information assets. The analyses of data breaches reported in 2014 reveal a significant number of data leakage incidents that are not within the focus of the DLP solutions. Furthermore, these analyses indicate that the classification of the provided data breach records is not qualified for detailed investigations. Therefore, advanced criteria for characterizing data leakage incidents are introduced, and the reported records are extended. The resulting analyses illustrate that DLP and information leakage prevention (ILP) demand various information security (IS) measures to be established in order to reduce the risk of technologically based data breaches. Furthermore, the effectiveness of DLP and information leakage prevention (ILP) measures is significantly influenced by non-technological aspects, such as the human factor. Therefore, this paper presents a concept for establishing DLP and ILP within the scope of IS."
  },
  {
    "year": "2015",
    "abstract": "In this paper, we use a stochastic geometric approach in order to study the impact on energy consumption when base stations are switched OFF independently of each other. We present here both the uplink and downlink analysis based on the assumption that the base stations are distributed according to an independent stationary Poisson point process. This type of modeling allows us to make use of the property that the spatial distribution of the base stations after thinning (switching OFF) is still a Poisson process. This implies that the probability distribution of the signal to interference and noise ratio (SINR) can be kept unchanged when switching-OFF base stations provided that we scale up the transmission power of the remaining base stations. We then solve the problem of optimally selecting the switch-OFF probabilities so as to minimize the energy consumptions, while keeping unchanged the SINR probability distribution. We then study the tradeoff in the uplink performance involved in switching-OFF base stations. These include the energy consumption, the coverage and capacity, and the impact on amount of radiation absorbed by the transmitting user."
  },
  {
    "year": "2015",
    "abstract": "Modern marine electric propulsion vessels have many systems. These interactions and integration aspects are essential when studying a system and subsystem behavior. This is especially important when considering fault scenarios,s harsh weather, and complex marine operations. However, many simulators, including a selection presented here, study the positioning system and the power system separately. This paper proposes a simulator combining the two systems, as an extension to the marine systems simulator MATLAB/Simulink library. The intended use cases and the according design choices are presented. New subsystem models include a power-based electrical bus model and a simplified diesel engine model. Both are validated through the simulation against established models. In addition, established models for generators, electrical storage devices, thrusters, and a mean-value diesel engine model are summarized with rich references. Three case studies illustrate the multi-domain use of the simulator: 1) a semi-submersible drilling rig performing station keeping under environmental disturbances; 2) the same vessel subject to an electrical bus reconfiguration; and 3) a supply vessel with a hybrid power plant."
  },
  {
    "year": "2015",
    "abstract": "Compartmental and data-based modeling of cerebral hemodynamics are alternative approaches that utilize distinct model forms and have been employed in the quantitative study of cerebral hemodynamics. This paper examines the relation between a compartmental equivalent circuit and a data-based input-output model of dynamic cerebral autoregulation (DCA) and dynamic CO2-vasomotor reactivity (DVR). The compartmental model is constructed as an equivalent circuit utilizing putative first principles and previously proposed hypothesis-based models. The linear input-output dynamics of this compartmental model are compared with the data-based estimates of the DCA-DVR process. This comparative study indicates that there are some qualitative similarities between the two-input compartmental model and experimental results."
  },
  {
    "year": "2015",
    "abstract": "In recent years, many studies have investigated the potential of demand response management (DRM) schemes to manage energy for residential buildings in a smart grid. However, most of the existing studies mainly focus on the theoretical design of DRM schemes and do not verify the proposed schemes through implementation. Smart grid research is highly interdisciplinary. As such, the establishment of testbeds to conduct DRM requires various skill sets that might not always be possible to arrange. However, the implementation of a DRM scheme is critical not only to verify the correctness of the design in a practical environment but also to address many important assumptions that are necessary for the actual deployment of the scheme. Thus, the theoretical aspect of DRM solutions should be discussed and verified in a practical environment to ensure that the scheme is suitable for deployment. In this paper, we propose a DRM scheme and construct a residential smart grid testbed to implement the proposed scheme. In the proposed DRM scheme, we suggest two different types of customer engagement plans, namely, green savvy plan and green aware plan, and design algorithms based on two user inconvenience indices to evaluate DRM for peak load reduction. The testbed verifies the effectiveness and efficiency of the proposed DRM scheme."
  },
  {
    "year": "2015",
    "abstract": "Mass customization enables the creation of personalized products that fulfill the features desired by specific customers. In this context, variability models are used to specify which configurable features are supported and which constraints among the features must be satisfied to guarantee the validity of the derived products. As the market demand grows and evolves, variability models become increasingly complex. In such entangled models, it is hard to identify which features are absolutely essential or dispensable because they are required to be included or excluded from all the possible products, respectively. This paper exposes the limitations of existing approaches to automatically detect those features and provides an algorithm that efficiently performs such detection."
  },
  {
    "year": "2015",
    "abstract": "In an industrial system, wireless sensor networks (WSNs) are usually adapted to industrial applications. Industrial system is a novel scenario to apply WSNs. Industrial WSNs are the base to establish a supervisory control and data acquisition system with the benefits of extending the network boundaries and enhancing the network scalability of the WSNs. The integration of industrial systems, such as smart grids and social networks, is an important trend for new network technologies. In many application scenarios of industrial systems, WSNs are controlled by different authorities. The network nodes that belong to different domains can share the sensor data by standard protocols. Moreover, in an applications, scenario that has high security requirements, the nodes of social networking WSNs could belong to different security levels; thus, these data can be controlled only by specific types of users. Therefore, the cross-domain fine-grained data usage is the core problem for this approach. To address this problem, this paper focuses on the cross-domain fine-grained data usage control mechanism of social networking WSNs in industrial systems, which includes cross-domain fine-grained access control and fuzzy clustering for sensing data for efficient analysis. In addition, dynamic service composition is proposed for data usage. The simulation results verify the feasibility and data usage effectiveness of the proposed scheme."
  },
  {
    "year": "2015",
    "abstract": "Smart world is envisioned as an era in which objects (e.g., watches, mobile phones, computers, cars, buses, and trains) can automatically and intelligently serve people in a collaborative manner. Paving the way for smart world, Internet of Things (IoT) connects everything in the smart world. Motivated by achieving a sustainable smart world, this paper discusses various technologies and issues regarding green IoT, which further reduces the energy consumption of IoT. Particularly, an overview regarding IoT and green IoT is performed first. Then, the hot green information and communications technologies (ICTs) (e.g., green radio-frequency identification, green wireless sensor network, green cloud computing, green machine to machine, and green data center) enabling green IoT are studied, and general green ICT principles are summarized. Furthermore, the latest developments and future vision about sensor cloud, which is a novel paradigm in green IoT, are reviewed and introduced, respectively. Finally, future research directions and open problems about green IoT are presented. Our work targets to be an enlightening and latest guidance for research with respect to green IoT and smart world."
  },
  {
    "year": "2015",
    "abstract": "The smart grid mainly suffers from two types of cascading failures: 1) interdependence cascading failure and 2) load propagation cascading failure. The former one happens due to the interdependence between power grid and communication networks. The latter one is caused by the load propagation in the single power grid. A tiny failure leads to the simultaneous occurrence of these two cascading failures. In this paper, we study the system robustness by considering the interdependence and load propagation. First, we develop a mathematical tool to analyze the load propagation in single network. Then, a percolation-based method is proposed to calculate the remaining fractions of survivals after the cascading failures stop. We estimate the node tolerance parameter T (the ratio of capacity to initial workload) threshold T'c, below which the entire system may suffer from the cascading failure. The effect of interdependence on Tc' is also studied, where lower Tc' is required for the less compact interdependence. We prove that the system performance approaches to the upper bound once the tolerance parameter T → ∞. Our analysis indicates that the fraction of survivals in the power grid is always greater than that in communication network, although the initial failure occurs in the power grid. The extensive simulations validate our mathematical analysis, and demonstrate that the relation between the number of initial failures and tolerance parameter threshold is super-linear."
  },
  {
    "year": "2015",
    "abstract": "This paper gives an extended analysis of automotive control systems as components of the integrated motion control (IMC). The cooperation of various chassis and powertrain systems is discussed from a viewpoint of improvement of vehicle performance in relation to longitudinal, lateral, and vertical motion dynamics. The classification of IMC systems is proposed. Particular attention is placed on the architecture and methods of subsystems integration."
  },
  {
    "year": "2015",
    "abstract": "In this paper, we address a big-data analysis method for estimating the driving range of an electric vehicle (EV), allowing drivers to overcome range anxiety. First, we present an estimating approach to project the life of battery pack for 1600 cycles (i.e., 8 years/160 000 km) based on the data collected from a cycle-life test. This approach has the merit of simplicity. In addition, it considers several critical issues that occur inside battery packs, such as the dependence of internal resistance and the state-of-health. Subsequently, we describe our work on driving pattern analysis of an EV, using a machine-learning approach, namely growing hierarchical self-organizing maps, to cluster the collected EV big data. This paper contains the analysis of energy consumption and driving range estimation for EVs, including powertrain simulation and driving behavior analysis. The experimental results, including both simulating battery degradation and analysis of driving behaviors, demonstrate a feasible solution for improving driving range estimation by the EV big data."
  },
  {
    "year": "2015",
    "abstract": "Incorporating cloud computing into heterogeneous networks, the heterogeneous cloud radio access network (H-CRAN) has been proposed as a promising paradigm to enhance both spectral and energy efficiencies. Developing interference suppression strategies is critical for suppressing the inter-tier interference between remote radio heads (RRHs) and a macro base station (MBS) in H-CRANs. In this paper, inter-tier interference suppression techniques are considered in the contexts of collaborative processing and cooperative radio resource allocation (CRRA). In particular, interference collaboration (IC) and beamforming (BF) are proposed to suppress the inter-tier interference, and their corresponding performance is evaluated. Closed-form expressions for the overall outage probabilities, system capacities, and average bit error rates under these two schemes are derived. Furthermore, IC- and BF-based CRRA optimization models are presented to maximize the RRH-accessed users' sum rates via power allocation, which is solved with convex optimization. Simulation results demonstrate that the derived expressions for these performance metrics for IC and BF are accurate, and the relative performance between IC and BF schemes depends on system parameters such as the number of antennas at the MBS, the number of RRHs, and the target signal-to-interference-plus-noise ratio threshold. Furthermore, it is seen that the sum rates of IC and BF schemes increase almost linearly with the transmit power threshold under the proposed CRRA optimization solution."
  },
  {
    "year": "2015",
    "abstract": "Body weight variations are an integral part of a person’s aging process. However, the lack of association between the age and the weight of an individual makes it challenging to model these variations for automatic face recognition. In this paper, we propose a regularizer-based approach to learn weight invariant facial representations using two different deep learning architectures, namely, sparse-stacked denoising autoencoders and deep Boltzmann machines. We incorporate a body-weight aware regularization parameter in the loss function of these architectures to help learn weight-aware features. The experiments performed on the extended WIT database show that the introduction of weight aware regularization improves the identification accuracy of the architectures both with and without dropout."
  },
  {
    "year": "2015",
    "abstract": "Based on the use of multi-beams, high throughput satellites (HTSs) provide high data rates to a large number of users. In this context, the distribution of frequency resources among multi-beams plays an important role. This is because unsuitable distributions might cause the wastage or the starvation of frequencies and bring about low throughput rates. This paper proposes a solution to mitigate the unsuitable allocations of frequency resources in an HTS. The solution is the priority code scheme (PCS), which seeks to respond to users' demands by dynamically scheduling frequency resources for precise satellite footprint areas. The key is to assign a priority code and an efficiency indicator to every multi-beam deployed on the system. The PCS algorithm involves the association of the efficiency indicator with the bandwidth utilization per beam to detect and correct arbitrary bandwidth allocations among the beams. Due to the PCS's cyclical repetition of its algorithm, the concurrence time of the scheme and the tardiness of the algorithm form part of the evaluation of the PCS. Furthermore, we support the implementation of the frequency-reuse process to enhance the exploitation of frequency resources. To evaluate the PCS performance, the analysis delves into the study of the bandwidth utilization, the interference among beams, the concurrence time, and the algorithm tardiness."
  },
  {
    "year": "2015",
    "abstract": "Indoor visible light communication (VLC) using a light emitting diode (LED) transmitter provides a high-signal-to-noise (SNR) link, which is ideal for video transmission. The LED lights illuminate their own attocells and convey video streams in the downlink to multiple mobile terminals (MTs). Three different transmission schemes are considered, namely: the unity frequency reuse, the higher frequency reuse factor-based transmission, and the novel vectored transmission. We then formulate the optimization problems aiming at minimizing the video distortion. This paper commences by evaluating the performance of the MT at specific positions and continues by characterizing the peak SNR (PSNR) degradation across the room. Finally, we quantify the average performance of multiple MTs randomly walking in the room. By investigating the PSNR degradation, the video rate achieved, the energy consumed, and the effects of buffering delay on the video distortion, we demonstrate that the proposed VLC system is capable of supporting high-quality multimedia transmissions."
  },
  {
    "year": "2015",
    "abstract": "We propose a novel method for seamline determination based on semantic segmentation for aerial image mosaicking. First, we train a convolutional neural network (CNN) for pixel labeling to extract building regions. Using the trained CNN, we create a building probability map from an input aerial image with no pre-processing. We then use Dijkstra’s algorithm to find the optimal seamline as a shortest path on the map. We evaluate the quality of the seamlines produced by our method on actual aerial images. Finally, we show that our seamlines never pass through any buildings and compare the effectiveness with the conventional mean-shift segmentation-based method."
  },
  {
    "year": "2015",
    "abstract": "This paper proposes a novel solution, called a decentralized, efficient, privacy-preserving, and selective aggregation (DEP2SA) scheme, designed to support secure and user privacy-preserving data collection in the advanced metering infrastructure. DEP2SA is more efficient and applicable in real-life deployment, as compared with the state of the art, by adopting and adapting a number of key technologies: 1) it uses a multi-recipient system model, making it more applicable to a liberalized electricity market; 2) it uses the homomorphic Paillier encryption and selective aggregation methods to protect users' consumption data against both external and internal attacks, thus making it more secure; 3) it aggregates data at the gateways that are closest to the data originator, thus saving bandwidth and reducing the risk of creating a performance bottleneck in the system; and 4) it uses short signature and batch signature verification methods to further reduce computational and communication overheads imposed on aggregating nodes. The scheme has been analyzed in terms of security, computational, and communication overheads, and the results show that it is more secure, efficient, and scalable than related schemes."
  },
  {
    "year": "2015",
    "abstract": "Cloud offloading is considered a promising approach for both energy conservation and storage/computation enhancement for resource-limited mobile devices. In this paper, we present a Lyapunov optimization-based scheme for cloud offloading scheduling, as well as download scheduling for cloud execution output, for multiple applications running in a mobile device with a multi-core CPU. We derive an online algorithm and prove performance bounds for the proposed algorithm with respect to average power consumption and average queue length, which is indicative of delay, and reveal the fundamental tradeoff between the two optimization goals. The performance of the proposed online scheduling scheme is validated with trace-driven simulations."
  },
  {
    "year": "2015",
    "abstract": "Due to harsh environment, large number of sensors, limited energy, and spectrum scarcity, intelligent sensing becomes a key issue to enable many practical applications in industrial Internet of Things (IoT). In such an industrial environment with noise and interference, an efficient cooperative spectrum sensing (CSS) scheme can achieve spectrum sharing between primary users (PUs) and secondary users (SUs), and effectively solve the spectrum scarcity and reduce energy consumption to make the IoT smarter. As a vital part of CSS, decision transmission (DT) between SUs and fusion center (FC) plays a crucial role. In traditional DT, each SU will transmit its local decision to FC with orthogonal channel in each sensing, which does not consider the packet error and packet loss due to noise during transmission, and aggravates spectrum scarcity and energy consumption. An energy-efficient reliable DT (ERDT) scheme is proposed to enhance CSS in industrial IoT, which considers both packet error and packet loss. First, the CSS mathematical model based on DT is formulated. Second, with rigorous mathematical deduction, the correct decision probability and the energy consumption are analyzed for both ERDT and DT based on logic OR-rule and AND-rule under three cases, respectively: 1) bit error only; 2) packet loss only; and 3) both bit error and packet loss. Detailed simulation results show that, compared with DT, the proposed ERDT can increase correct decision probability and reduce energy consumption for CSS under three different cases. When the existence probability of PU is 50%, the energy consumption of ERDT is only half of that of DT in CSS. Furthermore, when there are 30 SUs in CSS, the existence probability of PU is 50%, both pocket loss rate and bit error rate are 0.05, and the correct decision probability of ERDT is approaching to 1 for CSS in industrial IoT."
  },
  {
    "year": "2015",
    "abstract": "Extreme learning machine (ELM) is emerged as an effective, fast, and simple solution for real-valued classification problems. Various variants of ELM were recently proposed to enhance the performance of ELM. Circular complex-valued extreme learning machine (CC-ELM), a variant of ELM, exploits the capabilities of complex-valued neuron to achieve better performance. Another variant of ELM, weighted ELM (WELM) handles the class imbalance problem by minimizing a weighted least squares error along with regularization. In this paper, a regularized weighted CC-ELM (RWCC-ELM) is proposed, which incorporates the strength of both CC-ELM and WELM. Proposed RWCC-ELM is evaluated using imbalanced data sets taken from Keel repository. RWCC-ELM outperforms CC-ELM and WELM for most of the evaluated data sets."
  },
  {
    "year": "2015",
    "abstract": "Sparsity of the ratings available in the recommender system database makes the task of rating prediction a highly underdetermined problem. This poses a limit on the accuracy and the quality of prediction. In this paper, we utilize secondary information pertaining to user's demography and item categories to enhance prediction accuracy. Within the matrix factorization framework, we introduce additional supervised label consistency terms that match the user and item factor matrices to the available secondary information (metadata). Matrix factorization model-conventionally employed in collaborative filtering techniques-yields dense user and dense item factor matrices-the assumption is that users have an affinity toward all latent factors and items possess all latent factors. Our formulation, based on a recent work, aims to recover a dense user and a sparse item factor matrix-this is a more reasonable model. Human beings show a natural interest toward all the factors, but every item cannot possess all the factors; this leads to a sparse item factor matrix. A natural outcome of our proposal is a solution to the pure cold start problem. We utilize the label consistency map generated from the proposed model to make reasonable recommendations for new users and new items which have not (been) rated yet. We demonstrate the performance of our model for a movie recommendation system. We also design an efficient algorithm for our formulation."
  },
  {
    "year": "2015",
    "abstract": "A major challenge in biometrics is performing the test at the client side, where hardware resources are often limited. Deep learning approaches pose a unique challenge: while such architectures dominate the field of face recognition with regard to accuracy, they require elaborate, multi-stage computations. Recently, there has been some work on compressing networks for the purpose of reducing run time and network size. However, it is not clear that these compression methods would work in deep face nets, which are, generally speaking, less redundant than the object recognition networks, i.e., they are already relatively lean. We propose two novel methods for compression: one based on eliminating lowly active channels and the other on coupling pruning with repeated use of already computed elements. Pruning of entire channels is an appealing idea, since it leads to direct saving in run time in almost every reasonable architecture."
  },
  {
    "year": "2015",
    "abstract": "Building big data analytics (BDA) applications in the cloud introduces inevitable challenges, such as loss of control and uncertainty. To address the existing challenges, numerous efforts have been made on BDA application engineering to optimize the quality of BDA applications in the cloud, such as performance and reliability. However, there is still a lack of systematic view on engineering BDA applications in the cloud. Therefore, in this paper, we present a conceptual framework named CF4BDA to analyze the existing work on BDA applications from two perspectives: 1) the lifecycle of BDA applications and 2) the objects involved in the context of BDA applications in the cloud. The framework can help researchers and practitioners identify the research opportunities in a structured way and guide implementing BDA applications in the cloud. We perform a preliminary evaluation of the usefulness of CF4BDA by applying it to analyze a set of representative studies."
  },
  {
    "year": "2015",
    "abstract": "Network survivability is an attribute that network is continually available even if a communication failure occurs, and is regarded as one of the most important concepts to design dependable computer networks. In the existing work, a power-aware mobile ad hoc network (MANET) is described by a Markov regenerative process, and takes account of the variability in power level, which is caused by the possible low-battery state in each communication node. However, it implicitly ignores effects by the so-called border effects, and lacks the reality in modeling. In this paper, we revisit a power-aware MANET model taking account of border effects and quantify the network survivability more accurately."
  },
  {
    "year": "2015",
    "abstract": "Smart sensor systems have been widely applied in various applications, e.g., patient monitoring and disease analysis, equipment monitoring and fault prediction, pollution monitoring and source detection, and sea searching and tide monitoring. We are moving towards the era of worldwide industrial sensor networks, where a massive amount of heterogeneous sensory data will be generated every day and will require advanced data management. However, the development of industrial applications with sensor networks has to take into account the following challenges:•Efficiently gathering, sharing, and integrating heterogeneous spatial and temporal data and then deriving valuable knowledge in a timely manner."
  },
  {
    "year": "2015",
    "abstract": "Limited memory bandwidth is considered as the major bottleneck in multimedia cloud computing for more and more virtual machines (VMs) of multimedia processing requiring high memory bandwidth simultaneously. Moreover, contending memory bandwidth among parallel running VMs leads to poor quality of service (QoS) of the multimedia applications, missing the deadlines of these soft real-time multimedia applications. In this paper, we present an adaptive framework, Service Maximization Optimization (SMO), which is designed to improve the QoS of the soft real-time multimedia applications in multimedia cloud computing. The framework consists of an automatic detection mechanism and an adaptive memory bandwidth control mechanism. With the automatic detection mechanism, the critical section to the multimedia application performance in the VMs is detected. Then, our adaptive memory bandwidth control mechanism adjusts the memory access rates of all the parallel running VMs to protect the QoS of the soft real-time multimedia applications. From the case studies with real-world multimedia applications, our SMO significantly improves the QoS of the soft real-time multimedia applications with a negligible penalty on system throughput."
  },
  {
    "year": "2015",
    "abstract": "In orthogonal frequency division multiple access systems, inter-cell interference (ICI) can be considered as a collision between resource blocks (RBs), which can be reduced by employing a power control strategy at colliding RBs. This paper presents a random neural network (RNN) and a genetic algorithm-based hybrid cognitive engine (CE) architecture to reduce the ICI and achieve the coverage and capacity optimization in a long-term evolution uplink system. The embedded CE within eNodeB learns from the local environment about the effect of ICI on the reliability of communications. Consequently, the CE dynamically selects the optimal transmission power for serving users based on an experienced signal-to-interference-plus-noise ratio and an ICI on a scheduled RB in the subsequent transmission time intervals. The CE also suggests acceptable transmit power to users operating on the same scheduled RB in adjacent cells through the X2 interface (a communication interface between eNodeBs). The RNN features help the CE to acquire long-term learning, fast decision making, and less computational complexity, which are essential for the development and practical deployment of any real-time cognitive communication system. In six different test cases, the simulation results have shown improvements up to 87% in long-term learning and a quick convergence of the RNN as compared with artificial neural network models. Moreover, the gains of 7% in average cell capacity and 118% in system coverage have been achieved as compared with a fractional power control method."
  },
  {
    "year": "2015",
    "abstract": "Small intestine is the longest organ in the gastrointestinal tract where much of the digestion and the food absorption take place. Wireless video capsule endoscope (VCE) is the first device taking 2-D pictures from the lesions and the abnormalities in the entire length of the small intestine. Since precise localization and mapping inside the small intestine is a very challenging problem, we cannot measure the distance traveled by the VCE to associate lesions and abnormalities to locations inside the small intestine, and we cannot use the 2-D pictures to reconstruct the 3-D image of interior of the entire small intestine in vivo. This paper presents the architectural concept of a novel cyber physical system (CPS), which can utilize the 2-D pictures of the small intestine taken by the VCE to reconstruct the 3-D image of the small intestine in vivo. Hybrid localization and mapping techniques with millimetric accuracy for inside the small intestine is presented as an enabling technology to facilitate the reconstruction of 3-D images from the 2-D pictures. The proposed CPS architecture provides for large-scale virtual experimentations inside the human body without intruding the body with a sizable equipment using reasonable clinical experiments for validation. The 3-D imaging of the small intestine in vivo allows a lesion to be pinpointed for follow-up diagnosis and/or treatment and the abnormalities may be observed from different angles in 3-D images for more thorough examination."
  },
  {
    "year": "2015",
    "abstract": "Spread spectrum is a technique introduced for mitigating electromagnetic interference (EMI) problems in many class of circuits. In this paper, with particular emphasis on switching DC/DC converters, we consider the most common and most efficient known spreading techniques, looking for spreading parameters that ensure the highest EMI reduction and the lowest performance reduction in the circuit where the spreading is applied. The result is an interesting tradeoff not only between EMI reduction and performance drop, but also on the EMI reduction itself when considering different EMI victim models. The proposed analysis is supported by measurements on two switching DC/DC converters: 1) based on pulse-width modulation and 2) based on the resonant converter class."
  },
  {
    "year": "2015",
    "abstract": "Providing top-k query services is relevant for storage servers which collect valuable files/data and process queries for data owners and mobile users. However, this kind of service could incur severe security concerns, because hackers or even the managers/administrators of the servers may steal important data sets and deceive users into responding to forged or incomplete query results. Therefore, these data sets need to be preserved in privacy, and moreover, the users should have the capability to verify the authenticity and integrity of the query results. As users will demand distinct information with various preferences and time domains in the pragmatic world, the traditional top-k queries are insufficient to satisfy their demands. To solve the problem, we provide the functional top-k queries in multi-dimensional space, such that the users can launch queries on the conjunction and sum of the dimensions/attributes. Some recent works have studied how to preserve data privacy and/or integrity for top-k queries on data storage applications. However, these prior works are limited to traditional top-k queries in preserving data privacy and/or integrity without permitting to systematically process top-k queries over encrypted domain in multi-dimensional space and without providing an efficiently verifiable mechanism for the corresponding query results. In this paper, we propose an efficient and effective method, called SFTopk, which addresses more challenging security problems in data privacy and verifiable functional top-k queries in multi-dimensional space. From performance evaluation experiments, it is shown that our proposed method is much more efficient than the prior works in terms of communication overhead and computation cost."
  },
  {
    "year": "2015",
    "abstract": "Diverse proprietary network appliances increase both the capital and operational expense of service providers, meanwhile causing problems of network ossification. Network function virtualization (NFV) is proposed to address these issues by implementing network functions as pure software on commodity and general hardware. NFV allows flexible provisioning, deployment, and centralized management of virtual network functions. Integrated with SDN, the software-defined NFV architecture further offers agile traffic steering and joint optimization of network functions and resources. This architecture benefits a wide range of applications (e.g., service chaining) and is becoming the dominant form of NFV. In this survey, we present a thorough investigation of the development of NFV under the software-defined NFV architecture, with an emphasis on service chaining as its application. We first introduce the software-defined NFV architecture as the state of the art of NFV and present relationships between NFV and SDN. Then, we provide a historic view of the involvement from middlebox to NFV. Finally, we introduce significant challenges and relevant solutions of NFV, and discuss its future research directions by different application domains."
  },
  {
    "year": "2015",
    "abstract": "The charging power of plug-in electric vehicles (PEVs) decreases significantly when the state of charge (SoC) gets closer to the fully charged state, which leads to a longer charging duration. Each time when the battery is charged at high rates, it incurs a significant degradation cost that shortens the battery life. Furthermore, the differences between demand preferences, battery types, and charging technologies make the operation of the charging stations a complex problem. Even though some of these issues have been addressed in the literature, the charging station modeling with battery models and different customer preferences have been neglected. To that end, this paper proposes two queueing-based optimization frameworks. In the first one, the goal is to maximize the system revenue for single class customers by limiting the requested SoC targets. The PEV cost function is composed of battery degradation cost, the waiting cost in the queue, and the admission fee. Under this framework, the charging station is modeled as aM/G/S/Kqueue, and the system performance is assessed based on the numerical and simulation results. In the second framework, we describe an optimal revenue model for multi-class PEVs, building upon the approach utilized in the first framework. Two charging strategies are proposed: 1) a dedicated charger model and 2) a shared charger model for the multi-class PEVs. We evaluate and compare these strategies. Results show that the proposed frameworks improve both the station performance and quality of service provided to customers. The results show that the system revenue is more than doubled when compared with the baseline scenario which includes no limitations on the requested SoC."
  },
  {
    "year": "2015",
    "abstract": "Fully parallel turbo decoders (FPTDs) have been shown to offer a more-than-sixfold processing throughput and latency improvement over the conventional logarithmic Bahl-Cocke-Jelinek-Raviv (Log-BCJR) turbo decoders. Rather than requiring hundreds or even thousands of time periods to decode each frame, such as the conventional Log-BCJR turbo decoders, the FPTD completes each decoding iteration using only one or two time periods, although up to six times as many decoding iterations are required to achieve the same error correction performance. Until now, it has not been possible to explain this increased iteration requirement using an extrinsic information transfer (EXIT) chart analysis, since the two component decoders are not alternately operated in the FPTD. Hence, in this paper, we propose a novel EXIT chart technique for characterizing the iterative exchange of not only extrinsic logarithmic likelihood ratios in the FPTD, but also the iterative exchange of extrinsic state metrics. In this way, the proposed technique can accurately predict the number of decoding iterations required for achieving iterative decoding convergence, as confirmed by the Monte Carlo simulation. The proposed technique offers new insights into the operation of FPTDs, which will facilitate improved designs in the future, in the same way as the conventional EXIT charts have enhanced the design and understanding of the conventional Log-BCJR turbo decoders."
  },
  {
    "year": "2015",
    "abstract": "Given the rapid growth in cloud computing, it is important to analyze the performance of different Hadoop MapReduce applications and to understand the performance bottleneck in a cloud cluster that contributes to higher or lower performance. It is also important to analyze the underlying hardware in cloud cluster servers to enable the optimization of software and hardware to achieve the maximum performance possible. Hadoop is based on MapReduce, which is one of the most popular programming models for big data analysis in a parallel computing environment. In this paper, we present a detailed performance analysis, characterization, and evaluation of Hadoop MapReduce WordCount application. We also propose an estimation model based on Amdahl's law regression method to estimate performance and total processing time versus different input sizes for a given processor architecture. The estimation regression model is verified to estimate performance and run time with an error margin of <;5%."
  },
  {
    "year": "2015",
    "abstract": "In this paper, we propose an adaptive medium access control (MAC) protocol for full-duplex (FD) cognitive radio networks in which FD secondary users (SUs) perform channel contention followed by concurrent spectrum sensing and transmission, and transmission only with maximum power in two different stages (called the FD sensing and transmission stages, respectively) in each contention and access cycle. The proposed FD cognitive MAC (FDC-MAC) protocol does not require synchronization among SUs, and it efficiently utilizes the spectrum and mitigates the self-interference in the FD transceiver. We develop a mathematical model to analyze the throughput performance of the FDC-MAC protocol, where both half-duplex (HD) transmission and FD transmission modes are considered in the transmission stage. Then, we study the FDC-MAC configuration optimization through adaptively controlling the spectrum sensing duration and transmit power level in the FD sensing stage. We prove that there exists optimal sensing time and transmit power to achieve the maximum throughput, and we develop an algorithm to configure the proposed FDC-MAC protocol. Extensive numerical results are presented to illustrate the optimal FDC-MAC configuration and the impacts of protocol parameters and the self-interference cancellation quality on the throughput performance. Moreover, we demonstrate the significant throughput gains of the FDC-MAC protocol with respect to the existing HD MAC and single-stage FD MAC protocols."
  },
  {
    "year": "2015",
    "abstract": "In this paper, we investigate the factors that influence the customer's decision in subscribing to a particular demand response management (DRM) scheme. Based on these factors, we suggest a classification of customer types that include non-green comfort seeking behavior (NCSB) and green incentive seeking behavior (GISB). We use multi-tier DRM plans that clearly specify the incentive and inconvenience for NCSB and GISB customers. The grid operator can obtain maximum profit margin (after paying out the incentives to participating users) from the DRM only if a specific number of customers participate in the NCSB and GISB plans. Any deviation from an ideal subscription pattern is undesirable from the grid's perspective. We also develop a mathematical framework that is based on logistic regression and considers the quantifiable as well as unquantifiable attributes of customer behavior. This model captures the probabilistic nature of customer preferences for different DRM plans. Simulation results reveal that the actual subscription of customers in NCSB and GISB plans significantly deviates from the ideal values. From these, we determine a compromise solution that lies between the ideal and the actual solutions. We also identify that along with economic factors, social factors, such as peer pressure and prompting green or caring behavior, could also be used as potential tools by the grid operator to influence the customer preference. This paper can also be used by the grid operator to design appropriate multi-tier DRM plans."
  },
  {
    "year": "2015",
    "abstract": "Iterative turbo equalization is capable of achieving impressive performance gains over the conventional non-iterative equalization having the same complexity, when communicating over channels that suffer from intersymbol interference (ISI). The state-of-the-art turbo equalizers employ the logarithmic Bahl-Cocke-Jelinek-Raviv (Log-BCJR) algorithm. However, due to the specific nature of serial data processing, the Log-BCJR algorithm introduces significant processing delays at the receiver. Therefore, in low-latency applications having a high throughput, the turbo equalizer might be deemed less attractive than its conventional counterparts. In order to circumvent this problem, in this paper, we conceived a novel fully parallel turbo equalization algorithm, which is capable of significantly reducing the data processing delay and, hence, improving both the processing latency and the attainable throughput at the receiver. The fully parallel equalizer is then combined with the fully parallel turbo decoder for improving the system performance achieved in terms of the bit error ratio. Furthermore, we propose a novel odd-even interleaver design for employment between the fully parallel equalizer and the fully parallel turbo decoder in order to reduce complexity by 50% in fully parallel turbo equalization arrangements, while retaining a comparable performance. Finally, we compare the computational complexity, latency, throughput, hardware resource requirements, and the bit error ratio of the proposed fully parallel scheme to those of a Log-BCJR-based turbo equalizer benchmarker."
  },
  {
    "year": "2015",
    "abstract": "In this paper, the dynamic channel characteristics at 23.5 GHz in an indoor scenario are investigated according to measurement and deterministic simulation. In order to obtain accurate channel realizations, the ray tracing (RT) software is calibrated on both the power delay profile and the path levels. For the measurement data, the propagation paths are identified using the non-parametric peak detection algorithm. The cluster-alike behaviors of these paths and the influence of the antenna radiation pattern are also studied through the comparison with the RT simulated paths. Subsequently, the evolutionary traces of channel with regard to the user equipment's movement are identified by associating the samples, which have the similar parameters at adjacent locations. The features of these traces are analyzed in both statistical and individual ways. Results show that the life durations of most traces are within 5 m. The line of sight and reflected paths with significant power survive longer than the others. These observations confirm the feasibility of designing adaptive beam tracking algorithms based on the spatial consistency of the dominant propagation paths. Moreover, high correlations among the variations of different parameters in the same trace are revealed."
  },
  {
    "year": "2015",
    "abstract": "Enterprises can save a significant energy by letting idle desktops sleep and awake them only when needed. Though existing mechanisms based on centralized or distributed sleep proxy scheme address this issue with good availability, which means that a sleeping machine can always be awoken when needed, they still feature some drawbacks, such as dedicated per-subnet servers, additional per-desktop CPU resource utilization, and extra energy consumption. This seriously impedes their widespread deployment in enterprises. We, thus, propose an improved scheme called wake-up system based on cloud (WaSCO). WaSCO not only provides high availability but also consumes low CPU resource and energy, as it does not need any specific server to help achieve high availability. This system offloads heavy computation from desktops to a stable cloud, which is responsible for managing agents in various subnets by using our proposed algorithm called choosing-and-guaranteeing (CGA) algorithm. When a remote user wakes up a desktop with WaSCO, the cloud sends a message to the selected agents in the subnet, which then send wake-on-LAN packets to wake up the target desktop. In essence, CGA algorithm ensures running agents in each subnet, dynamically adjusts the number of agents, and selects a few desktops, rather than all the desktops, as agents. Experimental results show that WaSCO outperforms centralized and distributed sleep proxy mechanisms in terms of deployment cost, CPU resource cost, and energy consumption, while still maintains high availability."
  },
  {
    "year": "2015",
    "abstract": "Following the two trends of computerization and informatization, another emerging trend is cyberization in which numerous and various cyber entities in cyberspace will exist in cyber-enabled worlds, including the cyber world and cyber-conjugated physical, social, and mental worlds. Computer science and information science, as holistic fields, have, respectively, played important roles in computerization and informatization. Similarly, it is necessary for there to be a corresponding field for cyberization. Cybermatics is proposed as such a holistic field for the systematic study of cyber entities in cyberspace and cyber world, and their properties, functions, and conjugations with entities in conventional spaces/worlds. This paper sets out to explain the necessity and rationale for, and significance of, the proposed field of Cybermatics, what it is and what it encompasses, and how it is related to other fields and areas."
  },
  {
    "year": "2015",
    "abstract": "The development of informationization and intelligentization prompts Internet developing toward a new era. A deep fusion among cyber space, physical space, social space, and thinking space brings a quaternionic cyber-physical-social-thinking hyperspace, based on which an embryo of smart world is being established through heterogeneous spaces. The smart world is expected to be an attractive perspective involving ubiquitous sensing, computing, and communication to achieve comprehensive interconnections of physical perception, cyber interaction, social correlation, and cognitive thinking. In this paper, evolution of the smart world is briefly introduced, and physical-based coordination, social-inspired interactivity, brain-abstracted cooperativity, and cyber-enabled homogeneity are, respectively, discussed as the main characteristics of the smart world."
  },
  {
    "year": "2015",
    "abstract": "This paper proposes a roadside unit (RSU)-coordinated synchronous multi-channel medium access control (MAC) scheme for vehicular ad hoc networks (VANETs). The proposed scheme allows the on board unit (OBU) to reserve service channels (SCHs) on the control channel (CCH) during almost the whole synchronous interval and supports simultaneous transmissions on different SCHs. It enhances the performance of VANETs and decreases the CCH congestion. Moreover, we use RSU to record the rendezvous information and broadcast it to the OBUs. This method avoids the multi-channel hidden terminal problem. An analytical model is developed to evaluate the aggregate throughput on SCHs. This model considers the following factors: non-safety message transmission probability in each synchronous interval, average size of the non-safety message, and the number of OBUs. Furthermore, the requirement of obtaining the maximum throughput is computed. Simulation results are provided to validate the analytical model and to demonstrate the improvement in throughput. The results indicate that the proposed scheme can provide higher aggregate throughput than that of the vehicular enhanced multi-channel MAC and IEEE 1609.4, and better performance in CCH congestion control especially under high network load conditions."
  },
  {
    "year": "2015",
    "abstract": "Recently, more and more traditional services are being migrated into a cloud computing environment that makes the quality of service (QoS) becomes an important factor for service selection and optimal service composition while forming cross-cloud service applications. Considering the nonlinear and dynamic property of QoS data, it is so difficult to achieve dynamic prediction while designing a QoS prediction method with unsatisfactory prediction accuracy. It is thus desirable to explore how to design an effective approach by incorporating some intelligent techniques into the QoS prediction method to improve prediction performance. In this paper, motivated by the adaptive critic design and Q-learning technique, we propose a novel QoS prediction approach to serve this purpose through the combination of fuzzy neural networks and adaptive dynamic programming (ADP), i.e., an online learning scheme. This approach extracts fuzzy rules from QoS data and employs the ADP method to parameter learning of the fuzzy rules. Moreover, we provide a convergence boundedness result for our proposed approach to guarantee the stability. Experimental results on a large-scale QoS service data set verify the prediction accuracy of our proposed approach."
  },
  {
    "year": "2015",
    "abstract": "Current research in the area of automatic visual object recognition heavily relies on testing the performance of new algorithms by using benchmark data sets. Such data sets can be based on standardized data sets collected systematically in a controlled environment (e.g., COIL-20), as well as benchmarks compiled by collecting images from various sources, normally via the World Wide Web (e.g., Caltech 101). Here, we test bias in benchmark data sets by separating a small area from each image such that the area is seemingly blank, and too small to allow manual recognition of the object. The method can be used to detect the existence of data set bias in a single-object recognition data set, and compare the bias to other data sets. The results show that all the tested data sets allowed classification accuracy higher than mere chance by using the small images, although the sub-images did not contain any visually interpretable information. That shows that the consistency of the images within the different classes of object recognition data sets can allow classifying the images even by algorithms that do not recognize objects. Among the tested data sets, PASCAL is the data set with the lowest observed bias, while data sets acquired in a controlled environment, such as COIL-20, COIL-100, and NEC Animals, are more vulnerable to bias, and can be classified by the sub-images with accuracy far higher than mere chance."
  },
  {
    "year": "2015",
    "abstract": "It is demonstrated numerically that a metamaterial-inspired, low profile (height approximately λ/80), electrically small (ka = 0.45) Huygens source antenna can be designed to radiate at 300 MHz in its broadside direction with a high radiation efficiency and a large front-to-back ratio. Two electrically small, near-field resonant parasitic (NFRP) antennas are first designed. Both are based on a coax-fed dipole antenna. An electric dipole response is obtained by combining it with a tunable Egyptian axe dipole (EAD) NFRP element. A magnetic dipole response is obtained by spatially loading the driven dipole with tunable, extruded capacitively loaded loop (CLL) NFRP elements. The driven dipole and the EAD and CLL NFRP elements are combined together and retuned to achieve a broadside radiating Huygens source antenna. Two different designs, one with two CLL elements and one with four, are obtained, and their performance characteristics are compared."
  },
  {
    "year": "2015",
    "abstract": "Context-awareness for big data applications is different from that of traditional applications in that it is getting challenging to obtain the contexts from big data due to the complexity, velocity, variety, and other aspects of big data, especially big video data. The awareness of contexts in big data is more difficult, and should be more in-depth than that of classical applications. Therefore, in this paper, we propose an in-depth context-awareness framework for a pervasive video cloud in order to obtain underlying contexts in big video data. In this framework, we propose an approach that combines the historical view with the current view to obtain meaningful in-depth contexts, where deep learning techniques are used to obtain raw context data. We have conducted initial evaluations to show the effectiveness of the proposed approach in terms of performance and also the accuracy of obtaining the contexts. The evaluation results show that the proposed approach is effective for real-time context-awareness in a pervasive video cloud."
  },
  {
    "year": "2015",
    "abstract": "In 5G mobile communication systems, massive multiple-input multiple-output (MIMO) and heterogeneous networks (HetNets) play crucial roles to achieve expected coverage and capacity across venues. This paper correspondingly addresses software-defined network (SDN) as the central controller of radio resource management in massive MIMO HetNets. In particular, we identify the huge spatial domain information management and complicated MIMO coordination as the grand challenges in 5G systems. Our work accordingly distinguishes itself by considering more network MIMO aspects, including flexibility and complexity of spatial coordination. In our proposed scheme, SDN controller first collects the user channel-state information in an effective way, and then calculates the null-space of victim users and applies linear precoding to that null-space. Simulation results show that our design is highly beneficial and easy to be deployed, due to its high quality of service performance but low computation complexity."
  },
  {
    "year": "2015",
    "abstract": "Initially appearing as an abstract object frequently used in math and physics, tensors have been attracting increasing interest in a broad range of research fields, such as engineering and data science. However, a few studies have addressed their application in wireless scenarios. In this paper, we investigate the wide applications of tensor techniques with an emphasis on the tensor voting method, which serves as an artificial intelligence approach for automatic inference and perceptual grouping. To illustrate the efficiency of the tensor voting approach, we tackle the tracking problem of inferring human mobility traces, which can provide key location information of networking objects. The trace inferring problem is considered under the circumstance that the recorded location information exhibits missing data and noise. Based on the tensor voting theory, we propose a sparse tensor voting algorithm and an implementation scheme with computational efficiency. The model is constructed based on the geometric connections between the input signals and encodes the structure information in the tensor matrix. The missing location information and noise can be distinguished via tensor decomposition. Once the trace information has been completed, further analysis of the inferred trace can be performed based on feature extraction to differentiate different objects. Moreover, we propose several feature extraction methods to characterize the inferred trace, including the scale invariant feature obtained from the fractal analysis. The proposed methods for trace completion and pattern analysis are applied to real human mobility traces. The results show that our proposed approach effectively recovers human mobility trace from the incomplete and noisy data input, and discovers meaningful patterns of inferred traces from various objects."
  },
  {
    "year": "2015",
    "abstract": "The significant benefits associated with microgrids have led to vast efforts to expand their penetration in electric power systems. Although their deployment is rapidly growing, there are still many challenges to efficiently design, control, and operate microgrids when connected to the grid, and also when in islanded mode, where extensive research activities are underway to tackle these issues. It is necessary to have an across-the-board view of the microgrid integration in power systems. This paper presents a review of issues concerning microgrids and provides an account of research in areas related to microgrids, including distributed generation, microgrid value propositions, applications of power electronics, economic issues, microgrid operation and control, microgrid clusters, and protection and communications issues."
  },
  {
    "year": "2015",
    "abstract": "This paper applies to the redesign of a brick in which the selection of the material and the internal geometry are designed to increase sound absorption together with an improvement in structural strength. The definition of the material opportunity is given by the sawmill industry in Mexico, which produces about 2 million tons of sawdust per year, almost all wasted, and another important waste material: plastic. Each year, Mexico produces 992 000 tons of low density polyethylene. These two waste products can be used as raw materials to create wood plastic composite."
  },
  {
    "year": "2015",
    "abstract": "In this paper, we study the optimum estimation of a band-unlimited continuous-time random process using discrete-time samples taken by a sensor powered by energy harvesting devices. In order to accurately represent a band-unlimited random process, a large sampling rate is needed, and this may yield a huge amount of data to be collected and transmitted. In the mean time, the energy required for sensing and transmitting the data must satisfy the constraints imposed by stochastic energy harvesting sources. To cope with these challenges, we propose a family of the best-effort random sensing policies. The best-effort random sensing schemes define a set of randomly chosen candidate sensing instants, and the sensor performs sensing at a given candidate sensing instant only if there is sufficient energy available. Otherwise an energy outage is declared and the sensor remains silent. It is shown through asymptotic analysis that the probability of energy outage during sensing is determined by the ratio between the energy harvesting rate and the energy consumption rate. For a given average energy harvesting rate, less samples per-unit time means a weaker temporal correlation between two adjacent samples, but a smaller energy outage probability, less data, and more energy per sample. Such a tradeoff relationship is captured by developing a closed-form expression of the estimation mean squared error (MSE), which analytically identifies the interactions among the various system parameters. The estimation MSE is minimized by optimizing the tradeoff among system parameters. The proposed optimum sensing scheme can asymptotically achieve the same performance as a system with the conventional energy sources, and significantly reduce the amount of data to be collected and transmitted."
  },
  {
    "year": "2015",
    "abstract": "Localization, finding the coordinates of an object with respect to other objects with known coordinates—hereinafter, referred to as anchors, is a nonlinear problem, as it involves solving circle equations when relating distances to Cartesian coordinates, or, computing Cartesian coordinates from angles using the law of sines. This nonlinear problem has been a focus of significant attention over the past two centuries and the progress follows closely with the advances in instrumentation as well as applied mathematics, geometry, statistics, and signal processing. The Internet-of-Things (IoT), with massive deployment of wireless tagged things, has renewed the interest and activity in finding novel, expert, and accurate indoor self-localization methods, where a particular emphasis is on distributed approaches. This paper is dedicated to reviewing a notable alternative to the nonlinear localization problem, i.e., a linear-convex method, based on Khan et al.’s work. This linear solution utilizes relatively unknown geometric concepts in the context of localization problems, i.e., the barycentric coordinates and the Cayley–Menger determinants. Specifically, in anm-dimensional Euclidean space, a set ofm+1anchors, objects with known locations, is sufficient (and necessary) to localize an arbitrary collection of objects with unknown locations—hereinafter, referred to as sensors, with a linear-iterative algorithm. To ease the presentation, we discuss the solution under a structural convexity condition, namely, the sensors lie inside the convex hull of at leastm+1anchors. Although rigorous results are included, several remarks and discussion throughout this paper provide the intuition behind the solution and are primarily aimed toward researchers and practitioners interested in learning about this challenging field of research. Additional figures and demos have been added as auxiliary material to support this aim."
  },
  {
    "year": "2015",
    "abstract": "A modified multilevel fast multipole algorithm (MLFMA) is proposed to accelerate the partial matrix vector products required in each iteration of the buffered block forward backward method (BBFB), which is a stationary iterative solver used to solve electromagnetic wave propagation and scattering problems. Applying the standard MLFMA to the computation of the partial matrix vector products results in significant redundancy, causing a loss of efficiency of the stationary method. The efficiency can be regained by implementing a modified MLFMA that is based on two simple algorithms. These involve determining precisely what a small subset of cubes is in need of having their associated fields recomputed in the MLFMA upward or downward process during each step of the BBFB process. Numerical experiments are presented to demonstrate the efficiency and the accuracy of the proposed method over the standard method. Although the modified MLFMA is only applied for the BBFB in this paper, it can, in principle, be extended for application to other stationary methods."
  },
  {
    "year": "2015",
    "abstract": "Presently, America’s average electrical power consumption is∼1.3kW/p; in the world as a whole, it is∼0.33kW/p. If, for 2050, a world goal of 1 kW/p is adopted, this implies an average electric power draw of 1 GW for each population cohort of 1 000 000 residents; and the Earth will have∼10 000 such cohorts. Multi-hour outages are already common; demand peaks daily; and renewable generation is intermittent. Hence, as a hedge against rare supply failures, each cohort would profit from local backup storage of electricity/energy in the order of 1–2 GWd. For comparison, the biggest electrochemical storage scheme yet seriously proposed will contain∼240MWh, while even the largest pumped hydro storage reservoirs are <50 GWh. In approximately 50 years, when fossil fuels have become scarce, we should already have constructed this bulk storage. This review argues that the principal contenders for the storage of electricity in bulk are: 1) electrochemical storage in flow batteries; 2) chemical storage in agents, such as ammonia, hydrogen, methanol, or light hydrocarbons; 3) compressed air energy storage; and 4) underground pumped hydro. Finally, it will argue that not one of these four contenders has yet been built, tested, and perfected, while virtually none of the needed storage capacity exists today."
  },
  {
    "year": "2015",
    "abstract": "To recognize people in unconstrained video, one has to explore the identity information in multiple frames and the accompanying dynamic signature. These identity cues include face, body, and motion. Our approach is based on video-dictionaries for face and body. Video-dictionaries are a generalization of sparse representation and dictionaries for still images. We design the video-dictionaries to implicitly encode temporal, pose, and illumination information. In addition, our video-dictionaries are learned for both face and body, which enables the algorithm to encode both identity cues. To increase the ability of our algorithm to learn nonlinearities, we further apply kernel methods for learning the dictionaries. We demonstrate our method on the Multiple Biometric Grand Challenge, Face and Ocular Challenge Series, Honda/UCSD, and UMD data sets that consist of unconstrained video sequences. Our experimental results on these four data sets compare favorably with those published in the literature. We show that fusing face and body identity cues can improve performance over face alone."
  },
  {
    "year": "2015",
    "abstract": "Level set estimation (LSE) is the process of using noisy observations of an unknown function to estimate the region(s) where the function values lie above a given threshold. It has a wide range of applications in many scientific and engineering areas, such as spectrum sensing or environment monitoring. In this paper, we study the energy-efficient LSE of a time-varying random field under a total power constraint. The fusion center of a wireless sensing system performs LSE by using discrete-time samples collected by a sensor. An accurate LSE usually requires a large number of samples to be collected and transmitted. However, most wireless sensing systems operate with a stringent power constraint that may not be able to meet the high energy demands imposed by the large amount of data. The gap between energy demands and supplies is a direct result of the so-called big data problem. It is critical to develop energy-efficient sampling schemes that can bridge this gap by reducing the amount of data required by LSE. Two sampling schemes are considered in this paper: 1) a dynamic active sampling scheme that sequentially and adaptively selects the next sampling instant in a myopic manner with knowledge learned from previous samples and 2) a uniform sampling scheme that employs a fixed sampling rate to minimize the LSE error probability in the long term. The exact analytical cost functions and their respective upper bounds of both sampling schemes are developed by using an optimum thresholding-based LSE algorithm. The design parameters of both sampling schemes are optimized by minimizing their respective cost functions. The analytical and simulation results demonstrate that both sampling schemes can significantly reduce the amount of data collected by the system while obtain accurate LSE under a stringent power constraint. In addition, the uniform sampling scheme slightly outperforms the dynamic active sampling scheme."
  },
  {
    "year": "2015",
    "abstract": "In a traditional file search mechanism, such as flooding, a peer broadcasts a query to its neighbors through an unstructured peer-to-peer (P2P) network until the time-to-live decreases to zero. A major disadvantage of flooding is that, in a large-scale network, this blind-choice strategy usually incurs an enormous traffic overhead. In this paper, we propose a method, called the statistical matrix form (SMF), which improves the flooding mechanism by selecting neighbors according to their capabilities. The SMF measures the following peer characteristics: (1) the number of shared files; (2) the content quality; (3) the query service; and (4) the transmission distance between neighbors. Based on these measurements, appropriate peers can be selected, thereby reducing the traffic overhead significantly. Our experimental results demonstrate that the SMF is effective and efficient. For example, compared with the flooding search mechanism in dynamic unstructured P2P networks, the SMF reduces the traffic overhead by more than 80%. Moreover, it achieves a good success rate and shorter response times."
  },
  {
    "year": "2015",
    "abstract": "Games have been an important tool for motivating undergraduate students majoring in computer science and engineering. However, it is difficult to build an entire game for education from scratch, because the task requires high-level programming skills and expertise to understand the graphics and physics. Recently, there have been many different game artificial intelligence (AI) competitions, ranging from board games to the state-of-the-art video games (car racing, mobile games, first-person shooting games, real-time strategy games, and so on). The competitions have been designed such that participants develop their own AI module on top of public/commercial games. Because the materials are open to the public, it is quite useful to adopt them for an undergraduate course project. In this paper, we report our experiences using the Angry Birds AI Competition for such a project-based course. In the course, teams of students consider computer vision, strategic decision-making, resource management, and bug-free coding for their outcome. To promote understanding of game contents generation and extensive testing on the generalization abilities of the student’s AI program, we developed software to help them create user-created levels. Students actively participated in the project and the final outcome was comparable with that of successful entries in the 2013 International Angry Birds AI Competition. Furthermore, it leads to the development of a new parallelized Angry Birds AI Competition platform with undergraduate students aiming to use advanced optimization algorithms for their controllers."
  },
  {
    "year": "2015",
    "abstract": "The latest trends in spectrum trading allow secondary users (SUs) to employ hybrid access models and exploit bandwidth employing either opportunistic spectrum access or exclusive spectrum access of vacant frequency bands (FBs) leased for exclusive usage. In this paper, the spectrum trading problem is addressed in an operational framework, where a primary spectrum owner (PSO) allocates specific FBs to a number of primary users (PUs) and at the same time allows SUs belonging to multiple service classes to operate opportunistically in a part of the spectrum primarily allocated to the PU operation. By offering exclusive bandwidth-price contracts to SUs in FBs, which are excluded from the PU access, the PSO expects additional revenue. In this framework, the PSO aims at maximizing its revenue through optimal contract design that considers the multiple SU service classes, about which the PSO may have at its disposal either complete or incomplete information. In the first case, it is proven that only one type of contract destined for the highest SU service class is profitable to the PSO. In the second case, two heuristic algorithms are proposed for the optimal contract design corresponding to offering a single contract or multiple contracts to various service classes. The simulation results for the complete and incomplete information cases have been examined with regard to the SU service classes and the number and transmission specifications of the PUs. The two proposed algorithms manage to address the incomplete information case and be close to the optimal values obtained when complete information is available. Finally, the social welfare, as the aggregate expected utility of all parties involved, is examined."
  },
  {
    "year": "2015",
    "abstract": "Direct-coupled cavity bandpass filters with Chebyshev response are studied and realized, using the technique of planar substrate-integrated waveguide (SIW) for the purpose of high-density integration and cost reduction. Using a general mode-matching technique, the scattering matrix of waveguide bifurcation-type discontinuity and asymmetric iris are established and analyzed. Such discontinuities are used as coupling components and building blocks in the design of filters that are realized on RT/duroid 6002 with the ordinary printed circuit board process in our work. Measurement for one of the three four-pole filters gives 3.4 dB insertion loss and 17 dB return loss over the whole passband at near 24 GHz (K-band) with 440-MHz bandwidth (1.8%). The design method can be widely adopted for the development of microwave-integrated circuits, and the mass-fabrication of such SIW filters can be made easy while maintaining a very low cost."
  },
  {
    "year": "2015",
    "abstract": "In this paper, we investigate a friendly spectrally shaped radar waveform design that can be used in a spectral band being utilized by one or more communication systems. We specifically consider legacy communication systems as opposed to cooperative communication systems to address the ever present problem of legacy technologies. This radar waveform is able to share the spectrum with the existing communication systems such that its detection performance is not compromised while trying to help the legacy systems maintain their own symbol error rates (SERs). We show with various scenarios that the spectrally shaped radar waveform outperforms the traditional wideband pulse waveform in terms of detection performance with the communication signals acting as interference to the radar. Moreover, SERs of the legacy systems employing quaternary phase-shift keying modulation in the presence of the shaped radar waveform (acting as interference) outperform SERs of systems under traditional radar pulse interference. These SERs are very close to theoretical noise-only SERs."
  },
  {
    "year": "2015",
    "abstract": "With the wide usage of long-term health care, research on wireless sensing system tends to focus on low power consumption. In this paper, a low-power and high-quality adaptive fuzzy resolution control system is created for wireless body sensor networks. The sampling clocks of analog-to-digital converters (ADCs) can be adaptively selected by an adaptive fuzzy resolution controller. The resolution of the detected signals can be adaptively changed according to the immediate feature of the signals. Users can set the regions of two condition-windows to create four adaptive conditions for the resolution control. The adaptive fuzzy resolution controller can produce control signals to select an appropriate sampling rate for the ADC with a fuzzy decision technique. The proposed adaptive fuzzy resolution controller was realized by VLSI implementation. It can operate at 100 MHz with only 539 gate counts, and its core area is 7124 μm2, synthesized using a 0.18-μm CMOS process. Compared with the previous work, the work presented in this paper achieved a reduction of 33.3% core area and an improved peak signal-to-noise ratio of 15.47 dB under an abnormal situation in a wireless ECG health care monitoring application."
  },
  {
    "year": "2015",
    "abstract": "To achieve insights about the impact of amplified loop interference, we consider a dual-hop fullduplex (FD) massive multiple-input multiple-output (MIMO) amplify-and-forward (AF) relaying system in terms of achievable ergodic rates for each user pair as well as spectrum and energy efficiencies. It is assumed that the base station (or relay) is equipped with MRx receive antennas and MTxtransmit antennas, while all sources and destinations have a single antenna. For such FD massive MIMO AF relaying systems, the closedform expressions of the lower bounds of achievable ergodic rates are derived first with a finite number of receive and transmit antennas at base station. Then, the asymptotic performance analysis is performed by considering three different power-scaling schemes: 1) PS= ES/MRxand PR= ER; 2) PS= ESand PR= ER/MTx; and 3) PS= ES/MRxand PR= ER/MTx, where ESand ERare fixed, and PSand PRdenote the transmit powers of each source and relay, respectively. Our results show that only when the power-scaling 2) is utilized, do the FD massive MIMO AF relay systems have the ability to restrict the loop interference, so that the system performance is free of loop interference when the number of antennas at the relay is large enough. On the contrary, with the power-scaling cases 1) and 3), the systems have no ability to cancel the loop interference even if MRxor MTx(or both) goes to infinity. The insight is different from the results in the FD massive MIMO decode-and-forward relaying systems where the loop interference can be entirely eliminated for the three power-scaling cases."
  },
  {
    "year": "2015",
    "abstract": "This paper considers three issues that arise in creating an algorithm for the robust detection of textured contact lenses in iris recognition images. The first issue is whether the accurate segmentation of the iris region is required in order to achieve the accurate detection of textured contact lenses. Our experimental results suggest that accurate iris segmentation is not required. The second issue is whether an algorithm trained on the images acquired from one sensor will well generalize to the images acquired from a different sensor. Our results suggest that using a novel iris sensor can significantly degrade the correct classification rate of a detection algorithm trained with the images from a different sensor. The third issue is how well a detector generalizes to a brand of textured contact lenses, not seen in the training data. This paper shows that a novel textured lens type may have a significant impact on the performance of textured lens detection."
  },
  {
    "year": "2015",
    "abstract": "An augmentative and alternative communication (AAC) device for people with speech disabilities is presented. This AAC system exhibits the advantages of two currently used systems: 1) usability of the communication boards and 2) natural oral communication of the electronic communicators. To improve comfort in use, robustness and versatility, the system is designed as two separate blocks linked by wireless communication via a wireless network of communication board sheets. The communication sheets, which are the interface with the user, are economical, simple to use, and scalable to adaptation of the number of symbols and the vocabulary of the individuals who use the system. The digital system (record/player system) controls the net, identifies the active sheets and the pushed symbols, and plays and records all sounds. This digital system can be easily replaced by other digital interfaces, such as computers, smartphones, or tablets, and can increase the function of the AAC with the possibility of using the Internet communication (emails and Skype, among others). The prototype has been evaluated in two special education schools, which are attended by children with severe motor disabilities with or without associated disorders and multi-deficiencies. Positive reviews from individuals who use the AAC system open the possibility of the system's use in both home and educational environments."
  },
  {
    "year": "2015",
    "abstract": "For decades, radar has been applied extensively in warfare, earth observation, rain detection, and industrial applications. All those areas are characterized by requirements such as high quality of service, reliability, robustness in harsh environment and short update time for environmental perception, and even imaging tasks. In the vehicle safety and driver assistance field, radars have found widespread application globally in nearly all vehicle brands. With the market introduction of the 2014 Mercedes-Benz S-Class vehicle equipped with six radar sensors covering the vehicles environment 360° in the near (up to 40 m) and far range (up to 200 m), autonomous driving has become a reality even in low-speed highway scenarios. A large azimuth field of view, multimodality and a high update rate have been the key innovations on the radar side. One major step toward autonomous driving was made in August 2013. A Mercedes-Benz research S-Class vehicle—referred to at Mercedes as Bertha—drove completely autonomously for about 100 km from Mannheim to Pforzheim, Germany. It followed the well-known historic Bertha Benz Memorial Route. This was done on the basis of one stereo vision system, comprising several long and short range radar sensors. These radars have been modified in Doppler resolution and dramatically improved in their perception capabilities. The new algorithms consider that urban scenarios are characterized by significantly shorter reaction and observation times, shorter mean free distances, a 360° interaction zone, and a large variety of object types to be considered. This paper describes the main challenges that Daimler radar researchers faced and their solutions to make Bertha see."
  },
  {
    "year": "2015",
    "abstract": "Recently, mobile networking systems have been designed with more complexity of infrastructure and higher diversity of associated devices and resources, as well as more dynamical formations of networks, due to the fast development of current Internet and mobile communication industry. In such emerging mobile heterogeneous networks (HetNets), there are a large number of technical challenges focusing on the efficient organization, management, maintenance, and optimization, over the complicated system resources. In particular, HetNets have attracted great interest from academia and industry in deploying more effective solutions based on artificial intelligence (AI) techniques, e.g., machine learning, bio-inspired algorithms, fuzzy neural network, and so on, because AI techniques can naturally handle the problems of large-scale complex systems, such as HetNets towards more intelligent and automatic-evolving ones. In this paper, we discuss the state-of-the-art AI-based techniques for evolving the smarter HetNets infrastructure and systems, focusing on the research issues of self-configuration, self-healing, and self-optimization, respectively. A detailed taxonomy of the related AI-based techniques of HetNets is also shown by discussing the pros and cons for various AI-based techniques for different problems in HetNets. Opening research issues and pending challenges are concluded as well, which can provide guidelines for future research work."
  },
  {
    "year": "2015",
    "abstract": "This paper reviews the basic concepts of rays, ray tracing algorithms, and radio propagation modeling using ray tracing methods. We focus on the fundamental concepts and the development of practical ray tracing algorithms. The most recent progress and a future perspective of ray tracing are also discussed. We envision propagation modeling in the near future as an intelligent, accurate, and real-time system in which ray tracing plays an important role. This review is especially useful for experts who are developing new ray tracing algorithms to enhance modeling accuracy and improve computational speed."
  },
  {
    "year": "2015",
    "abstract": "This paper investigates the problem of database-assisted spectrum access in dynamic TV white spectrum networks, in which the active user set is varying. Since there is no central controller and information exchange, it encounters dynamic and incomplete information constraints. To solve this challenge, we formulate a state-based spectrum access game and a robust spectrum access game. It is proved that the two games are ordinal potential games with the (expected) aggregate weighted interference serving as the potential functions. A distributed learning algorithm is proposed to achieve the pure strategy Nash equilibrium (NE) of the games. It is shown that the best NE is almost the same with the optimal solution and the achievable throughput of the proposed learning algorithm is very close to the optimal one, which validates the effectiveness of the proposed game-theoretic solution."
  },
  {
    "year": "2015",
    "abstract": "This paper has two parts. The first one deals with how to use large random matrices as building blocks to model the massive data arising from the massive (or large-scale) multiple-input, multiple-output (MIMO) system. As a result, we apply this model for distributed spectrum sensing and network monitoring. The part boils down to the streaming, distributed massive data, for which a new algorithm is obtained and its performance is derived using the central limit theorem that is recently obtained in the literature. The second part deals with the large-scale testbed using software-defined radios (particularly, universal software radio peripheral) that takes us more than four years to develop this 70-node network testbed. To demonstrate the power of the software-defined radio, we reconfigure our testbed quickly into a testbed for massive MIMO. The massive data of this testbed are of central interest in this paper. For the first time, we have modeled the experimental data arising from this testbed. To our best knowledge, there is no other similar work."
  },
  {
    "year": "2015",
    "abstract": "This paper considers borderless 5G ultra-dense networks (UDNs). In particular, a novel scheduling algorithm is proposed that achieves a more uniform distribution of user-throughput than that of the state-of-the-art maximum-throughput (MT) schedulers. The proposed scheduling algorithm also takes the coherence time of the channel into account as well as the impact to the acquired channel state information. A novel radio frame structure that is appropriate for achieving a 1-ms round trip time latency is also proposed. Such a low latency allows one to employ multiuser as well as cooperative multiple input multiple output schemes for mobile users. An evaluation of matched-filter and zero-forcing precoding for mobile users in UDNs is included. The performance of the proposed 5G UDN concept is assessed using a system-level simulator. Extensive numerical results show that the proposed borderless scheduling concept achieves ~77% higher median user-throughput than that of the MT scheduler at the cost of ~17% lower area-throughput. Such results are obtained for a high density of mobile users at velocities of ~50 km/h."
  },
  {
    "year": "2015",
    "abstract": "This paper examines the AP-to-ECM interfaces of five vehicles equipped with electronic throttle control systems. All five vehicles employ simple voltage level sensing from two or three sensors in the accelerator pedal assembly. The purpose of this paper is to identify differences in the AP-to-ECM interfaces of vehicles with high reported rates of unintended acceleration compared to vehicles with low reported rates of unintended acceleration. This paper does not attempt to identify the root causes of unintended acceleration; however, it points out important design issues that suggest a set of best practices for an electronic throttle control design."
  },
  {
    "year": "2015",
    "abstract": "The general problem of a queue-aware radio resource management and scheduling design is investigated for wireless communications under quasi-static fading channel conditions. Based on an analysis of the source buffer queuing system, the problem is formulated as a constrained nonlinear discrete programming problem. The state transition matrix of the queuing system determined by the queue-aware scheduler is shown to have a highly dynamic structure, so that the conventional matrix analysis and optimization tools are not applicable. By reformulating the problem into a nonlinear integer programming problem on an integer convex set, a direct search approach is considered. Two types of search algorithms, gradient based and gradient-free, are investigated. An integer steepest-descent search with a sub-sequential interval search algorithm and a constrained discrete Rosenbrock search (CDRS) algorithm is proposed to solve the nonlinear integer problem. Both algorithms are shown to have low complexity and good convergence. The numerical results for a single user resource allocation are presented, which show that both algorithms outperform equal partitioning and random partitioning queue-aware scheduling. The dynamic programming (DP) solution given by the relative value iteration algorithm, which provides the true optima but has high complexity, is used as a benchmark. In the majority of the numerical examples, the performance of the CDRS algorithm is almost identical to that of the DP approach in terms of both the average queue length minimization and the average packet blocking plus packet retransmission minimization, but it is less complex, and thus has better scalability."
  },
  {
    "year": "2015",
    "abstract": "Indoor localization of smart hand-held devices is essential for location-based services of pervasive applications. The previous research mainly focuses on exploring wireless signal fingerprints for this purpose, and several shortcomings need to be addressed first before real-world usage, e.g., demanding a large number of access points or labor-intensive site survey. In this paper, through a systematic empirical study, we first gain in-depth understandings of Bluetooth characteristics, i.e., the impact of various factors, such as distance, orientation, and obstacles on the Bluetooth received signal strength indicator (RSSI). Then, by mining from historical data, a novel localization model is built to describe the relationship between the RSSI and the device location. On this basis, we present an energy-efficient indoor localization scheme that leverages user motions to iteratively shrink the search space to locate the target device. An Motion-assisted Device Tracking Algorithm has been prototyped and evaluated in several real-world scenarios. Extensive experiments show that our algorithm is efficient in terms of localization accuracy, searching time and energy consumption."
  },
  {
    "year": "2015",
    "abstract": "Mixed-type categorical and numerical data are a challenge in many applications. This general area of mixed-type data is among the frontier areas, where computational intelligence approaches are often brittle compared with the capabilities of living creatures. In this paper, unsupervised feature learning (UFL) is applied to the mixed-type data to achieve a sparse representation, which makes it easier for clustering algorithms to separate the data. Unlike other UFL methods that work with homogeneous data, such as image and video data, the presented UFL works with the mixed-type data using fuzzy adaptive resonance theory (ART). UFL with fuzzy ART (UFLA) obtains a better clustering result by removing the differences in treating categorical and numeric features. The advantages of doing this are demonstrated with several real-world data sets with ground truth, including heart disease, teaching assistant evaluation, and credit approval. The approach is also demonstrated on noisy, mixed-type petroleum industry data. UFLA is compared with several alternative methods. To the best of our knowledge, this is the first time UFL has been extended to accomplish the fusion of mixed data types."
  },
  {
    "year": "2015",
    "abstract": "In the near future, i.e., beyond 4G, some of the prime objectives or demands that need to be addressed are increased capacity, improved data rate, decreased latency, and better quality of service. To meet these demands, drastic improvements need to be made in cellular network architecture. This paper presents the results of a detailed survey on the fifth generation (5G) cellular network architecture and some of the key emerging technologies that are helpful in improving the architecture and meeting the demands of users. In this detailed survey, the prime focus is on the 5G cellular network architecture, massive multiple input multiple output technology, and device-to-device communication (D2D). Along with this, some of the emerging technologies that are addressed in this paper include interference management, spectrum sharing with cognitive radio, ultra-dense networks, multi-radio access technology association, full duplex radios, millimeter wave solutions for 5G cellular networks, and cloud technologies for 5G radio access networks and software defined networks. In this paper, a general probable 5G cellular network architecture is proposed, which shows that D2D, small cell access points, network cloud, and the Internet of Things can be a part of 5G cellular network architecture. A detailed survey is included regarding current research projects being conducted in different countries by research groups and institutions that are working on 5G technologies."
  },
  {
    "year": "2015",
    "abstract": "This paper presents a complete approach to a successful utilization of a high-performance extreme learning machines (ELMs) Toolbox for Big Data. It summarizes recent advantages in algorithmic performance; gives a fresh view on the ELM solution in relation to the traditional linear algebraic performance; and reaps the latest software and hardware performance achievements. The results are applicable to a wide range of machine learning problems and thus provide a solid ground for tackling numerous Big Data challenges. The included toolbox is targeted at enabling the full potential of ELMs to the widest range of users."
  },
  {
    "year": "2015",
    "abstract": "This paper proposes a joint energy efficiency (EE) and spectrum efficiency (SE) tradeoff analysis as a multi-objective optimization problem (MOP) in the uplink of multi-user multi-carrier two-tier orthogonal frequency division multiplexing access heterogeneous networks subject to users’ maximum transmission power and minimum rate constraints. The proposed MOP is modeled such that the network providers can dynamically tune the tradeoff parameters to switch between different communication scenarios with diverse design requirements. In order to find its Pareto optimal solution, the MOP is transformed, using a weighted sum method, into a single-objective optimization problem (SOP), which itself can further be transformed from a fractional form, by exploiting fractional programming, into a subtractive form. Since the formulated SOP is hard to solve due to the combinatorial channel allocation indicators, we reformulate the SOP into a better tractable problem by relaxing the combinatorial indicators using the idea of time-sharing. We then prove that this reformulated SOP is strictly quasi-concave with respect to the transmission power and the subcarrier allocation indicator. We then propose an iterative two-layer distributed framework to achieve an upper bound Pareto optimal solution of the original proposed MOP. The numerical simulations demonstrate the effectiveness of our proposed two-layer framework achieving an upper bound Pareto optimal solution, which is very close to an optimal solution, with fast convergence, lower and acceptable polynomial complexity, and balanced EE–SE tradeoff."
  },
  {
    "year": "2015",
    "abstract": "Evaluating patient progress and making discharge decisions regarding inpatient medical rehabilitation rely upon the standard clinical assessments administered by trained clinicians. Wearable inertial sensors can offer more objective measures of patient movement and progress. We undertook a study to investigate the contribution of wearable sensor data to predict discharge functional independence measure (FIM) scores for 20 patients at an inpatient rehabilitation facility. The FIM utilizes a seven-point ordinal scale to measure patient independence while performing several activities of daily living, such as walking, grooming, and bathing. Wearable inertial sensor data were collected from ecological ambulatory tasks at two time points mid-stay during inpatient rehabilitation. Machine learning algorithms were trained with sensor-derived features and clinical information obtained from medical records at admission to the inpatient facility. While models trained only with clinical features predicted discharge scores well, we were able to achieve an even higher level of prediction accuracy when also including the wearable sensor-derived features. Correlations as high as 0.97 for leave-one-out cross validation predicting discharge FIM motor scores are reported."
  },
  {
    "year": "2015",
    "abstract": "In this paper, we tackle the problem of minimum time length link scheduling in 60-GHz ad hoc wireless networks using directional antennas with directional beamforming, under both traffic demand and signal to interference and noise ratio constraints. Both single-hop and multi-hop cases are considered. For the single-hop scenario, a binary integer programming problem is formulated by incorporating a general interference model for directional transmissions and a Markov chain-based blockage model. Two effective solution algorithms are proposed, including a greedy algorithm that maximizes the instant throughput for each time slot, and a column generation-based algorithm that iteratively improves the current link schedule. For the multi-hop scenario, we develop a more complicated problem formulation incorporating both route selection and flow conservation constraints. We also develop an effective algorithm to solve the multi-hop problem. The performance of the proposed algorithms is validated with simulations."
  },
  {
    "year": "2015",
    "abstract": "Advanced persistent threat (APT) is a serious threat to the Internet. With the aid of APT malware, attackers can remotely control infected machines and steal sensitive information. DNS is popular for malware to locate command and control (C&C) servers. In this paper, we propose a novel system placed at the network egress point that aims to efficiently and effectively detect APT malware infections based on malicious DNS and traffic analysis. The system uses malicious DNS analysis techniques to detect suspicious APT malware C&C domains, and then analyzes the traffic of the corresponding suspicious IP using the signature-based and anomaly based detection technology. We extracted 14 features based on big data to characterize different properties of malware-related DNS and the ways that they are queried, and we also defined network traffic features that can identify the traffic of compromised clients that have remotely been controlled. We built a reputation engine to compute a reputation score for an IP address using these features vector together. Our experiment was performed at a large local institute network for two months, and all the features were studied with big data, which includes ~400 million DNS queries. Our security approach cannot only substantially reduce the volume of network traffic that needs to be recorded and analyzed but also improve the sustainability of the system."
  },
  {
    "year": "2015",
    "abstract": "Multi-radio access technology (RAT) cellular communication systems limit connected users to utilizing a single RAT even when employing multi-mode user equipment (UE) capable of utilizing multi-RATs. Single-mode access, combined with static spectrum partitioning between co-deployed RATs and independent resource allocation for employed RATs, results in suboptimal spectrum utilization in multi-RAT systems. This paper models user access in multi-RAT systems and proposes enabling multi-mode UE to simultaneously utilize multiple RATs, using multi-RAT carrier aggregation, to improve the performance and spectrum utilization of multi-RAT systems. Several realizations of multi-mode access with varying implementation requirements are presented and discussed. Detailed system-level simulations, for a system co-deploying High Speed Packet Access (HSPA) and Long-Term Evolution (LTE), are performed to investigate the gains and limitations of different user access configurations in multi-RAT systems."
  },
  {
    "year": "2015",
    "abstract": "We present an electrocardiogram (ECG)-based data encryption (EDE) scheme for implantable medical devices (IMDs). IMDs, including pacemakers and cardiac defibrillators, perform therapeutic or even life-saving functions and store sensitive data; therefore, it is important to prevent adversaries from having access to them. The EDE is designed with the ability to provide information-theoretically unbreakable encryption where two well-known techniques of classic one-time pads (OTPs) and error correcting codes are combined to achieve a cryptographic primitive for IMDs. Unlike other ECG-based key agreement schemes where ECG features are used to facilitate a key distribution, in the EDE scheme, random binary strings generated from ECG signals are directly used as keys for encryption. OTP keys are generated by the IMD and the programmer, respectively, before each encryption attempt; thus, the EDE does not require a cryptographic infrastructure to support a key distribution, storage, revocation, and refreshment. Protected by the EDE, IMDs could not be accessed by adversaries; however, medical personnel can have access to them by measuring real-time ECG data in emergencies. Therefore, the EDE design achieves a balance of high security and high accessibility for the IMD. Our data and security analysis shows that the EDE is a viable scheme for protecting IMDs."
  },
  {
    "year": "2015",
    "abstract": "A mobile app for smartphones and tablets to document pressure ulcers was previously developed. The mobile app is part of the rapidly growing field of mobile health. The mobile app replaces paper-based documentation in a healthcare facility with an electronic record. In a user trial in 2013, a key finding was the high value attributed to wound image (photograph) galleries in the mobile app and wound tracking though graphing progression. Consequently, work was undertaken to enhance the imaging features by developing image analysis algorithms for size and color determination of wounds from wound images taken with an on-board smartphone or tablet camera, using no peripheral hardware or ancillary devices in setting up the image. The reliance solely on the internal smartphone sensors to generate high-accuracy measurements brings novelty to the work and specifically in the field of wound management. The work includes three components. The first component, referred to as mask image, obtains the dimensions of an object in the image. The second component, referred to as camera calibration, reconstructs an image taken on an angle (3-D) referenced back to a 2-D plane. The third algorithm determines the range of colors present in an image, separating the image into three component colors by extracting components from the Red Green Blue format of the image, and converting output to red yellow black. An expert system and/or machine learning is recommended to enhance the correlation of wound color to wound stage."
  },
  {
    "year": "2015",
    "abstract": "Big data strongly demands a network infrastructure having the capability to efficiently collect, process, cache, share, and deliver the data, instead of simple transmissions. Such network designs show the requirements of energy efficiency, availability, high performance, and data-aware intelligence. To meet these requirements, we adopt the information-centric networking (ICN) approach, where data are retrieved through names and in-network caching is utilized. However, as the typical existing ICN architectures, content centric network (CCN) cannot efficiently utilize the caches for data sharing because of the on-path caching strategy, and network of information (NetInf) demonstrates the resolution latency for data retrievals. To design an efficient and effective ICN architecture for big data sharing, we combine the strong points of CCN and NetInf, where information islands (IOIs) and management plane are utilized for direct data retrieval and global data discovery, respectively. We provide a reference architecture and propose an aggregatable name-based routing (ANBR), which can naturally enable consumers to retrieve the closest copy of information. In this network, each piece of data can be cached at one IOI at most once, which greatly improves the efficiency of cache usages. The consumers first try to retrieve the data in the local IOI, and then try to globally retrieve it from the closest IOI, holding the copy of the data if necessary. We investigate the impact from the key factor, IOI size, to the energy consumption of ANBR. It shows that energy consumption first decreases and then increases as the IOI size increases, and the optimized IOI size can be found for deployment. Furthermore, we study the relation between the optimized IOI size and the average retrieval times for the data. The result shows that the optimized IOI size increases as the average retrieval times increase."
  },
  {
    "year": "2015",
    "abstract": "Recent decades have seen significant progress in the field of artificial hands. Most of the surveys, which try to capture the latest developments in this field, focused on actuation and control systems of these devices. In this paper, our goal is to provide a comprehensive survey of the sensors for artificial hands. In order to present the evolution of the field, we cover five year periods starting at the turn of the millennium. At each period, we present the robot hands with a focus on their sensor systems dividing them into categories, such as prosthetics, research devices, and industrial end-effectors. We also cover the sensors developed for robot hand usage in each era. Finally, the period between 2010 and 2015 introduces the reader to the state of the art and also hints to the future directions in the sensor development for artificial hands."
  },
  {
    "year": "2015",
    "abstract": "Anew wideband hybrid antenna is proposed in this paper. It combines an F-shaped conducting monopole antenna and a water dielectric resonator antenna (DRA) to effectively broaden the antenna bandwidth. The F-shaped monopole is also used to excite the water DRA. The unique features of water, such as transparency and liquidity, allow complex feeding structures to be placed, and tuned inside the water DRA. A comprehensive parametric study is conducted to optimize the antenna performance. The final design is made and tested. A good agreement is obtained between the simulation and the measurement results. Compared with the conventional straight probe, the proposed antenna has a wider bandwidth from 410 to 870 MHz (a fractional bandwidth of 71.8%) for S11<; -6 dB, with a compact size (52 mm × 51.5 mm × 10 mm, roughly 0.071λ × 0.07λ × 0.0136λ at 410 MHz). It is shown that this new antenna is a very good candidate for hand-portable applications such as Digital Video Broadcasting- Handheld."
  },
  {
    "year": "2015",
    "abstract": "Wrinkles play an important role in the face-based analysis. They have been widely used in applications, such as facial retouching, facial expression recognition, and face age estimation. Although a few techniques for a wrinkle analysis have been explored in the literature, poor detection limits the accuracy and reliability of wrinkle segmentation. Therefore, an automated wrinkle detection method is crucial to maintain consistency and reduce human error. In this paper, we propose Hessian line tracking (HLT) to overcome the detection problem. HLT is composed of Hessian seeding and directional line tracking. It is an extension of a Hessian filter; however, it significantly increases the accuracy of wrinkle localization when compared with existing methods. In the experimental phase, three coders were instructed to annotate wrinkles manually. To assess the manual annotation, both intrareliability and interreliability were measured, with an accuracy of 94% or above. The experimental results show that the proposed method is capable of tracking hidden pixels; thus, it increases connectivity of detection between wrinkles, allowing some fine wrinkles to be detected. In comparison to the state-of-the-art methods such as the Cula Method, Frangi Filter, and Hybrid Hessian Filter, the proposed HLT yields better results, with an accuracy of 84%. This paper demonstrates that the HLT is a remarkably strong detector of forehead wrinkles in 2-D images."
  },
  {
    "year": "2015",
    "abstract": "This paper aims to understand, identify, and mitigate the impacts of residential electric vehicle (EV) charging on distribution system voltages. A thorough literature review on the impacts of residential EV charging is presented, followed by a proposed method for evaluating the impacts of EV loads on the distribution system voltage quality. Practical solutions to mitigate EV load impacts are discussed as well, including infrastructural changes and indirect controlled charging with time-of-use (TOU) pricing. An optimal TOU schedule is also presented, with the aim of maximizing both customer and utility benefits. This paper also presents a discussion on implementing smart charging algorithms to directly control EV charging rates and EV charging starting times. Finally, a controlled charging algorithm is proposed to improve the voltage quality at the EV load locations while avoiding customer inconvenience. The proposed method significantly decreases the impacts of EV load charging on system peak load demand and feeder voltages."
  },
  {
    "year": "2015",
    "abstract": "How to quickly and accurately measure the volume of a large cavity is challenging. This paper presents an efficient method to measure the volume of a large conducting cavity. The proposed method is based on statistical wave theory. By measuring the Q factor in the time and frequency domains, the volume of the cavity can be extracted. In the time domain, the Q factor can be extracted directly from the time domain response, while in the frequency domain, the Q factor depends on the volume of the cavity and the transferred power; the transferred power can be measured directly. By correcting the frequency domain Q with the radiation efficiency of antennas, the Q factors obtained from both the time and frequency domains are equal in a well-stirred chamber; this provides an opportunity to measure the volume of the cavity. Measurements are conducted to verify the proposed method. Although the measurement is conducted using electromagnetic waves, acoustic waves can also be used; in this case, the approach can be applied to any cavity, not limited to a conducting cavity. The advantages and the limitations of the proposed method are also discussed."
  },
  {
    "year": "2015",
    "abstract": "This paper provides 28- and 73-GHz urban omnidirectional propagation large-scale path loss data measured in downtown New York City during the summers of 2012 and 2013, and 38 GHz data measured in downtown Austin in the summer of 2011. The data provided herein may be used by antenna, propagation, and communications researchers for emerging mobile and/or backhaul millimeter-wave (mmWave) system analyses. This paper also presents measurement layout maps with transmitter and receiver locations and GPS coordinates, so that anyone may create similar or new measurements and models, or may perform further processing, such as with ray-tracers and modeling tools, in addition to studying mmWave system performance. Using the data provided herein, large-scale path loss models using a standard close-in 1 meter free-space reference distance are provided for each of the three frequency bands."
  },
  {
    "year": "2015",
    "abstract": "The device-to-device (D2D) communication paradigm in 5G networks provides an effective infrastructure to enable different smart city applications such as public safety. In future smart cities, dense deployment of wireless sensor networks (WSNs) can be integrated with 5G networks using D2D communication. D2D communication enables direct communication between nearby user equipments (UEs) using cellular or ad hoc links, thereby improving the spectrum utilization, system throughput, and energy efficiency of the network. In this paper, we propose a hierarchal D2D communication architecture where a centralized software-defined network (SDN) controller communicates with the cloud head to reduce the number of requested long-term evolution (LTE) communication links, thereby improving energy consumption. The concept of local and central controller enables our architecture to work in case of infrastructure damage and hotspot traffic situation. The architecture helps to maintain the communication between disaster victims and first responders by installing multi-hop routing path with the support of the SDN controller. In addition, we highlight the robustness and potential of our architecture by presenting a public safety scenario, where a part of the network is offline due to extraordinary events such as disaster or terrorist attacks."
  },
  {
    "year": "2015",
    "abstract": "Acoustic metrics extracted from speech have the potential to serve as novel biomarkers for a variety of neurological and neurodevelopmental conditions, as is evidenced by the rapidly growing corpus of research articles studying the links between brain impairments and speech. In this paper, we discuss the advantages and the disadvantages of speech biomarkers and the various challenges in the design and the implementation of portable speech-based diagnostic and assessment tools. Furthermore, we provide a case study, presenting our experiences in developing an assessment tool for the detection of mild traumatic brain injuries (concussions) and discuss the challenges in obtaining and analyzing large sets of speech recordings that can be used to study the impact of brain injuries on vocal features."
  },
  {
    "year": "2015",
    "abstract": "Multiscale decomposition has been an invaluable tool for the processing of physiological signals. Much focus on multiscale decomposition for processing such signals have been based on scale-space theory and wavelet transforms. In this paper, we take a different perspective on multiscale decomposition by investigating the feasibility of utilizing a Bayesian-based method for multiscale signal decomposition called Bayesian residual transform (BRT) for the purpose of physiological signal processing. In BRT, a signal is modeled as the summation of residual signals, each characterizing information from the signal at different scales. A deep cascading framework is introduced as a realization of the BRT. Signal-to-noise ratio analysis using electrocardiography signals was used to illustrate the feasibility of using the BRT for suppressing the noise in physiological signals. Results in this paper show that it is feasible to utilize the BRT for processing physiological signals for tasks, such as noise suppression."
  },
  {
    "year": "2015",
    "abstract": "The introduction of cellphones with poor receiver sensitivity, known as dirty devices, to the Code Division Multiple Access (CDMA) cellular network results in a reduction in cell edge coverage, degraded capacity, and higher drop-call rates. This poor network performance has a negative impact on customer experience and satisfaction, which may result in increased churn for CDMA wireless carriers. Previous research on the impact of dirty devices has used computer-based simulations. This paper demonstrates the influence of dirty devices on a live commercial CDMA network using two distinct approaches. The first approach deploys dirty and non-dirty devices in a live commercial cell, and measures the impact on forward base transceiver station (BTS) power and forward link Ec/Io (Ec is the received pilot chip energy and Io is the spectral density of the total power seen by the device). A series of tests on a live CDMA network with known dirty devices shows an increase in BTS transmit power resources compared with non-dirty devices, leading to the degradation of forward link Ec/Io. The second approach uses a statistical study of the aggregated pilot strength measurement messages reported by the mobile to ascertain the impact of known dirty devices on the overall network Ec/Io of non-dirty devices."
  },
  {
    "year": "2015",
    "abstract": "This paper investigates low-complexity approaches to small-cell base-station (SBS) design, suitable for future 5G millimeter-wave (mmWave) indoor deployments. Using large-scale antenna systems and high-bandwidth spectrum, such SBS can theoretically achieve the anticipated future data bandwidth demand of 10000 fold in the next 20 years. We look to exploit small cell distances to simplify SBS design, particularly considering dense indoor installations. We compare theoretical results, based on a link budget analysis, with the system simulation of a densely deployed indoor network using appropriate mmWave channel propagation conditions. The frequency diverse bands of 28 and 72 GHz of the mmWave spectrum are assumed in the analysis. We investigate the performance of low-complexity approaches using a minimal number of antennas at the base station and the user equipment. Using the appropriate power consumption models and the state-of-the-art sub-component power usage, we determine the total power consumption and the energy efficiency of such systems. With mmWave being typified nonline-of-sight communication, we further investigate and propose the use of direct sequence spread spectrum as a means to overcome this, and discuss the use of multipath detection and combining as a suitable mechanism to maximize link reliability."
  },
  {
    "year": "2015",
    "abstract": "This paper investigates the problem of joint multiuser admission control and beamforming optimization for multiple input single-output heterogeneous networks (HetNets). Considered is a HetNet where multiple newly deployed femtocell base stations (FBSs) have the coverage overlapped with that of an existing macrocell base-station (MBS). The design objective is to serve as many femto users (FUEs) as possible at their quality-of-service (QoS) requirements while maintaining the QoS requirements at the macro user (MUE). This paper then proposes three algorithmic schemes to perform multiuser admission control and beamforming optimization based on the levels of coordination between the MBS and FBSs. In Scheme I with full MBS-FBS coordination, a joint optimization framework is presented, and two solution approaches are proposed to determine the admission control and beamforming design for the FUEs in a centralized manner. In Scheme II with limited MBS-FBS coordination, a distributed algorithm that allows each FBS to unilaterally determine its admission control and beamforming strategy is proposed. This algorithm requires a certain coordination from the MBS by setting a limit on the amount of cross-tier intercell interference (ICI) that can be generated by each FBS. The convergence of the proposed distributed algorithm to a fixed point, where the QoS at the MUEs and the admitted FUEs is guaranteed, is then proved. Finally, in Scheme III without any MBS-FBS coordination, a joint multiuser admission control and zero-forcing beamforming design is then proposed. In particular, each FBS distributively admits its own FUEs while suppressing its cross-tier ICI to the MUEs, which effectively maintains the QoS at these MUEs. The simulation results show comparable performances between the distributed algorithms and the centralized ones in terms of the number of FUEs served and the network power usage."
  },
  {
    "year": "2015",
    "abstract": "Using modeling-based design, we demonstrate a tunable nanoelectromechanical system (NEMS) capable of operating in the 800-MHz-to-1.9-GHz frequency band without the need for continuous electrostatic tuning stimuli using reversible structural transitions of solid-state materials. We show that permanent, yet reversible, tuning of such a resonator in this region is possible, but only when the structural support platform is made of ultralight and thin 2-D elements. Using graphene as the top and bottom electrodes with a layer of the well-known phase change material Ge2Sb2Te5, we provide a pathway for highly functional NEMS that employ 2-D electrodes and phase change materials in tunable resonant circuits. Given the recent advances in graphene NEMS, and because the resonator properties are not dependent on the electronic quality, rather the mass of the graphene, such a design would enable the application of tunable phase change NEMS with no active power requirement in a variety of applications in the future."
  },
  {
    "year": "2015",
    "abstract": "A flexible orthogonal frequency division multiplex (OFDM)-based modulation scheme is proposed under the name of Flexible Configured OFDM (FC-OFDM). It enables a flexible subband configuration and targets a multi-service scenario, which is envisioned for future 5G networks. The proposed FC-OFDM scheme provides a good compromise between the filter bank multi-carrier with offset quadrature amplitude modulation and the classical cyclic prefix-based OFDM system. The detailed system structure is illustrated in this paper, together with efficiency evaluations."
  },
  {
    "year": "2015",
    "abstract": "Neighbor discovery was initially conceived as a means to deal with energy issues at deployment, where the main objective was to acquire information about network topology for subsequent communication. Nevertheless, over recent years, it has been facing new challenges due to the introduction of mobility of nodes over static networks mainly caused by the opportunistic presence of nodes in such a scenario. The focus of discovery has, therefore, shifted toward more challenging environments, where connectivity opportunities need to be exploited for achieving communication. In fact, discovery has traditionally been focused on tradeoffs between energy and latency in order to reach an overlapping of communication times between neighboring nodes. With the introduction of opportunistic networking, neighbor discovery has instead aimed toward the more challenging problem of acquiring knowledge about the patterns of encounters between nodes. Many Internet of Things applications (e.g., smart cities) can, in fact, benefit from such discovery, since end-to-end paths may not directly exist between sources and sinks of data, thus requiring the discovery and exploitation of rare and short connectivity opportunities to relay data. While many of the older discovery approaches are still valid, they are not entirely designed to exploit the properties of these new challenging scenarios. A recent direction in research is, therefore, to learn and exploit knowledge about mobility patterns to improve the efficiency in the discovery process. In this paper, a new classification and taxonomy is presented with an emphasis on recent protocols and advances in this area, summarizing issues and ways for potential improvements. As we will show, knowledge integration in the process of neighbor discovery leads to a more efficient scheduling of the resources when contacts are expected, thus allowing for faster discovery, while, at the same time allowing for energy savings when such contacts are not expect..."
  },
  {
    "year": "2015",
    "abstract": "The Internet of Things (IoT) makes smart objects the ultimate building blocks in the development of cyber-physical smart pervasive frameworks. The IoT has a variety of application domains, including health care. The IoT revolution is redesigning modern health care with promising technological, economic, and social prospects. This paper surveys advances in IoT-based health care technologies and reviews the state-of-the-art network architectures/platforms, applications, and industrial trends in IoT-based health care solutions. In addition, this paper analyzes distinct IoT security and privacy features, including security requirements, threat models, and attack taxonomies from the health care perspective. Further, this paper proposes an intelligent collaborative security model to minimize security risk; discusses how different innovations such as big data, ambient intelligence, and wearables can be leveraged in a health care context; addresses various IoT and eHealth policies and regulations across the world to determine how they can facilitate economies and societies in terms of sustainable development; and provides some avenues for future research on IoT-based health care based on a set of open issues and challenges."
  },
  {
    "year": "2015",
    "abstract": "Music is everywhere in the world, and its applications in commerce are extremely versatile. Generally speaking, in order to create some music for background music, it is necessary to engage sound recordists and instrumental performers. However, the process is very time-consuming and costly. In this paper, a real-time emotion-based music accompaniment system is proposed to solve this issue. For different emotions, a fuzzy logic controller is designed to adjust the tempo of the music, and an adaptive partition evolutionary genetic algorithm is developed to create corresponding melodies. The chord progressions are generated via music theory, and the instrumentation is disposed by the conception of the probability. What is noteworthy is that all the processes can be output by Virtual Studio Technology in real time so that users can listen directly to the composing results from any emotions. From the experimental results, the proposed adaptive partition evolutionary genetic algorithm performs better than other optimal algorithms in such topics."
  },
  {
    "year": "2015",
    "abstract": "Ultra-wideband millimeter-wave (mmWave) propagation measurements were conducted in the 28- and 73-GHz frequency bands in a typical indoor office environment in downtown Brooklyn, New York, on the campus of New York University. The measurements provide large-scale path loss and temporal statistics that will be useful for ultra-dense indoor wireless networks for future mmWave bands. This paper presents the details of measurements that employed a 400 Megachips-per-second broadband sliding correlator channel sounder, using rotatable highly directional horn antennas for both co-polarized and cross-polarized antenna configurations. The measurement environment was a closed-plan in-building scenario that included a line-of-sight and non-line-of-sight corridor, a hallway, a cubicle farm, and adjacent-room communication links. Well-known and new single-frequency and multi-frequency directional and omnidirectional large-scale path loss models are presented and evaluated based on more than 14 000 directional power delay profiles acquired from unique transmitter and receiver antenna pointing angle combinations. Omnidirectional path loss models, synthesized from the directional measurements, are provided for the case of arbitrary polarization coupling, as well as for the specific cases of co-polarized and cross-polarized antenna orientations. The results show that novel large-scale path loss models provided here are simpler and more physically based compared to previous 3GPP and ITU indoor propagation models that require more model parameters and offer very little additional accuracy and lack a physical basis. Multipath time dispersion statistics for mmWave systems using directional antennas are presented for co-polarization, crosspolarization, and combined-polarization scenarios, and show that the multipath root mean square delay spread can be reduced when using transmitter and receiver antenna pointing angles that result in the strongest received power. Raw omnidirectional path..."
  },
  {
    "year": "2015",
    "abstract": "An improved adaptive neuro-fuzzy inference system (IANFIS) is proposed to build a model to predict the resonant frequency shift performance of surface acoustic wave (SAW) gas sensors. In the proposed IANFIS, by directly minimizing the root-mean-squared-error performance criterion, the Taguchi-genetic learning algorithm is used in the ANFIS to find both the optimal premise and consequent parameters and to simultaneously determine the most suitable membership functions. The five design parameters of SAW gas sensors are considered to be the input variables of the IANFIS model. The input variables include the number of electrode finger pairs, the electrode overlap, the separation distance of two interdigital transducers on the substrate, the dimensions of the stable temperature-cut (ST-cut) quartz substrate, and the electrode thickness. The output variable of the IANFIS model is composed of the resonant frequency shift performance. The results predicted by the proposed IANFIS are compared with those obtained by the back-propagation neural network. The comparison has shown that the performance prediction of resonant frequency shift using the proposed IANFIS is effective. In addition, the sensitivity analyses of the five design parameters have also shown that both the electrode overlap and the dimensions of the ST-cut quartz substrate have the most influence on the resonant frequency shift performance."
  },
  {
    "year": "2015",
    "abstract": "An electromagnetic wave propagating through the ionosphere is subject to path delay and the depolarizing effect of Faraday rotation, both of which are dependent on global position and geometry. These effects introduce error and consequently reduce the range resolution of remote sensing polarimetric measurements. Satellite-to-ground communications may be adversely altered by these effects so as to inhibit signal reception. The work presented here introduces a simple vectorized model for a large-field-of-view, low-Earth-orbit, satellite system that yields Faraday rotation and path delay according to global position and geometric parameters. Comparison is made with current models, through the simulation of Faraday rotation and path delay. The presented work may extend the range over which Faraday rotation and path delay estimation are reliable. The work presented forms part of a large-field-of-view, low-Earth-orbit satellite model exploiting multiple-input multiple-output polarimetry in three dimensions."
  },
  {
    "year": "2015",
    "abstract": "K nearest neighbor (kNN) queries, which retrieve the k nearest sensor data items associated with a location (location-dependent sensor data) from the location of the query issuer, are useful for location-based services in mobile environments. Here, we focus on the kNN query processing in mobile ad hoc networks (MANETs). Key challenges in designing system protocols for the MANETs include low-overhead adaptability to network topology changes due to node mobility, and query processing that achieves high accuracy of the query result without a centralized server. In this paper, we propose the filling area (FA) method to efficiently process kNN queries in the MANETs. The FA method achieves low overhead in query processing by reducing a search area. In the FA method, data items remain at nodes near the locations with which the items are associated, and nodes cache data items whose locations are near their own so that the query issuer retrieves kNNs from nearby nodes. Through extensive simulations, we verify that our proposed approach achieves low overhead and high accuracy of the query result."
  },
  {
    "year": "2015",
    "abstract": "In response to the new challenges in the design and operation of communication networks, and taking inspiration from how living beings deal with complexity and scalability, in this paper we introduce an innovative system concept called COgnition-BAsed NETworkS (COBANETS). The proposed approach develops around the systematic application of advanced machine learning techniques and, in particular, unsupervised deep learning and probabilistic generative models for system-wide learning, modeling, optimization, and data representation. Moreover, in COBANETS, we propose to combine this learning architecture with the emerging network virtualization paradigms, which make it possible to actuate automatic optimization and reconfiguration strategies at the system level, thus fully unleashing the potential of the learning approach. Compared with the past and current research efforts in this area, the technical approach outlined in this paper is deeply interdisciplinary and more comprehensive, calling for the synergic combination of expertise of computer scientists, communications and networking engineers, and cognitive scientists, with the ultimate aim of breaking new ground through a profound rethinking of how the modern understanding of cognition can be used in the management and optimization of telecommunication networks."
  },
  {
    "year": "2015",
    "abstract": "System-level simulations have become an indispensable tool for predicting the behavior of wireless cellular systems. As exact link-level modeling is unfeasible due to its huge complexity, mathematical abstraction is required to obtain equivalent results by less complexity. A particular problem in such approaches is the modeling of multiple coherent transmissions. Those arise in multiple-input-multiple-output transmissions at every base station but nowadays so-called coordinated multipoint (CoMP) techniques have become very popular, allowing to allocate two or more spatially separated transmission points. Also, multimedia broadcast single frequency networks (MBSFNs) have been introduced recently in long-term evolution (LTE), which enables efficient broadcasting transmission suitable for spreading information that has a high user demand as well as simultaneously sending updates to a large number of devices. This paper introduces the concept of runtime-precoding, which allows to accurately abstract many coherent transmission schemes while keeping additional complexity at a minimum. We explain its implementation and advantages. For validation, we incorporate the runtime-precoding functionality into the Vienna LTE-A downlink system-level simulator, which is an open source tool, freely available under an academic noncommercial use license. We measure simulation run times and compare them against the legacy approach as well as link-level simulations. Furthermore, we present multiple application examples in the context of intrasite and intersite CoMP for train communications and MBSFN."
  },
  {
    "year": "2015",
    "abstract": "This paper focuses on the design of vector perturbation (VP) precoding for coordinated multi-point (CoMP) multi-user downlink transmission. Precoding is performed by individual base stations (BSs) in a distributed manner using only the downlink channel coefficients and user data local to a BS. A cascade precoder structure with an outer precoder managing the inter-cell interference (ICI) and an inner precoder performing mean-squared-error (MSE) minimization-based VP to mitigate the intra-cell interference is proposed. Three different outer precoding techniques are considered. In the first technique, the outer precoder is designed to fully eliminate the ICI by trading off the degrees of freedom (DoFs) available through multiple antennas. While the proposed technique outperforms existing conventional-VP based designs, a large portion of DoF is consumed by the ICI elimination. To overcome this issue, in the second technique, interference alignment-based outer precoding that minimizes the total leakage interference is proposed. To further improve the system performance, in the third approach, precoding by joint minimization of total leakage interference plus MSE is performed. Numerical results show that the proposed cascade precoding structure is an efficient way to use the DoF of CoMP multi-user downlink transmission."
  },
  {
    "year": "2015",
    "abstract": "Future 5G mobile network architecture is expected to offer capacities to accommodate the inexorable rise in mobile data traffic and to meet further stringent latency and reliability requirements to support diverse high data rate applications and services. Mobile cloud computing (MCC) in 5G has emerged as a key paradigm, promising to augment the capability of mobile devices through provisioning of computational resources on demand, and enabling resource-constrained mobile devices to offload their processing and storage requirements to the cloud infrastructure. Follow-me cloud (FMC), in turn, has emerged as a concept that allows seamless migration of services according to the corresponding users mobility. Meanwhile, software-defined networking (SDN) is a new paradigm that permits the decoupling of the control and data planes of traditional networks and provides programmability and flexibility, allowing the network to dynamically adapt to change traffic patterns and user demands. While the SDN implementations are gaining momentum, the control plane is still suffering from scalability and performance concerns for a very large network. In this paper, we address these scalability and performance issues in the context of 5G mobile networks by introducing a novel SDN/OpenFlow-based architecture and control plane framework tailored for MCC-based systems and more specifically for FMC-based systems where mobile nodes and network services are subject to constraints of movements and migrations. Contrary to a centralized approach with a single SDN controller, our approach permits the distribution of the SDN/OpenFlow control plane on a two-level hierarchical architecture: a first level with a Global FMC Controller (G-FMCC), and a second level with several Local FMC Controllers (L-FMCCs). Thanks to our control plane framework and Network Function Virtualization (NFV) concept, the L-FMCCs are deployed on-demand, where and when needed, depending on the global system load. Results obt..."
  },
  {
    "year": "2015",
    "abstract": "A new medical image segmentation pipeline for accurate bone segmentation from computed tomography (CT) imaging is proposed in this paper. It is a two-step methodology, with a pre-segmentation step and a segmentation refinement step, as follows. First, the user performs a rough segmenting of the desired region of interest. Second, a fully automatic refinement step is applied to the pre-segmented data. The automatic segmentation refinement is composed of several sub-steps, namely, image deconvolution, image cropping, and interpolation. The user-defined pre-segmentation is then refined over the deconvolved, cropped, and up-sampled version of the image. The performance of the proposed algorithm is exemplified with the segmentation of CT images of a composite femur bone, reconstructed with different reconstruction protocols. Segmentation outcomes are validated against a gold standard model, obtained using the coordinate measuring machine Nikon Metris LK V20 with a digital line scanner LC60-D and a resolution of28μm. High sub-pixel accuracy models are obtained for all tested data sets, with a maximum average deviation of 0.178 mm from the gold standard. The algorithm is able to produce high quality segmentation of the composite femur regardless of the surface meshing strategy used."
  },
  {
    "year": "2015",
    "abstract": "This paper proposes a new computational method for retrieving shapes under unpredictable conditions such as when occlusion, geometric distortion, and differences in image resolution occur simultaneously. The human visual system retrieves shapes from incomplete information in the real world, and it has inspired a lot of computational methods of retrieving shapes. In order to retrieve shapes, the observed shapes are decided to be alike or unlike remembered shapes in memory after the comparison of these shapes. To compare the observed and remembered shapes, they must first be appropriately represented so that the points on each shape can be mapped and compared. For this reason, the shape retrieval process needs an appropriate shape representation and shape mapping methods. Moreover, the shape representations should be normalized before the mapping process. However, a normalization process for representations under unpredictable conditions has not yet been established. In this paper, we describe a shape retrieval method that enables us to retrieve shapes under unpredictable conditions with a suitable normalization process. Using curvature partition and angle-length profile, our shape retrieval method normalizes the shape representation before it does the mapping. As a result, unlike the previously proposed methods, it can be used under unpredictable conditions such as when occlusion, geometric distortion, and differences in image resolution occur simultaneously."
  },
  {
    "year": "2015",
    "abstract": "With rapid economic growth, the management of employees working outdoors gradually becomes more intensive. The traditional attendance systems typically focus on collecting employee attendance information and are not suited to the quantification of work performance. As a remedy to this issue, a smart work performance measurement system is proposed in this paper. The proposed system consists of three components: 1) a smartphone-based APP to collect employee attendance, work, and location information; 2) a data warehouse to preprocess and store the data; and 3) a smart data analysis center to make a comprehensive and systematic evaluation of employee work performance. Management can obtain the key information quickly and adjust work assignments based on performance. New solutions and algorithms for indoor and outdoor location, GPS deviation improvement, and work performance measurement are put forward. This system is being used by over 1100 police officers in one city traffic management bureau, and the observed results are encouraging, demonstrating the efficiency and accuracy of our system, and helping governments to better regulate the traffic operation and reduce associated costs."
  },
  {
    "year": "2015",
    "abstract": "Industrial wireless sensor networks have attracted much attention as a cornerstone to making the smart factories real. Utilizing industrial wireless sensor networks as a base for smart factories makes it possible to optimize the production line without human resources, since it provides industrial Internet of Things service, where various types of data are collected from sensors and mined to control the machines based on the analysis result. On the other hand, a fog computing node, which executes such real-time feedback control, should be capable of real-time data collection, management, and processing. To achieve these requirements, in this paper, we introduce wireless computing system (WCS) as a fog computing node. Since there are a lot of servers and each server has 60 GHz antennas to connect to other servers and sensors, WCS has high collecting and processing capabilities. However, in order to fulfill a demand for real-time feedback control, WCS needs to satisfy an acceptable delay for data collection. In addition, lower power consumption is required in order to reduce the cost for the factory operation. Therefore, we propose an energy-efficient and delay-aware WCS. Since there is a tradeoff relationship between the power consumption and the delay for data collection, our proposed system controls the sleep schedule and the number of links to minimize the power consumption while satisfying an acceptable delay constraint. Furthermore, the effectiveness of our proposed system is evaluated through extensive computer simulations."
  },
  {
    "year": "2015",
    "abstract": "The Kish Key Distribution (KKD) system has been proposed as a classical alternative to quantum key distribution, making use of temperature-matched thermal noise. Previous analyses assume instant propagation of signals along the cable connecting the two users. We describe a new attack that takes an advantage of propagation delays. At the start of each bit period, the noise temperature will then be increased from zero to its final value. During this process, the noise temperature variation will take time to propagate along the line, resulting in a temperature mismatch. We analyze the information leak due to this effect and consider several potential mitigation schemes."
  },
  {
    "year": "2015",
    "abstract": "Several models have been previously suggested for learning correlated representations between source and target modalities. In this paper, we propose a novel coupled autoassociative neural network for learning a target-to-source image representation for heterogenous face recognition. This coupled network is unique, because a cross-modal transformation is learned by forcing the hidden units (latent features) of two neural networks to be as similar as possible, while simultaneously preserving information from the input. The effectiveness of this model is demonstrated using multiple existing heterogeneous face recognition databases. Moreover, the empirical results show that the learned image representation-common latent features-by the coupled auto-associative produces competitive cross-modal face recognition results. These results are obtained by training a softmax classifier using only the latent features from the source domain and testing using only the latent features from the target domain."
  },
  {
    "year": "2015",
    "abstract": "The five senses of sight, sound, smell, touch, and taste define the world for us. Sensing technologies augment these primary sensing capabilities: Mechanical sensors broaden our perception of the world by being sensitive to objects that we cannot ‘sense’ and then translate it in a format so that we can. A false color-map of galaxies produced by a radio-telescope, the trajectories in a bubble-chamber of a subatomic particle, the sound of a carbon monoxide alarm, or an MRI image of the brain are all translations of information adapted specifically for us. It is hardly surprising, therefore, that the history of sensors parallels the development of science and technology."
  },
  {
    "year": "2015",
    "abstract": "An increasing number of mobile users (MUs) share common interests in general information, such as traffic information, weather forecast, and domestic/international news. However, the centralized infrastructure-based system has not been designed for efficiently disseminating the information of common interest (IoCI) to numerous requesters. Thanks to the rapid development of mobile devices equipped with large storage and multiple communication modes, opportunistic communication between a pair of MUs can be readily realized. With the aid of opportunistic networks formed by MUs, we can improve the connectivity of cellular networks in the rural areas, we can offload the tele-traffic from the overloaded cellular networks, and we can efficiently disseminate the IoCI in the densely populated areas. We commence with a detailed survey on the cross-disciplinary research area of social network analysis aided telecommunication networking. We continue by focusing our attention on the family of integrated cellular and large-scale opportunistic networks, whose performance is dominated by the inter-contact duration as well as the contact duration between any pairs of MUs. A continuous-time-pure-birth Markov chain is invoked for analyzing the relevant performance. We demonstrate that the delivery ratio of the IoCI before it expires becomes higher than 90% with the aid of opportunistic networks consisting of 20 MUs. Moreover, our experiments based on the InfoCom 2006 mobility trace show that the opportunistic networks are capable of offloading 58% of the tele-traffic from the cellular networks. Thereafter, we propose a hybrid information dissemination scheme for the integrated cellular and small-scale opportunistic networks, which is comprised of two main stages: 1) the base station-aided single-hop multicast (BSSHM) stage and 2) the cooperative multicast aided spontaneous dissemination stage. In contrast to their large-scale counterparts, in small-scale opportunistic networks, the in..."
  },
  {
    "year": "2015",
    "abstract": "Emergency navigation algorithms for evacuees in confined spaces typically treat all evacuees in a homogeneous manner, using a common metric to select the best exit paths. In this paper, we present a quality of service (QoS) driven routing algorithm to cater to the needs of different types of evacuees based on age, mobility, and level of resistance to fatigue and hazard. Spatial information regarding the location and the spread of hazards is also integrated into the routing metrics to avoid situations where evacuees may be directed toward hazardous zones. Furthermore, rather than persisting with a single decision algorithm during an entire evacuation process, we suggest that evacuees may adapt their course of action with regard to their ongoing physical condition and environment. A widely tested routing protocol known as the cognitive packet network with random neural networks and reinforcement learning are employed to collect information and provide advice to evacuees, and is beneficial in emergency navigation owing to its low computational complexity and its ability to handle multiple QoS metrics in its search for safe exit paths. The simulation results indicate that the proposed algorithm, which is sensitive to the needs of evacuees, produces better results than the use of a single metric. Simulations also show that the use of dynamic grouping to adjust the evacuees' category, and routing algorithms that have regard for their on-going health conditions and mobility, can achieve higher survival rates."
  },
  {
    "year": "2015",
    "abstract": "We present an automatic parameter setting method to achieve an accurate second-order Kalman filter tracker based on a steady-state performance index. First, we propose an efficient steady-state performance index that corresponds to the root-mean-square (rms) prediction error in tracking. We then derive an analytical relationship between the proposed performance index and the generalized error covariance matrix of the process noise, for which the automatic determination using the derived relationship is presented. The model calculated by the proposed method achieves better accuracy than the conventional empirical model of process noise. Numerical analysis and simulations demonstrate the effectiveness of the proposed method for targets with accelerating motion. The rms prediction error of the tracker designed by the proposed method is 63.8% of that with the conventional empirically selected model for a target accelerating at 10 m/s2."
  },
  {
    "year": "2015",
    "abstract": "Big data analytics applied to signaling, traffic, and wireless environment data in mobile communication networks can help realize autonomous network optimization and build big data-based network operation. In this paper, a signaling-based intelligent network optimization scheme is introduced and applied to the current mobile communication networks, such as 4G Long Term Evolution. In 5G era, big data analytics can help mine user and service requirements from the radio access network level, thus allowing a more efficient 5G design and operation. This paper illustrates how it would significantly facilitate local content provision, dynamical network and functionality deployment, user behavior awareness, fine-tuned network operation, and globally optimized energy saving solutions. It is anticipated that the big data-based 5G network design, and the operation will be greener and softer, and better meet the ever increasing user-centric requirements of mobile communication."
  },
  {
    "year": "2015",
    "abstract": "Our reports of published research in several of the peer-reviewed journal articles in 1996, 2002, and 2004 have generated a lot of controversy over the last two decades, including the most recent publication by Foster and Chou. In this paper, we present arguments based on physics that the main reason for higher exposure of children (also women and men with smaller heads and likely thinner pinnae) to radiofrequency energy from mobile phones is the closer placement of the cell phone radiation source by several millimeters to the tissues of the head, e.g., the brain. Using heterogeneous anatomically derived shaped models of the head, we have previously reported that the exposure increases by a compounding rate of 10%–15% for every single millimeter of closer location of the radiating antenna. This is similar to the report of∼20% increase for every millimeter in the Foster and Chou’s paper from their (1) even though their simplistic (1) is valid only for a homogenous tissue slab of infinite size and the radiation source that is a wire dipole rather than a mobile telephone. Both of their assumptions for (1) are obviously not applicable for human exposures to mobile telephones. Actually, the physical reason for such a rapid drop off of coupled energy is that the radiofrequency electromagnetic fields close to a radiating source in the so-called near-field region reduce in strength very rapidly with every millimeter of distance, even faster than in the far-field region, where the electromagnetic fields reduce inversely with the square of the distance from the source."
  },
  {
    "year": "2015",
    "abstract": "Wireless sensor nodes have a wide span of applications ranging from industrial monitoring to military operations. These nodes are highly constrained in terms of battery life, processing capabilities, and in-built memory. Industrial wireless sensor networks (IWSNs) have to meet the constraints and peculiarities of industrial environments to ensure synchronization with parallel production processes. Applications of WSNs in industrial communication vary from condition monitoring and sensing to process automation. The 6LoWPAN standard enables efficient utilization of IPv6 protocol over low-power wireless personal area networks (LoWPANs). The use of 6LoWPANs for industrial communication necessitates the fulfillment of special QoS and security. We examine the aspect of secured information dissemination for industrial control and automation processes in this paper. Researchers have proposed several schemes to secure transfer of data over the Internet. Public key infrastructure (PKI) is one of the most popular security schemes being used in the present scenario. The hostile deployment scenarios of 6LoWPANs and resource constraints of the nodes necessitate the presence of a robust security mechanism to safeguard the communication. In this paper, we propose an integration scheme for PKI and 6LoWPAN to meet the enhanced security needs of industrial communication. The approach is to delegate a major portion of key management activity to the edge routers (gateway) of the LoWPAN and limit the involvement of the end nodes to minimal communication with the edge router. We do not propose a change in the current PKI, but we put forth a scheme to facilitate the integration of PKI to 6LoWPAN in an efficient manner. The effectiveness of the proposed algorithm was evaluated using a protocol analyzer for normal 6LoWPAN traffic as well as HUI HC-01 compressed traffic. A marginal increase of 2% in channel utilization was observed, which scaled down to 1% using HUI HC-01 compression. The res..."
  },
  {
    "year": "2015",
    "abstract": "Personalization approaches seek to estimate user preferences in order to recommend content or social network connections, or to serve personalized advertisements to users. Such approaches are being increasingly adopted by organizations to build customized personalization applications. Leveraging the growing popularity of Web videos for such approaches necessitates the ability to classify Web videos into application-specific categories, since different applications are interested in different aspects of the user preferences. A key requirement of supervised classification models to address this is the availability of training videos labeled to the arbitrary application-specific categories. In order to address this requirement, we propose a completely automated framework to obtain training Web videos for arbitrary categories, which does not rely on any manual labeling of videos. This is achieved utilizing keywords to retrieve training videos, thereby simplifying the problem of obtaining training videos to the problem of selecting keywords to retrieve them. We show that there are two opposing objectives (proximity and diversity) that need to be considered while developing such keyword selection techniques. We propose two efficient approaches (linear combination of proximity and diversity and annealing-based alternating optimization) and study the tradeoffs between them, with respect to performance and the human input required to tune parameters of the approach. Through experiments over several sets of categories, we demonstrate the feasibility of the automated framework to select training videos for application-specific categorization. We also show that the proposed approaches lead to a substantial improvement in the performance of classification models, as compared with other automated methods."
  },
  {
    "year": "2015",
    "abstract": "With the explosive growth of mobile data traffic and rapidly rising energy price, how to implement caching at small cells in an energy-efficient way is still an open problem and requires further research efforts. In this paper, we study the energy-efficient context-aware resource allocation problem, which falls into the category of mixed integer nonlinear programming (MINLP) and is NP-hard. To provide a tractable solution, the MINLP problem is decoupled and reformulated as a one-to-one matching problem under two-sided preferences, which are modeled as the maximum energy efficiency that can be achieved under the expected matching. An iterative algorithm is developed to establish preference profiles by employing nonlinear fractional programming and Lagrange dual decomposition. Then, we propose an energy-efficient matching algorithm based on the Gale-Shapley algorithm, and provide the detailed discussion and analysis of stability, optimality, implementation issues, and algorithmic complexity. The proposed matching algorithm is also extended to scenarios with preference, indifference, and incomplete preference lists by introducing some tie-breaking and preference deletion rules. The simulation results demonstrate that the proposed algorithm achieves significant performance and satisfaction gains compared with the conventional algorithms."
  },
  {
    "year": "2015",
    "abstract": "The aim of this paper is to present results on output power level distributions of radio base stations (RBSs) and user devices connected to a wideband code division multiple access-based third generation (3G) mobile communication network in India and relate the results to realistic human exposure to radio frequency (RF) electromagnetic field (EMF) emitted by the corresponding RBSs and the devices. The output power level distributions have been obtained through network-based measurements. In downlink, data from 868 RBSs were gathered during seven days. The RBSs were connected to five different radio network controllers (RNCs) located in different regions of India. The mean, the median, and the 95th percentile RBS output power values were found to be 24%, 21%, and 53%, respectively, of the maximum available power. In the uplink direction, output power levels of 3G devices connected to 1256 RBSs and the same five RNCs as in the downlink were assessed separately for voice, data, voice + data, and video applications. In total, more than 1 million hours of data traffic and more than 700 000 h of voice calls were measured in the uplink. The mean output power for the voice, data, the voice + data, and the video were found to be around 1%, 3%, 2%, and 4%, respectively, of the maximum available power for the 3G user devices. The findings are in line with previously published results obtained in other networks in Europe, and demonstrate that knowledge on realistic power levels is important for accurate assessments of the RF EMF exposure."
  },
  {
    "year": "2015",
    "abstract": "The greater vulnerability of children to the effects of environmental hazards has raised concerns about their exposure to and the resultant absorption of mobile phone radiation. Foster and Chou (2014) reviewed published studies that used computer models of radio-frequency electromagnetic fields to estimate and compare the tissue dose rate in the heads of children and adults using mobile phones. Their review confuses exposure with absorption, and the study results conclude erroneously that children are not more exposed than adults. We show that their review was not executed systematically. There are discrepancies between text summaries and the graphed ratios of child: adult peak special specific absorption rate, in line with the author's hypothesis that children have the same or lower tissue dose than adults. Even the underlying precept of their review is flawed, as the results of deterministic models are treated as random variables. In fact, model results are entirely determined by the underlying assumptions and the structure of the model. Models are included in their unsystematic review that do not consider differences in dielectric constants among different tissues, or across ages, while other models that consider such differences are not included. In this paper, we discuss the differences between exposure and tissue absorption and re-examine the results presented by Foster and Chou. Based upon our review, we suggest an alternative interpretation of the published literature. In an Appendix, we discuss modeling of tissue dose in the context of governmental safety certification processes."
  },
  {
    "year": "2015",
    "abstract": "Virtual private LAN service (VPLS) is a Layer 2 virtual private network technique that has gained enormous popularity in industrial networks. However, the deployment of legacy VPLS architectures in large-scale networks is challenging due to unresolved security and scalability issues. In this paper, we propose a novel hierarchical VPLS architecture based on host identity protocol. The proposed architecture tackles both security and scalability issues in legacy VPLS architectures. It secures the VPLS network by delivering vital security features such as authentication, confidentiality, integrity, availability, and secured control protocol. The security analysis and simulation results confirm that the proposed architecture is protected from various IP-based attacks as well. Theoretical analysis and simulation results have also verified that the proposed architecture provides scalability in control, forwarding, and security planes. Finally, the data plane performance of the proposed architecture is measured in a real-world testbed implementation."
  },
  {
    "year": "2015",
    "abstract": "A method for detecting a low observable target track using an acceleration-based overall motion model is proposed. Unlike the existing track-before-detect methods that are based on sequential state updates, this method computes integrated echo energy for the entire hypothesized motion. The detection and the estimation of the track are made simultaneously using the batch processing approach. A comparison of track detection probability shows higher performance against low observable targets. Using a motion similarity metric and motion model homogeneity, a performance prediction model is derived and compared with the simulation results."
  },
  {
    "year": "2015",
    "abstract": "The spatial distribution of base stations (BSs) and traffic demands is essential for efficient network planning and BS sleeping, which are key elements of green cellular networking. This paper investigates their statistics, relation, and modeling, based on large-scale measurement data from commercial cellular networks. The spatial distribution of BSs shows not only high nonuniformity over a region but also diverse patterns in different regions, and thus the widely used homogeneous Poisson point process can only approximate the BS patterns in a specific small area. Therefore, the inhomogeneous PPP (IPPP), in particular, the Cox point process with spatially varying intensity is used to model the BS distribution over any spatial scale. To model the intensity distribution of the IPPP, we exploit the relation, shown to be sublinear, between the BS distribution and the peak hour (PH) traffic density, based on the finding that the PH traffic density can be approximated by a log-normal distribution. Finally, we propose a spatial modeling framework for network simulations, and discuss potential applications of the proposed spatial distribution model of the BS patterns and the PH traffic density."
  },
  {
    "year": "2015",
    "abstract": "This paper introduces a novel algorithm that increases the efficiency of the current cloud-based smart-parking system and develops a network architecture based on the Internet-of-Things technology. This paper proposed a system that helps users automatically find a free parking space at the least cost based on new performance metrics to calculate the user parking cost by considering the distance and the total number of free places in each car park. This cost will be used to offer a solution of finding an available parking space upon a request by the user and a solution of suggesting a new car park if the current car park is full. The simulation results show that the algorithm helps improve the probability of successful parking and minimizes the user waiting time. We also successfully implemented the proposed system in the real world."
  },
  {
    "year": "2015",
    "abstract": "The use of cryptographic techniques such as encryption and hashing largely increases the energy consumption of sensors, which aggravates the original critical energy constraint problem of wireless sensor networks (WSNs). To reduce the burden of sensors, compression can be utilized. Since the traditional chaos-based schemes are not directly applicable for WSNs, we present a hybrid security solution. The hybrid security consists of 8-bit integer chaotic block encryption and a chaos-based message authentication codes. It aims to promote the security and performance of data gathering. In this paper, a hybrid security and compressive sensing-based scheme for multimedia sensor data gathering is presented. It has light security mechanism and thus decreases the complexity and energy consumption of system. Performance analysis about security and compression is carried out. The results show that our scheme is more applicable for WSNs multimedia data gathering from security and compression efficiency."
  },
  {
    "year": "2015",
    "abstract": "In this paper, a new evaluation criterion is developed to investigate the performance of non-orthogonal multiple access (NOMA) from an information theoretic point of view. In particular, the relations among the capacity region of the broadcast channel and two rate regions achieved by NOMA and time-division multiple access (TDMA) are first illustrated. Based on these relations, a new evaluation criterion is proposed for NOMA in wireless fading scenarios, where the key idea is to compare NOMA with TDMA statistically in terms of not only the sum rate but also the individual rates. In a wireless downlink scenario with user pairing, the developed analytical results show that NOMA can outperform TDMA not only in terms of the sum rate but also in terms of each user's individual rate, particularly when the difference between the users' channels is large. The optimal power allocation for a special case of NOMA user pairing is also established."
  },
  {
    "year": "2015",
    "abstract": "In this paper, we study the joint pilot assignment and resource allocation for system energy efficiency (SEE) maximization in the multi-user and multi-cell massive multi-input multi-output network. We explicitly consider the pilot contamination effect during the channel estimation in the SEE maximization problem, which aims to optimize the power allocation, the number of activated antennas, and the pilot assignment. To tackle the SEE maximization problem, we transform it into a subtractive form, which can be solved more efficiently. In particular, we develop an iterative algorithm to solve the transformed problem where optimization of power allocation and number of antennas is performed, and then pilot assignment optimization is conducted sequentially in each iteration. To tackle the first sub-problem, we employ a successive convex approximation (SCA) technique to attain a solvable convex optimization problem. Moreover, we propose a novel iterative low-complexity algorithm based on the Hungarian method to solve the pilot assignment sub-problem. We also describe how the proposed solution approach can be useful to address the sum rate (SR) maximization problem. In addition to the algorithmic developments, we characterize the optimal structure of both SEE and SR maximization problems. The numerical studies are conducted to illustrate the convergence of the proposed algorithms, impacts of different parameters on the SR and SEE, and significant performance gains of the proposed solution compared the conventional design."
  },
  {
    "year": "2015",
    "abstract": "Wireless sensor networks (WSNs) are anticipated to be widely adopted in the various monitoring and control applications due to their versatility and low cost. One of the most promising and emerging WSNs applications is their use in monitoring smart grid assets. Although WSNs can provide cost efficient and reliable solutions, they are not suitable for delay critical application, because they were initially designed for low data rate applications and they may be challenged when sudden faults or failures occur in the monitored environments. Therefore, to prevent extensive delays of critical data, appropriate quality of service (QoS) techniques should be used. In this paper, we present an adaptive QoS scheme (AQoS) and an adaptive guaranteed time slot (AGTS) allocation scheme for IEEE 802.15.4-based WSNs used in high traffic intensity smart grid monitoring applications. Both AQoS and AGTS schemes can adaptively reduce the end-to-end delay and flexibly tune the GTS to provide the required QoS differentiation to delay critical smart grid monitoring applications."
  },
  {
    "year": "2015",
    "abstract": "Routing in wireless multihop networks (WMHNs) relies on a delicate balance of diverse and often conflicting parameters, when aiming for maximizing the WMHN performance. Classified as a non-deterministic polynomial-time hard problem, routing in WMHNs requires sophisticated methods. As a benefit of observing numerous variables in parallel, quantum computing offers a promising range of algorithms for complexity reduction by exploiting the principle of quantum parallelism (QP), while achieving the optimum full-search-based performance. In fact, the so-called non-dominated quantum optimization (NDQO) algorithm has been proposed for addressing the multiobjective routing problem with the goal of achieving a near-optimal performance, while imposing a complexity of the order of O(N) and O(N√N) in the best and worst case scenarios, respectively. However, as the number of nodes in the WMHN increases, the total number of routes increases exponentially, making its employment infeasible despite the complexity reduction offered. Therefore, we propose a novel optimal quantum-assisted algorithm, namely, the non-dominated quantum iterative optimization (NDQIO) algorithm, which exploits the synergy between the hardware and the QP for the sake of achieving a further complexity reduction, which is on the order of O(√N) and O(N√N) in the best and worst case scenarios, respectively. In addition, we provide simulation results for demonstrating that our NDQIO algorithm achieves an average complexity reduction of almost an order of magnitude compared with the near-optimal NDQO algorithm, while having the same order of power consumption."
  },
  {
    "year": "2015",
    "abstract": "Recently, the renewable distributed energy resources (DERs) have become more and more popular due to carbon-free energy sources and environment-friendly electricity generation. Unfortunately, these power generation patterns are mostly intermittent in nature and distributed over the electrical grid, which creates challenging problems in the reliability of the smart grid. Thus, the smart grid has a strong requisite for an efficient communication infrastructure to facilitate estimating the DER states. In contrast to the traditional methods of centralized state estimation (SE), we propose a distributed approach to microgrid SE based on the concatenated coding structure. In this framework, the DER state is treated as a dynamic outer code, and the recursive systematic convolutional (RSC) code is seen as a concatenated inner code for protection and redundancy in the system states. Furthermore, in order to properly monitor the intermittent energy source from any place, this paper proposes a distributed SE method. Particularly, the outputs of the local SE are treated as measurements, which are fed into the master fusion station. At the end, the global SE can be obtained by combining local SEs with corresponding weighting factors. The weighting factors can be calculated by inspiring the covariance intersection method. The simulation results show that the proposed method is able to estimate the system state properly."
  },
  {
    "year": "2015",
    "abstract": "In this paper, we investigate the design of a radio resource control (RRC) protocol in the framework of long-term evolution (LTE) of the 3rd Generation Partnership Project regarding provision of low cost/complexity and low energy consumption machine-type communication (MTC), which is an enabling technology for the emerging paradigm of the Internet of Things. Due to the nature and envisaged battery-operated long-life operation of MTC devices without human intervention, energy efficiency becomes extremely important. This paper elaborates the state-of-the-art approaches toward addressing the challenge in relation to the low energy consumption operation of MTC devices, and proposes a novel RRC protocol design, namely, semi-persistent RRC state transition (SPRST), where the RRC state transition is no longer triggered by incoming traffic but depends on pre-determined parameters based on the traffic pattern obtained by exploiting the network memory. The proposed RRC protocol can easily co-exist with the legacy RRC protocol in the LTE. The design criterion of SPRST is derived and the signalling procedure is investigated accordingly. Based upon the simulation results, it is shown that the SPRST significantly reduces both the energy consumption and the signalling overhead while at the same time guarantees the quality of service requirements."
  },
  {
    "year": "2015",
    "abstract": "In this paper, we propose a variation-based method to linearize the nonlinear dynamics of robotic systems, whose configuration spaces contain the manifoldsS2andSO(3), along dynamically feasible reference trajectories. The proposed variation-based linearization results in an implicitly time-varying linear system, representing the error dynamics, that is globally valid. We illustrate this method through three different systems: 1) a 3-D pendulum: 2) a spherical pendulum; and 3) a quadrotor with a suspended load, whose dynamics evolve onSO(3),S2, andSE(3)×S2, respectively. We show that for these systems, the resulting time-varying linear system obtained as the linearization about a reference trajectory is controllable for all possible reference trajectories. Finally, a linear quadratic regulator-based controller is designed to attenuate the error so as to locally exponentially stabilize tracking of a reference trajectory for the nonlinear system. Several simulations results are provided to validate the effectiveness of this method."
  },
  {
    "year": "2015",
    "abstract": "Wireless sensor networks (WSNs) are a prominent fundamental technology of the Internet of Things (IoTs). Rather than device-to-device communications, group communications in the form of broadcasting and multicasting incur efficient message deliveries among resource-constrained sensor nodes in the IoT-enabled WSNs. Secure and efficient key management is in many cases used to protect the authenticity, integrity, and confidentiality of multicast messages. This paper develops two group key establishment protocols for secure multicast communications among the resource-constrained devices in IoT. Major deployment conditions and requirements of each protocol are described in terms of the specific IoT application scenarios. Furthermore, the applicability of the two protocols is analyzed and justified by a comprehensive analysis of the performance, scalability, and security of the protocols proposed."
  },
  {
    "year": "2015",
    "abstract": "Citation recommendation is an interesting and significant research area as it solves the information overload in academia by automatically suggesting relevant references for a research paper. Recently, with the rapid proliferation of information technology, research papers are rapidly published in various conferences and journals. This makes citation recommendation a highly important and challenging discipline. In this paper, we propose a novel citation recommendation method that uses only easily obtained citation relations as source data. The rationale underlying this method is that, if two citing papers are significantly co-occurring with the same citing paper(s), they should be similar to some extent. Based on the above rationale, an association mining technique is employed to obtain the paper representation of each citing paper from the citation context. Then, these paper representations are pairwise compared to compute similarities between the citing papers for collaborative filtering. We evaluate our proposed method through two relevant real-world data sets. Our experimental results demonstrate that the proposed method significantly outperforms the baseline method in terms of precision, recall, and F1, as well as mean average precision and mean reciprocal rank, which are metrics related to the rank information in the recommendation list."
  },
  {
    "year": "2015",
    "abstract": "As wireless communication has become completely integrated into people's day-to-day lives, so has its reliance on wireless connectivity. The Centers for Disease Control reports that in 2013 39.4% of the U.S. population did not own a traditional wired telephone. While the convenience of a wireless phone is clear, it has created a situation where it is the only communication device that people have during a natural disaster. Far too often during disasters communications with these devices fail, often occurring in one of two ways. First, natural disasters destroy the network hardware that provides the required network connectivity. Second, as people need these devices for safety and security, the network that these devices utilize becomes overloaded. The research in this paper provides evidence that there is a statistically measurable increase in wireless communication traffic during a severe winter storm. This empirical study explores the increase of voice minutes of use and text messaging during a severe winter storm."
  },
  {
    "year": "2015",
    "abstract": "Large power systems are normally operated with their neutral points directly earthed. At a major generating or switching station, this results in the provision of a large earth grid buried in the ground. The design of earthing systems requires a worst case approach. There is a possibility of heavy currents flowing into the earth grid from the overhead earth wires through the tower during a line conductor fault and from lightning strikes. The flow of earth current during a fault or lightning conditions results in a rise of earth grid potential with respect to a physically remote earth point, which can lead to unsafe conditions under some conditions for personnel and connected electrical plant. This paper aims to investigate the potential of adding novel coatings to the conventional copper earth grid conductors to enhance overall conductivity and diminish corrosion. This contributes to lowering the rise of earth grid potential. Graphene-coated copper performance as an earth grid conductor is evaluated with staged low voltage fault and the corrosion behavior in both a destructive and nondestructive environment. A comparison of the simulation software packages CDEGS and CST is also carried out using lightning strike conditions."
  },
  {
    "year": "2015",
    "abstract": "The construction of a battlefield surveillance system is very important for monitoring the attack of enemy aircrafts and missiles, which integrates various sensors and mobile devices. Then, multiple battlefield surveillance systems can be connected together to form a battlefield surveillance network. The mobile nodes can be deployed in a certain region to monitor enemy aircrafts and missiles. Thus, some important issues have to be solved efficiently, including the cooperation across the administrative domains of a cloud network, the direction-of-arrival (DOA), and a polarization estimation algorithm for a mobile wireless sensor network (MWSN). In this paper, the architecture of a battlefield surveillance system is constructed based on mobile cloud computing and 5G link. The root multiple signal classification (Root-MUSIC)-like algorithm is proposed for estimating the 1-D DOA and a polarization parameter with a uniform linear array. The Root-MUSIC algorithm is replaced by the Fourier transform, the former algorithm that can be extended to an arbitrary topology structure of a MWSN. Then, the proposed algorithm is extended to the 2-D DOA and a polarization estimation in further. Based on the deployment of different MWSNs, the estimation results of DOA and polarization parameters are fused in order to improve the estimation performance. Finally, the parameter information (DOA and polarization parameter) of enemy aircrafts and missiles can be achieved. The computer simulation verifies the effectiveness of the proposed algorithm. The proposed algorithm ensures the parameter estimation accuracy with a low computational complexity."
  },
  {
    "year": "2015",
    "abstract": "This paper presents an open course in the University Network of Interactive Laboratories, which offers several virtual and remote laboratories on automatic control, accessible to anyone. All the details on one of these labs (a two electric coupled drives system that allows performing control practices in a 2 × 2 MIMO system with industrial applications) and the activities that can be performed with it are given. We use a low-cost solution for developing the virtual and remote labs shared in this open course, based on the use of a free authoring tool Easy Java/Javascript Simulations (EJsS) for building the laboratories' user interfaces and a cheap development platform board (BeagleBone Black). The virtual and remote labs are deployed into a free Learning Management System (Moodle) Web environment that facilitates their management and maintenance."
  },
  {
    "year": "2015",
    "abstract": "In this paper, we approach the problem of forecasting a time series (TS) of an electrical load measured on the Azienda Comunale Energia e Ambiente (ACEA) power grid, the company managing the electricity distribution in Rome, Italy, with an echo state network (ESN) considering two different leading times of 10 min and 1 day. We use a standard approach for predicting the load in the next 10 min, while, for a forecast horizon of one day, we represent the data with a high-dimensional multi-variate TS, where the number of variables is equivalent to the quantity of measurements registered in a day. Through the orthogonal transformation returned by PCA decomposition, we reduce the dimensionality of the TS to a lower number k of distinct variables; this allows us to cast the original prediction problem in k different one-step ahead predictions. The overall forecast can be effectively managed by k distinct prediction models, whose outputs are combined together to obtain the final result. We employ a genetic algorithm for tuning the parameters of the ESN and compare its prediction accuracy with a standard autoregressive integrated moving average model."
  },
  {
    "year": "2015",
    "abstract": "Rule induction is a practical approach to knowledge discovery. Provided that a problem is developed, rule induction is able to return the knowledge that addresses the goal of this problem as if-then rules. The primary goals of knowledge discovery are for prediction and description. The rule format knowledge representation is easily understandable so as to enable users to make decisions. This paper presents the potential of rule induction for energy efficiency. In particular, three rule induction techniques are applied to derive knowledge from a dataset of thousands of Irish electricity customers' time-series power consumption records, socio-demographic details, and other information, in order to address the following four problems: 1) discovering mathematically interesting knowledge that could be found useful; 2) estimating power consumption features for customers, so that personalized tariffs can be assigned; 3) targeting a subgroup of customers with high potential for peak demand shifting; and 4) identifying customer attitudes that dominate energy conservation."
  },
  {
    "year": "2015",
    "abstract": "Aiming at realizing ubiquitous spectrum access and improving the spectrum efficiency in a software-defined network, in this paper, we adopt the concept of cognitive radio and propose a joint subcarrier and power allocation (JSPA) scheme for reciprocally benefited spectrum access with secondary users (SUs) cooperating with primary users (PUs). In our proposed JSPA scheme, SUs adopt the decode-and-forward relaying protocol to help PUs in a two-stage way. Meanwhile, a fraction of unallocated licensed spectrum is allocated for the secondary transmission in every stage. However, if the two-stage cooperation still cannot satisfy the PUs’ outage quality-of-service (QoS) requirement, SUs then switch to the access mode and entirely capture the licensed spectrum. Our proposed scheme is to maximize the average transmission rate of SUs through joint optimal subcarriers and power allocation under the constraint of PUs’ outage QoS requirement. The closed-form expressions about the outage probability and average transmission of both PUs and SUs are derived, and we prove the optimality of our scheme. Simulation results show that, compared with the conventional cognitive cooperation scheme, the average transmission rate of SUs is improved."
  },
  {
    "year": "2015",
    "abstract": "Recently, a novel class of on-site coding receivers was proposed. The architecture is suitable for digital beamforming in addition to offering multiple-input multiple-output capabilities. Essential to its realization is a code division multiplexing technique aggregating multiple signal paths at the analog front end into a single analog-to-digital converter. As a result, a significant hardware reduction and a higher power efficiency are achieved when compared with the conventional digital beamforming techniques. In this paper, we examine the system's performance with different types of spreading codes, both orthogonal and nonorthogonal, namely Walsh-Hadamard and Gold codes. Bit error rate calculations show that Walsh-Hadamard codes outperform Gold codes in achieving higher dynamic range with less signal-to-noise ratio degradation, assuming a perfectly synchronous system."
  },
  {
    "year": "2015",
    "abstract": "As it becomes increasingly apparent that 4G will not be able to meet the emerging demands of future mobile communication systems, the question what could make up a 5G system, what are the crucial challenges, and what are the key drivers is part of intensive, ongoing discussions. Partly due to the advent of compressive sensing, methods that can optimally exploit sparsity in signals have received tremendous attention in recent years. In this paper, we will describe a variety of scenarios in which signal sparsity arises naturally in 5G wireless systems. Signal sparsity and the associated rich collection of tools and algorithms will thus be a viable source for innovation in 5G wireless system design. We will also describe applications of this sparse signal processing paradigm in Multiple Input Multiple Output random access, cloud radio access networks, compressive channel-source network coding, and embedded security. We will also emphasize an important open problem that may arise in 5G system design, for which sparsity will potentially play a key role in their solution."
  },
  {
    "year": "2015",
    "abstract": "Variable stiffness actuation has recently attracted great interest in robotics, especially in areas involving a high degree of human-robot interaction. After investigating various design approaches for variable stiffness actuated (VSA) robots, currently the focus is shifting to the control of these systems. Control of VSA robots is challenging due to the intrinsic nonlinearity of their dynamicsdynamics and the need to satisfy constraints on input and state variables. Contrary to the partially open-loop state-of-the-art approaches, in this paper, we present a close-loop control framework for VSA robots leveraging recent increases in computational resources and advances in optimization algorithms. In particular, we generate reference trajectories by means of open-loop optimal control, and track these trajectories via nonlinear model predictive control in a closed-loop manner. In order to show the advantages of our proposed scheme with respect to the previous (partially open-loop) ones, extensive simulation and real-world experiments were conducted using a two link planar manipulator for a ball throwing task. The results of these experiments indicate that the closed-loop scheme outperforms the partially open loop one due to its ability to compensate for model uncertainties and external disturbances, while satisfying the imposed constraints."
  },
  {
    "year": "2015",
    "abstract": "Novel electronic detection techniques are being increasingly sought as components of highly scalable technologies for high-throughput biosensing applications. Among the techniques being considered, electrochemical detection offers an attractive alternative. Advancement in nanoscale electrochemistry makes this an opportune moment to consider the prospects of its integration with CMOS processes. This paper focuses on the new properties and challenges that emerge from the downscaling of electrode dimensions, focusing, in particular, on redox-cycling-based approaches to nanoscale electrochemical devices. We explore the possibilities of interfacing arrays of such devices with CMOS process technology to create highly parallelized integrated platforms. We cite selective examples to provide a qualitative overview of the general design constraints that attend any system-level integration process. We also discuss several challenges that limit the scalability of such platforms and that need to be overcome to create reliable and robust CMOS-integrated electrochemical biosensing platforms."
  },
  {
    "year": "2015",
    "abstract": "This paper is not about the details of yet another robot control system, but rather the issues surrounding real-world robotic implementation. It is a fact that in order to realize a future where robots coexist with people in everyday places, we have to pass through a developmental phase that involves some risk. Putting a “Keep Out, Experiment in Progress” sign on the door is no longer possible, since we are now at a level of capability that requires testing over long periods of time in complex realistic environments that contain people. We all know that controlling the risk is important—a serious accident could set the field back globally—but just as important is convincing others that the risks are known and controlled. In this paper, we describe our experience going down this path and we show that mobile robotics research health and safety assessment is still unexplored territory in universities and is often ignored. We hope that this paper will make robotics research labs in universities around the world take note of these issues rather than operating under the radar to prevent any catastrophic accidents."
  },
  {
    "year": "2015",
    "abstract": "This paper demonstrates an innovative and simple solution for obstacle detection and collision avoidance of unmanned aerial vehicles (UAVs) optimized for and evaluated with quadrotors. The sensors exploited in this paper are low-cost ultrasonic and infrared range finders, which are much cheaper though noisier than more expensive sensors such as laser scanners. This needs to be taken into consideration for the design, implementation, and parametrization of the signal processing and control algorithm for such a system, which is the topic of this paper. For improved data fusion, inertial and optical flow sensors are used as a distance derivative for reference. As a result, a UAV is capable of distance controlled collision avoidance, which is more complex and powerful than comparable simple solutions. At the same time, the solution remains simple with a low computational burden. Thus, memory and time-consuming simultaneous localization and mapping is not required for collision avoidance."
  },
  {
    "year": "2015",
    "abstract": "This paper presents design, implementation, and evaluation of a miniature rectenna for energy harvesting applications. The rectenna produces DC power from a distant microwave energy transmitter. The generated DC power is then utilized to operate a head-mountable deep brain stimulation device. The rectenna consists of a miniature three-layer planar inverted-F antenna and a Schottky-diode-based bridge rectifier. The antenna has a volume of π × 6 × 1.584 mm3, a resonance frequency of 915 MHz with a simulated bandwidth of 18 MHz (907-925 MHz), and a measured bandwidth of 18 MHz (910-928 MHz) at the return loss of -10 dB. A dielectric substrate of FR-4 of εr = 4.5 and δ = 0.02 is used for simulation and fabrication of the antenna and the rectifier due to its low cost. An L-section impedance matching circuit is employed between the antenna and the rectifier to reduce the mismatch loss. The impedance matching circuit operates as a low-pass filter eliminating higher order harmonics. A deep brain stimulation device is successfully operated by the rectenna at a distance of 20 cm away from a microwave energy transmitter of power 26.77 dBm. The motivation of this paper includes creation of a deep brain stimulation device that operates indefinitely without a battery. From the application standpoint, the developed energy harvesting rectenna facilitates long-term deep brain stimulation of laboratory animals for preclinical research investigating neurological disorders."
  },
  {
    "year": "2015",
    "abstract": "Gait pattern performance is a very important issue in the field of humanoid robots, and more and more researchers are now engaged in such studies. However, the tuning processes of the parameters or postures are very tedious and time-consuming. In order to solve this problem, an artificial bee colony (ABC) learning algorithm for a central pattern generator (CPG) gait produce method is proposed in this paper. Furthermore, the fitness of the bee colony is considered through environmental impact assessment, and it is also estimated from the cause of colony collapse disorder from the results of recent investigations in areas, such as pesticides, electromagnetic waves, viruses, and the timing confusion of the bee colony caused by climate change. Each environmental disaster can be considered by its adjustable weighting values. In addition, the developed biped gait learning method is called the ABC-CPG algorithm, and it was verified in a self-developed high-integration simulator. The strategy systems, motion control system, and gait learning system of the humanoid robot are also integrated through the proposed 3-D simulator. Finally, the experimental results show that the proposed environmental-impact-assessed ABC-CPG gait learning algorithm is feasible and can also successfully achieve the best gait pattern in the humanoid robot."
  },
  {
    "year": "2015",
    "abstract": "Wireless standardization activities, such as the Bluetooth V4.x (Bluetooth smart), have led to an explosive growth in innovative short-range wireless devices capable of providing various services to assist everyday life. Allowing machine-to-machine (M2M) communication between these M2M devices also presents new commercial service opportunities. M2M service providers, including telecom carriers, may need to establish new networks to accommodate M2M interactions while identifying new business opportunities. Typically, M2M devices need another capable device, such as a smartphone, to act as a gateway to connect to the Internet to operate at their full potential. We propose a novel scheme in which any Internet-enabled device in the vicinity can act as an M2M gateway for M2M devices. This eliminates the need for the M2M device user to also own a smartphone or an Internet-enabled device to obtain services such as online-to-offline. This scheme faces new challenges, such as how to identify the best M2M gateway for a required service and the QoS. This paper proposes a method to select an M2M gateway from a large number of possibilities in order to increase service availability while also obtaining better signal strength for higher QoS. The methods presented are evaluated in terms of their performance, including energy consumption, and a service deployment guideline is derived using real-world data collected at an exhibition, which gave encouraging results."
  },
  {
    "year": "2015",
    "abstract": "The use of fingerprint embedding at the physical layer enables a receiver to authenticate a transmitter by detecting a low-power authentication tag superimposed upon the message waveform; a theoretical framework for such fingerprinting has been outlined. We carry out single-carrier single-antenna software defined radio (SDR) experiments with a wireless communications link over which we transmit and receive packets with the embedded fingerprinting. We analyze these experimental results and find they match well with theoretical predictions. This paper demonstrates that the method of superimposed fingerprints can deliver high probability of authentication without additional bandwidth and with minimal impact on bit-error rate in SDR systems."
  },
  {
    "year": "2015",
    "abstract": "This paper presents an experimental investigation of diversity gain influenced by polarization and spatial diversity techniques in ultrawideband (UWB) radio technology. To this aim, two different coplanar-fed UWB diversity antennas having the same size and employing identical monopoles were taken and both effective and apparent diversity gain were measured in a reverberation chamber. To extract the diversity gain, three commonly used approaches to combine diversity (i.e., selection, equal gain, and maximal ratio) have been considered. Results showed that >1 dB (~ 26 %) improvement in diversity gain is obtained over most of the considered band for polarization diversity case as compared with spatial case, showing the usefulness of polarization diversity for future UWB diversity applications."
  },
  {
    "year": "2015",
    "abstract": "Indium tin oxide (ITO) is one of the most commonly used optically transparent conductors in applications, such as electro-optic antennas, displays, and optical coatings. However, their RF frequency-dependent electrical properties have not been reported in the literature. In this paper, we present measurements of the electrical properties (permittivity and conductivity) of ITO films in the 0.1-20-GHz frequency range. Measurements were carried out using an in-house open-ended coaxial probe technique employing a one-port reflection coefficient. As usual, calibration and numerical post-processing is needed to extract the electrical properties of the ITO film placed on a 0.5-mm-thick Eagle glass. The measured conductivity was on the order of 105throughout the frequency range, and the real and imaginary parts of the permittivity were on the order of 106at lower frequencies and 105at higher frequencies."
  },
  {
    "year": "2015",
    "abstract": "There has been significant interest in the use of fully connected graphical models and deep-structured graphical models for the purpose of structured inference. However, fully connected and deep-structured graphical models have been largely explored independently, leaving the unification of these two concepts ripe for exploration. A fundamental challenge with unifying these two types of models is in dealing with computational complexity. In this paper, we investigate the feasibility of unifying fully connected and deep-structured models in a computationally tractable manner for the purpose of structured inference. To accomplish this, we introduce a deep-structured fully connected random field (DFRF) model that integrates a series of intermediate sparse autoencoding layers placed between state layers to significantly reduce the computational complexity. The problem of image segmentation was used to illustrate the feasibility of using the DFRF for structured inference in a computationally tractable manner. Results in this paper show that it is feasible to unify fully connected and deep-structured models in a computationally tractable manner for solving structured inference problems such as image segmentation."
  },
  {
    "year": "2015",
    "abstract": "Responding to the unprecedented challenges imposed by the 5G communications ecosystem, emerging heterogeneous network architectures allow for improved integration between multiple radio access technologies. When combined with advanced cloud infrastructures, they bring to life a novel paradigm of heterogeneous cloud radio access network (H-CRAN). The novel H-CRAN architecture opens door to improved network-wide management, including coordinated cross-cell radio resource allocation. In this paper, emphasizing the lack of theoretical performance analysis, we specifically address the problem of cooperative radio resource management in H-CRAN by providing a comprehensive mathematical methodology for its real-time performance optimization. Our approach enables flexible balance between throughput and fairness metrics, as may be desired by the network operator, and demonstrates attractive benefits when compared against the state-of-the-art multiradio resource allocation strategies. The resulting algorithms are suitable for efficient online implementation, which principal feasibility is confirmed by our proof-of-concept prototype."
  },
  {
    "year": "2015",
    "abstract": "Robotics research and education have gained significant attention in recent years due to increased development and commercial deployment of industrial and service robots. A majority of researchers working on robot grasping and object manipulation tend to utilize commercially available robot-manipulators equipped with various end effectors for experimental studies. However, commercially available robotic grippers are often expensive and are not easy to modify for specific purposes. To extend the choice of robotic end effectors freely available to researchers and educators, we present an open-source low-cost three-finger robotic gripper platform for research and educational purposes. The 3-D design model of the gripper is presented and manufactured with a minimal number of 3-D-printed components and an off-the-shelf servo actuator. An underactuated finger and gear train mechanism, with an overall gripper assembly design, are described in detail, followed by illustrations and a discussion of the gripper grasping performance and possible gripper platform modifications. The presented open-source gripper platform computer-aided design model is released for downloading on the authors research lab website (<;uri xlink:href=\"http://www.alaris.kz\" xlink:type=\"simple\">www.alaris.kz<;/uri>) and can be utilized by robotics researchers and educators as a design platform to build their own robotic end effector solutions for research and educational purposes."
  },
  {
    "year": "2015",
    "abstract": "The main vision of the Internet of Things (IoT) is to equip real-life physical objects with computing and communication power so that they can interact with each other for the social good. As one of the key members of IoT, Internet of Vehicles (IoV) has seen steep advancement in communication technologies. Now, vehicles can easily exchange safety, efficiency, infotainment, and comfort-related information with other vehicles and infrastructures using vehicular ad hoc networks (VANETs). We leverage on the cloud-based VANETs theme to propose cyber-physical architecture for the Social IoV (SIoV). SIoV is a vehicular instance of the Social IoT (SIoT), where vehicles are the key social entities in the machine-to-machine vehicular social networks. We have identified the social structures of SIoV components, their relationships, and the interaction types. We have mapped VANETs components into IoT-A architecture reference model to offer better integration of SIoV with other IoT domains. We also present a communication message structure based on automotive ontologies, the SAE J2735 message set, and the advanced traveler information system events schema that corresponds to the social graph. Finally, we provide the implementation details and the experimental analysis to demonstrate the efficacy of the proposed system as well as include different application scenarios for various user groups."
  },
  {
    "year": "2015",
    "abstract": "In a machine-to-machine network, the throughput performance plays a very important role. Recently, an attractive energy harvesting technology has shown great potential to the improvement of the network throughput, as it can provide consistent energy for wireless devices to transmit data. Motivated by that, an efficient energy harvesting-based medium access control (MAC) protocol is designed in this paper. In this protocol, different devices first harvest energy adaptively and then contend the transmission opportunities with energy level related priorities. Then, a new model is proposed to obtain the optimal throughput of the network, together with the corresponding hybrid differential evolution algorithm, where the involved variables are energy-harvesting time, contending time, and contending probability. Analytical and simulation results show that the network based on the proposed MAC protocol has greater throughput than that of the traditional methods. In addition, as expected, our scheme has less transmission delay, further enhancing its superiority."
  },
  {
    "year": "2015",
    "abstract": "The rapid and efficient development of soft active materials requires readily available, compact testing equipment. We propose a desktop-sized, cost-efficient, and open source radial stretching system as an alternative to commercially available biaxial and uniaxial stretching devices. It allows for doubling the diameter of an elastomer membrane while measuring the applied force. Our development enables significant cost reduction (<300€) and increase the availability of equibiaxial deformation measurements for scientific material analysis. Construction plans, source code, and electronic circuit diagrams are freely available under a creative commons license."
  },
  {
    "year": "2015",
    "abstract": "We have previously evaluated the feasibility of a serial code accelerator core with 3-D DRAM stacked on the core operating at high frequencies. While operating at such high frequencies (>24 GHz), there are concerns with removing heat from the 3-D stack. We propose the use of thin diamond sheets, which have high thermal conductivity, as a heat spreader by bonding it close to the processor core substrate and memory stacks. We show, through thermal modeling using COMSOL finite-element analysis tools, the feasibility of diamond as an effective heat spreader in a processor-memory 3-D stack."
  },
  {
    "year": "2015",
    "abstract": "The characterization of the performance of wireless devices is key to developing new RF products conforming to the latest communications protocols. Traditionally, communication performance tests have focused on the RF performance of the tested devices, e.g., smart phones, pads, laptops, and so on. In particular, the focus has shifted from conducted (i.e., cabled) measurements to more realistic over-the-air (OTA) characterization of the RF performance of these devices in transmit or receive mode. For example, the receiver performance of 2G and 3G wireless devices can be measured in terms of the total isotropic sensitivity (TIS) that depends on the antenna and the receiver parts of a wireless device. These measurements can be performed in a reverberation chamber setup. However, standard TIS measurements can be time-consuming and do not reflect the actual performance gains of multiple-input multiple-output antenna systems operating over orthogonal frequency division multiplexing channels, such as those encountered in 4G long-term evolution (LTE) systems. Therefore, in order to meet both time and cost efficiency requirements, we propose here a new method to determine the TIS, as well as the diversity performance, of an LTE device based on throughput measurements. The proposed method shows that the TIS of an LTE device is characterized much faster directly from OTA throughput measurements than from standard TIS measurements and with excellent accuracy."
  },
  {
    "year": "2015",
    "abstract": "To reduce the complexity associated with application-specific tuning of wireless sensor networks (WSNs), dynamic profiling enables an accurate view of an application's runtime behavior, such that the network can be reoptimized at runtime in response to changing application behavior or environmental conditions. However, the dynamic profiling must be able to accurately capture application behavior without incurring significant runtime overheads. Since application- and sensor-specific constraints dictate the profiling requirements and tolerated overheads, designers require design assistance to quickly evaluate and select appropriate profiling methodologies. To increase designer productivity, we formulate profiling methodology design guidelines based on extensive evaluation and analysis of a variety of profiling methodologies suitable for dynamically monitoring WSNs with respect to network traffic overhead, power, and code impacts associated with each method. While energy consumption increases are reasonable, ranging from 0.5% to 2.6%, network traffic, code size, and computation time overheads can be as high as 66.2%, 75.9%, and 136.6%, respectively. Our results show that these overhead variations are highly application specific, and a single profiling method is not suitable for all types of application behavior, thus necessitating, application-specific profiling methodology customization. To facilitate rapid development of these profiling methodologies, we present a profiler-customization methodology consisting of a code generator module, overhead estimation module, and profile data management module. Using our profiling-customization methodology, designers can rapidly evaluate the overhead of different profiling methodologies, and automatically integrate the most appropriate methodology into the application at design time."
  },
  {
    "year": "2015",
    "abstract": "With the accelerated development of Internet-of-Things (IoT), wireless sensor networks (WSNs) are gaining importance in the continued advancement of information and communication technologies, and have been connected and integrated with the Internet in vast industrial applications. However, given the fact that most wireless sensor devices are resource constrained and operate on batteries, the communication overhead and power consumption are therefore important issues for WSNs design. In order to efficiently manage these wireless sensor devices in a unified manner, the industrial authorities should be able to provide a network infrastructure supporting various WSN applications and services that facilitate the management of sensor-equipped real-world entities. This paper presents an overview of industrial ecosystem, technical architecture, industrial device management standards, and our latest research activity in developing a WSN management system. The key approach to enable efficient and reliable management of WSN within such an infrastructure is a cross-layer design of lightweight and cloud-based RESTful Web service."
  },
  {
    "year": "2015",
    "abstract": "Presents a listing of IEEE Access Associate Editors."
  },
  {
    "year": "2015",
    "abstract": "With the introduction of new depth-sensing technologies, interactive hand-gesture devices (such as smart televisions and displays) have been rapidly emerging. However, given the lack of a common vocabulary, most hand-gesture control commands are device-specific, burdening the user into learning different vocabularies for different devices. In order for hand gestures to become a natural communication for users with interactive devices, a standardized interactive hand-gesture vocabulary is necessary. Recently, researchers have approached this issue by conducting studies that elicit gesture vocabularies based on users’ preferences. Nonetheless, a universal vocabulary has yet to be proposed. In this paper, a thorough design methodology for achieving such a universal hand-gesture vocabulary is presented. The methodology is derived from the work of Wobbrock et al. and includes four steps: 1) a preliminary survey eliciting users’ attitudes; 2) a broader user survey in order to construct the universal vocabulary via results of the preliminary survey; 3) an evaluation test to study the implementation of the vocabulary; and 4) a memory test to analyze the memorability of the vocabulary. The proposed vocabulary emerged from this methodology achieves an agreement score exceeding those of the existing studies. Moreover, the results of the memory test show that, within a 15-min training session, the average accuracy of the proposed vocabulary is 90.71%. Despite the size of the proposed gesture vocabulary being smaller than that of similar work, it shares the same functionality, is easier to remember and can be integrated with smart TVs, interactive digital displays, and so on."
  },
  {
    "year": "2015",
    "abstract": "The aim of this study is the computerization of the argument Delphi method. The Delphi method is mainly designed for qualitative prediction within a group of experts, where the experts make predictions and a facilitator controls these predictions until the experts end up with a level of consensus. Argument Delphi, as opposed to the classical Delphi model, is built on the contradictions of the ideas of the experts. Argument Delphi mainly focuses on a discussion topic and asks experts to create new arguments and criticize other arguments from other experts. After a certain level of contradiction, the method yields an amount of contradictory, criticized arguments and builds a decision over these antitheses, as in the Hegelian approach. This is the first time the argument Delphi method has been modeled in a graph of arguments and the problem of qualitative decision has been transferred into a graph problem using Delphi method. This paper is also the first time that argument aggregation and evaluation methods have been proposed. Moreover, the computerized version of argument Delphi is applied to real-world problems using crowd involvement through Facebook. The problem is defined as the prediction of petroleum prices for the end of year and more than 100 contributors from all around the world argued and criticized each other. This paper also discusses the findings of this case study."
  },
  {
    "year": "2015",
    "abstract": "Sparse representation has attracted much attention from researchers in fields of signal processing, image processing, computer vision, and pattern recognition. Sparse representation also has a good reputation in both theoretical research and practical applications. Many different algorithms have been proposed for sparse representation. The main purpose of this paper is to provide a comprehensive study and an updated review on sparse representation and to supply guidance for researchers. The taxonomy of sparse representation methods can be studied from various viewpoints. For example, in terms of different norm minimizations used in sparsity constraints, the methods can be roughly categorized into five groups: 1) sparse representation with l0-norm minimization; 2) sparse representation with lp-norm (0 <; p <; 1) minimization; 3) sparse representation with l1-norm minimization; 4) sparse representation with l2,1-norm minimization; and 5) sparse representation with l2-norm minimization. In this paper, a comprehensive overview of sparse representation is provided. The available sparse representation algorithms can also be empirically categorized into four groups: 1) greedy strategy approximation; 2) constrained optimization; 3) proximity algorithm-based optimization; and 4) homotopy algorithm-based sparse representation. The rationales of different algorithms in each category are analyzed and a wide range of sparse representation applications are summarized, which could sufficiently reveal the potential nature of the sparse representation theory. In particular, an experimentally comparative study of these sparse representation algorithms was presented."
  },
  {
    "year": "2015",
    "abstract": "Uncertainty evaluation plays an important role in ensuring that a designed system can indeed achieve its desired performance. There are three standard methods to evaluate the propagation of uncertainty: 1) analytic linear approximation; 2) Monte Carlo (MC) simulation; and 3) analytical methods using mathematical representation of the probability density function (pdf). The analytic linear approximation method is inaccurate for highly nonlinear systems, which limits its application. The MC simulation approach is the most widely used technique, as it is accurate, versatile, and applicable to highly nonlinear systems. However, it does not define the uncertainty of the output in terms of those of its inputs. Therefore, designers who use this method need to resimulate their systems repeatedly for different combinations of input parameters. The most accurate solution can be attained using the analytical method based on pdf. However, it is unfortunately too complex to employ. This paper introduces the use of an analytical standard uncertainty evaluation (ASUE) toolbox that automatically performs the analytical method for multivariate polynomial systems. The backbone of the toolbox is a proposed ASUE framework. This framework enables the analytical process to be automated by replacing the complex mathematical steps in the analytical method with a Mellin transform lookup table and a set of algebraic operations. The ASUE toolbox was specifically designed for engineers and designers and is, therefore, simple to use. It provides the exact solution obtainable using the MC simulation, but with an additional output uncertainty expression as a function of its input parameters. This paper goes on to show how this expression can be used to prevent overdesign and/or suboptimal design solutions. The ASUE framework and toolbox substantially extend current analytical techniques to a much wider range of applications."
  },
  {
    "year": "2015",
    "abstract": "Fluorescence molecular tomography (FMT) plays an important role in in vivo small animal imaging. However, due to the diffusive nature of photon propagation in biological tissues, FMT suffers from a low spatial resolution, which limits its capability of resolving the distribution of fluorescent biomarkers. In this paper, we investigate the effect of functional and structural a priori information on the accuracy of FMT reconstruction by a hybrid FMT and X-ray computed tomography imaging system. The results from numerical simulation and phantom experiments suggest that the fluorescence targets embedded in heterogeneous medium can be localized when structural a priori information is utilized to constrain the reconstruction process. In addition, both the functional and structural a priori information are essential for the recovery of fluorophore concentration."
  },
  {
    "year": "2015",
    "abstract": "Quantum key distribution (QKD) is an innovative technology that exploits the laws of quantum mechanics to generate and distribute unconditionally secure shared key for use in cryptographic applications. However, QKD is a relatively nascent technology where real-world system implementations differ significantly from their ideal theoretical representations. In this paper, we introduce a modeling framework built upon the OMNeT++ discrete event simulation framework to study the impact of implementation nonidealities on QKD system performance and security. Specifically, we demonstrate the capability to study the device imperfections and practical engineering limitations through the modeling and simulation of a polarization-based, prepare and measure BB84 QKD reference architecture. The reference architecture allows users to model and study complex interactions between physical phenomenon and system-level behaviors representative of real-world design and implementation tradeoffs. Our results demonstrate the flexibility of the framework to simulate and evaluate current, future, and notional QKD protocols and components."
  },
  {
    "year": "2015",
    "abstract": "Random number generators (RNGs) are the foundation of strong security and privacy measures. With an increasing number of smart devices being connected to the Internet, the demand for secure communication will only increase. An important outgrowth of Internet-connected devices is the embedding of sensors. Yet, there remains a paucity of good protocols to provide sensor-based secure RNG seeds. In their raw form, sensor data are a weak source of RNG seeds for two reasons: 1) adversarial control—a malicious party gaining control of the sensor and generating a known data sequence and 2) collinearity across sensors—inherent correlated sequences generated because sensors are embedded in the same device. We propose a new seeding technique that leverages sensor data to provide secure seeds for RNG. Given the current proliferation of sensors and Internet-connectivity on smart devices, this technique could increase cybersecurity in a variety of domains, without additional cost."
  },
  {
    "year": "2015",
    "abstract": "One of the common ways to perform data-driven fault diagnosis is to employ statistical models, which can classify the data into nominal (healthy) and a fault class or distinguish among different fault classes. The former is termed fault (anomaly) detection, and the latter is termed fault isolation (classification, diagnosis). Traditionally, statistical classifiers are trained using data from faulty and nominal behaviors in a batch mode. However, it is difficult to anticipate, a priori, all the possible ways in which failures can occur, especially when a new vehicle model is introduced. Therefore, it is imperative that diagnostic algorithms adapt to new cases on an ongoing basis. In this paper, a unified methodology to incrementally learn new information from evolving databases is presented. The performance of adaptive (or incremental learning) classification techniques is discussed when: 1) the new data has the same fault classes and same features and 2) the new data has new fault classes, but with the same set of observed features. The proposed methodology is demonstrated on data sets derived from an automotive electronic throttle control subsystem."
  },
  {
    "year": "2015",
    "abstract": "Silicon nanowire field-effect transistors (Si-NW FETs) have been demonstrated as a versatile class of potentiometric nanobiosensors for real time, label-free, and highly sensitive detection of a wide range of biomolecules. In this review, we summarize the principles of such devices and recent developments in device fabrication, fluid integration, surface functionalization, and biosensing applications. The main focus of this review is on CMOS compatible Si-NW FET nanobiosensors."
  },
  {
    "year": "2015",
    "abstract": "A novel water dense dielectric patch antenna (DDPA) fed by an L-shaped probe is proposed and investigated. In contrast to the water antennas in the literature, including the water monopole and the water dielectric resonator antenna, the operation mechanism of the proposed water DDPA is similar to the conventional metallic patch antenna. The antenna is excited in a mode like the TM10mode of the rectangular patch antenna. An L-shaped probe, which is widely used for the conventional patch antenna, is used to excite the water DDPA. A study on the bandwidth performance of the proposed design reveals that wide bandwidth can be achieved for the antenna by choosing a thick supporting substrate between the water patch and the ground plane. A prototype is fabricated to confirm the correctness of the design. An impedance bandwidth of 8%, maximum gain of 7.3 dBi, radiation efficiency up to 70%, and symmetrically unidirectional patterns with low backlobe and low cross polarization levels are obtained. Furthermore, owing to the transparency of the water patch, the proposed water DDPA can be conveniently integrated with the solar cells to realize a dual-function design. Measurements on the prototype demonstrate that the existence of the solar cells does not significantly affect the performance of the antenna and vice versa."
  },
  {
    "year": "2015",
    "abstract": "In analyzing single-channel synthetic aperture radar (SAR) imagery, three interrelated questions often arise. First, should one use the detected or the complex-valued image? Second, what is the `best' statistical model? Finally, what constitutes the `best' signal processing methods? This paper addresses these questions from the overarching perspective of the generalized central limit theorem, which underpins nonlinear signal processing. A novel procedure for characterizing the nonlinear dynamics in SAR imagery is proposed. To apply the procedure, three complementary 1-D abstractions for a 2-D SAR chip are introduced. Our analysis is demonstrated on real-world datasets from multiple SAR sensors. The nonlinear dynamics are found to be resolution dependent. As the SAR chip is detected, nonlinear effects are found to be obliterated (i.e., for magnitude-detection) or altered (i.e., for power-detection). In the presence of extended targets (i.e., nonlinear scatterers), it is recommended to use the complex-valued chip rather than the detected one. Furthermore, to exploit the intrinsic nonlinear statistics, it is advised to utilize relevant nonlinear signal analysis techniques."
  },
  {
    "year": "2015",
    "abstract": "Most genetic networks, such as that for the biological clock, are part of much larger modules controlling fundamental processes in the cell, such as metabolism, development, and response to environmental signals. For example, the biological clock is part of a much larger network controlling the circadian rhythms of about 2418 distinct genes in the genome (with 11 000 genes) of the model system, Neurospora crassa. Predicting and understanding the dynamics of all of these genes and their products in a genetic network describing how the clock functions is a challenge and beyond the current capability of the fastest serial computers. We have implemented a novel variable-topology supernet ensemble method using Markov chain Monte Carlo simulations to fit and discover a regulatory network of unknown topology composed of 2418 genes describing the entire clock circadian network, a network that is found in organisms ranging from bacteria to humans, by harnessing the power of the general-purpose graphics processing unit and exploiting the hierarchical structure of that genetic network. The result is the construction of a genetic network that explains mechanistically how the biological clock functions in the filamentous fungus N. crassa and is validated against over 31 000 data points from microarray experiments. Two transcription factors are identified targeting ribosome biogenesis in the clock network."
  },
  {
    "year": "2015",
    "abstract": "In this paper, we review new and emerging energy sources for wireless implantable microdevices. After a brief historical background, we review the developments in power sources in the decades following the pioneering works of Zworykin and Mackay in the late 1950s. These include deployment of lithium batteries and inductive powering in the 1970s, which resulted in significant growth and commercialization of implantable medical devices, such as cardiac pacemakers and cochlear implants. Recent research in nanoscale materials for energy generation has created intriguing possibilities for next generation implantable power sources in the form of flexible and biodegradable batteries and supercapacitors. In addition, energy harvesting/remote powering from various environmental physical and chemical sources within the body utilizing nanoscale materials can also offer unique possibilities for autonomous implantable microscale and nanoscale devices."
  },
  {
    "year": "2015",
    "abstract": "Wireless sensor networks (WSNs) are widely applied in various industrial applications. In this paper, we present an efficient data gathering scheme that guarantees the Quality of Service and optimizes the following network performance metrics as well as the end-to-end reliability in WSNs: (1) minimum total energy consumption; (2) minimum unit data transmitting energy consumption; and (3) maximum utilization efficiency defined as network lifetime per unit deployment. We first transform the performance optimization problem into a problem to optimize the following parameters: (1) deployed nodal number N*; (2) nodal placement d*; and (3) nodal transmission structure p*. Then, we prove that the optimization problem is solvable mathematically. For our observation, the sensor nodes close to the sink trend failed early since they consumed more energy with heavier relay traffic destined for the sink, which seriously affects the network performance. The key point of this optimization is adopting lower reliability requirements and shorter transmission distance for nodes near the sink. Consequently, this reduces the energy consumption of the nodes in the hotspot area. Meanwhile, it adopts higher reliability requirements and farther transmission distance for nodes far from the sink to make full use of the node residual energy, so as to optimize the network performance without harming network reliability. Numerical simulation results demonstrate that our optimal approach improves the network lifetime by 18%-48% and network utility by 17%, and guarantees desire reliability level."
  },
  {
    "year": "2015",
    "abstract": "Wireless sensor networks (WSNs) have been widely applied in various industrial applications, which involve collecting a massive amount of heterogeneous sensory data. However, most of the data-gathering strategies for WSNs cannot avoid the hotspot problem in local or whole deployment area. Hotspot problem affects the network connectivity and decreases the network lifetime. Hence, we propose a tree-cluster-based data-gathering algorithm (TCBDGA) for WSNs with a mobile sink. A novel weight-based tree-construction method is introduced. The root nodes of the constructed trees are defined as rendezvous points (RPs). Additionally, some special nodes called subrendezvous points (SRPs) are selected according to their traffic load and hops to root nodes. RPs and SRPs are viewed as stop points of the mobile sink for data collection, and can be reselected after a certain period. The simulation and comparison with other algorithms show that our TCBDGA can significantly balance the load of the whole network, reduce the energy consumption, alleviate the hotspot problem, and prolong the network lifetime."
  },
  {
    "year": "2015",
    "abstract": "Due to the technological evolution and the increasing popularity of smartphones, people can access an application using authentication based on biometric approaches from many different devices. Device interoperability is a very challenging problem for biometrics, which needs to be further studied. In this paper, we focus on interoperability device compensation for online signature verification since this biometric trait is gaining a significant interest in banking and commercial sector in the last years. The proposed approach is based on two main stages. The first one is a preprocessing stage where data acquired from different devices are processed in order to normalize the signals in similar ranges. The second one is based on feature selection taking into account the device interoperability case, in order to select to select features which are robust in these conditions. This proposed approach has been successfully applied in a similar way to two common system approaches in online signature verification, i.e., a global features-based system and a time functions-based system. Experiments are carried out using Biosecure DS2 (Wacom device) and DS3 (Personal Digital Assistant mobile device) dynamic signature data sets which take into account multisession and two different scenarios emulating real operation conditions. The performance of the proposed global features-based and time functions-based systems applying the two main stages considered in this paper have provided an average relative improvement of performance of 60.3% and 26.5% Equal Error Rate (EER), respectively, for random forgeries cases, compared with baseline systems. Finally, a fusion of the proposed systems has achieved a further significant improvement for the device interoperability problem, especially for skilled forgeries. In this case, the proposed fusion system has achieved an average relative improvement of 27.7% EER compared with the best performance of time functions-based system. These results ..."
  },
  {
    "year": "2015",
    "abstract": "Metallic materials can be considered to be in a peculiar position for production of medical devices, because of their attractive properties. In this paper, a new enriched Co-based composition is proposed for dental application, starting from a conventional CoCrMo alloy. Macrostructural and microstructural investigations, mechanical and corrosion resistance evaluation, and metal ions release are carried out. The best composition, among the studied alloys, is identified for dental purpose."
  },
  {
    "year": "2015",
    "abstract": "A Taguchi-based-genetic algorithm (TBGA) is used in an adaptive neuro-fuzzy inference system (ANFIS) to optimize design parameters for surface acoustic wave (SAW) gas sensors. The Taguchi method is used to reduce the number of experiments and collect performance data for an SAW gas sensor. The TBGA has two optimization roles. In the ANFIS, the TBGA selects appropriate membership functions and optimizes both the premise and the consequent parameters by minimizing the performance criterion of the root mean squared error. Another role of the TBGA is optimizing design parameters for an SAW gas sensor. Simulated experimental application of the proposed TBGA-based ANFIS approach showed that, in terms of both resonant frequency shift and precision performance, this systematic design approach obtains far superior results compared with the conventional trial-and-error design methods and other Taguchi-based design methods."
  },
  {
    "year": "2015",
    "abstract": "Plug-in hybrid electric vehicles (PHEVs) offer the potential to significantly reduce greenhouse gas emissions, if vehicle consumers are willing to adopt this new technology. Consequently, there is much interest in exploring PHEV market penetration models. In prior work, we developed an agent-based model (ABM) of potential PHEV consumer adoption that incorporated several spatial, social, and media influences to identify nonlinear interactions among potential leverage points that may impact PHEV market penetration. In developing that model, the need for additional data to properly inform both the decision-making rules and agent initialization became apparent. To address these issues, we recently conducted and analyzed an extensive consumer survey; in this paper, we modify the ABM to reflect the survey findings. A unique aspect is a one-to-one correspondence between agents in the model and survey respondents, and thus yielding distributions and cross correlations in agent attributes that accurately reflect the survey population. We also implement a used-PHEV market, and allow agents to purchase new or used compact PHEVs or vehicles of their current type. Based on our prior survey response analysis, our modified model includes a PHEV-technology threshold component, a multinomial logistic prediction of willingness to consider a compact PHEV based on dynamically changing attitudes, and agent-specific delay discounting functions that predict the amount agents are willing to pay up front for greater fuel savings. We thus independently account for agents' discomfort with the new PHEV technology, their desire to drive a more environmentally friendly vehicle, and their willingness to pay a higher sticker price for a PHEV. Results of ten survey-based ABM scenarios are reported with implications for policy-makers and manufacturers. We believe close integration of the design of consumer surveys and the development of ABMs is a key step in developing useful decision-support models..."
  },
  {
    "year": "2015",
    "abstract": "This paper presents a significant change in current electric power grid response and recovery schemes by developing a framework for proactive recovery of electric power assets with the primary objective of resiliency enhancement. Within the proposed framework, which can potentially present the next generation decision-making tool for proactive recovery, several coordinated models will be developed including: 1) the outage models to indicate the impact of hurricanes on power system components; 2) a stochastic pre-hurricane crew mobilization model for managing resources before the event; and 3) a deterministic post-hurricane recovery model for managing resources after the event. Proposed models will be extended to ensure applicability to a variety of electric power grids with different technologies and regulatory issues. The theoretical and practical implications of the developed models will push the research frontier of proactive response and recovery schemes in electric power grids, while its flexibility will support application to a variety of infrastructures, in response to a wide range of extreme weather events and natural disasters."
  },
  {
    "year": "2015",
    "abstract": "Recently, we proposed a code-modulated multipath receiver front-end to reduce the number of analog-to-digital converters (ADCs) behind antenna elements and realize significant area, cost, and power reduction. More specifically, code division multiplexing was implemented at the analog front-end for path combining into a single ADC. At the digital baseband, the reverse process was applied to recover signals pertaining to each path. Such front-ends are suitable for spatial diversity and multiplexing and for beamforming. For the latter, it is important to accurately predict the angle of arrival. That is, it is important to faithfully recover the phase difference between adjacent signal paths at the digital baseband. In this paper, the impact of on-site coding on phase error is examined for a two-path receiver using orthogonal Walsh-Hadamard codes of length 8. Simulations show that the relative phase difference, Δφ, between signal paths can be faithfully recovered at the digital baseband. Hardware implementation of a two-path receiver with on-site coding was realized and three different phase measurements were conducted, namely, Δφ = 27°, 40°, and 45°. These measurements confirmed that upon signal and code synchronization, the phases were faithfully recovered with minimal degradation."
  },
  {
    "year": "2015",
    "abstract": "In large-dimensional wireless systems, such as cooperative multicell processing, millimeterwave, and massive multiple input multiple output systems, or cells having a high user density, such as airports, train stations, and metropolitan areas, sufficiently accurate estimation of all the channel gains is required for performing coherent detection. Therefore, they may impose an excessive complexity. As an attractive design alternative, differential modulation relying on noncoherent detection may be invoked for eliminating the requirement for channel estimation at the base station, although at the cost of some performance degradation. In this treatise, we propose low-complexity hard-input hard-output, hard-input soft-output, as well as soft-input soft-output quantum-assisted multiple symbol differential detectors (MSDDs) that perform equivalently to the optimal, but highly complex maximum a posteriori probability MSDDs in multiuser systems, where the users are separated both in the frequency domain and in the time domain. When using an MSDD, the detection of a user's symbols is performed over windows of differentially modulated symbols; hence, they exhibit an increased complexity with respect to the conventional differential detector while simultaneously improving the performance of the system, especially at high Doppler frequencies."
  },
  {
    "year": "2015",
    "abstract": "An investigation of an off-the-shelf solid-state lighting device with the primary focus on the accompanied light-emitting diode (LED) electrical driver (ED) has been conducted. A set of 10 EDs were exposed to temperature humidity life testing of 85% RH and 85 °C (85/85) without an electrical bias per the JEDEC standard JESD22-A101C in order to accelerate the ingress of moisture into the aluminum electrolytic capacitor (AEC) and the EDs in order to assess the reliability of the LED drivers for harsh environment applications. The capacitance and equivalent series resistance for each AEC inside the ED were measured using a handheld LCR meter as possible leading indications of failure. The photometric quantities of a single pristine light engine were monitored in order to investigate the interaction between the light engine and the EDs. These parameters were used in assessing the overall reliability of the EDs. In addition, a comparative analysis has been conducted between the 85/85 accelerated test data and a previously published high-temperature storage life accelerated test of 135 °C. The results of the 85/85 acceleration test and the comparative analysis are presented in this paper."
  },
  {
    "year": "2015",
    "abstract": "This paper aims to make 60-GHz experimentation possible for a wider range of research groups. We do this by describing a low-cost front-end that can be used in combination with any baseband processing platform. We provide detailed instructions and software for connection with the USRP N200/N210, including general classes for controlling the board and example single-input single-output and 2 x 2 Multiple-Input Multiple Output applications. In addition, we provide measurements to assess the impact of phase noise and other hardware impairments in low-cost millimeter-wave systems for hybrid measurement and simulation studies. Finally, we also perform performance measurements on the hardware. All our materials, such as the hardware design, the software, and the measurements, are freely available."
  },
  {
    "year": "2015",
    "abstract": "This paper presents a novel method to address the actuator saturation for nonlinear hybrid systems by directly incorporating user-defined input bounds in a controller design. In particular, we consider the application of bipedal walking and show that our method [based on a quadratic programming (QP) implementation of a control Lyapunov function (CLF)-based controller] enables a gradual performance degradation while still continuing to walk under increasingly stringent input bounds. We draw on our previous work, which has demonstrated the effectiveness of the CLF-based controllers for stabilizing periodic gaits for biped walkers. This paper presents a framework, which results in more effective handling of control saturations and provides a means for incorporating a whole family of user-defined constraints into the online computation of a CLF-based controller. This paper concludes with an experimental validation of the main results on the bipedal robot MABEL, demonstrating the usefulness of the QP-based CLF approach for real-time robotic control."
  },
  {
    "year": "2015",
    "abstract": "Provides a listing of current committee members and society officers."
  },
  {
    "year": "2015",
    "abstract": "Powerful quantum error correction codes (QECCs) are required for stabilizing and protecting fragile qubits against the undesirable effects of quantum decoherence. Similar to classical codes, hashing bound approaching QECCs may be designed by exploiting a concatenated code structure, which invokes iterative decoding. Therefore, in this paper, we provide an extensive step-by-step tutorial for designing extrinsic information transfer (EXIT) chart-aided concatenated quantum codes based on the underlying quantum-to-classical isomorphism. These design lessons are then exemplified in the context of our proposed quantum irregular convolutional code (QIRCC), which constitutes the outer component of a concatenated quantum code. The proposed QIRCC can be dynamically adapted to match any given inner code using EXIT charts, hence achieving a performance close to the hashing bound. It is demonstrated that our QIRCC-based optimized design is capable of operating within 0.4 dB of the noise limit."
  },
  {
    "year": "2015",
    "abstract": "Large-scale fingerprint recognition involves capturing ridge patterns at different time intervals using various methods, such as live-scan and paper-ink approaches, introducing intraclass variations in the fingerprint. The performance of existing algorithms is significantly affected when fingerprints are captured with diverse acquisition settings such as multisession, multispectral, multiresolution, with slap, and with latent fingerprints. One of the primary challenges in developing a generic and robust fingerprint matching algorithm is the limited availability of large data sets that capture such intraclass diversity. In this paper, we present the multisensor optical and latent fingerprint database of more than 19000 fingerprint images with different intraclass variations during fingerprint capture. We also showcase the baseline results of various matching experiments on this database. The database is aimed to drive research in building robust algorithms toward solving the problem of latent fingerprint matching and handling intraclass variations in fingerprint capture. Some potential applications for this database are identified and the research challenges that can be addressed using this database are also discussed."
  },
  {
    "year": "2015",
    "abstract": "Cloud computing is developing so fast that more and more data centers have been built every year. This naturally leads to high-power consumption. Virtual machine (VM) consolidation is the most popular solution based on resource utilization. In fact, much more power can be saved if we know the power consumption of each VM. Therefore, it is significant to measure the power consumption of each VM for green cloud data centers. Since there is no device that can directly measure the power consumption of each VM, modeling methods have been proposed. However, current models are not accurate enough when multi-VMs are competing for resources on the same server. One of the main reasons is that the resource features for modeling are correlated with each other, such as CPU and cache. In this paper, we propose a tree regression-based method to accurately measure the power consumption of VMs on the same host. The merits of this method are that the tree structure will split the data set into partitions, and each is an easy-modeling subset. Experiments show that the average accuracy of our method is about 98% for different types of applications running in VMs."
  }
]