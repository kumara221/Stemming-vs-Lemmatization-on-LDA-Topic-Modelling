[
  {
    "year": "2014",
    "abstract": "This paper is about how the SP theory of intelligence and its realization in the SP machine (both outlined in this paper) may help in the design of the brains of autonomous robots, meaning robots that do not depend on external intelligence or power supplies, are mobile, and have human-like versatility and adaptability in intelligence. This paper addresses three main problems: 1) how to increase the computational and energy efficiency of computers and to reduce their size and weight; 2) how to achieve human-like versatility in intelligence; and 3) likewise for human-like adaptability in intelligence. Regarding the first problem, the SP system has the potential for substantial gains in computational efficiency, with corresponding cuts in energy consumption and the bulkiness of computers: 1) by reducing the size of data to be processed; 2) by exploiting statistical information that the system gathers as an integral part of how it works; and 3) via a new version of Donald Hebb's concept of a cell assembly. Toward human-like versatility in intelligence, the SP system has strengths in unsupervised learning, natural language processing, pattern recognition, information retrieval, several kinds of reasoning, planning, problem solving, and more, with seamless integration among structures and functions. The SP system's strengths in unsupervised learning and other aspects of intelligence may help in achieving human-like adaptability in intelligence via: 1) one-trial learning; 2) learning of natural language; 3) learning to see; 4) building 3-D models of objects and of a robot's surroundings; 5) learning regularities in the workings of a robot and in the robot's environment; 6) exploration and play; 7) learning major skills; and 8) learning via demonstration. Also discussed are how the SP system may process parallel streams of information, generalization of knowledge, correction of over-generalizations, learning from dirty data, how to cut the cost of learning, and reinforcemen..."
  },
  {
    "year": "2014",
    "abstract": "Device-to-device (D2D) communications have been proposed as an underlay to long-term evolution (LTE) networks as a means of harvesting the proximity, reuse, and hop gains. However, D2D communications can also serve as a technology component for providing public protection and disaster relief (PPDR) and national security and public safety (NSPS) services. In the United States, for example, spectrum has been reserved in the 700-MHz band for an LTE-based public safety network. The key requirement for the evolving broadband PPDR and NSPS services capable systems is to provide access to cellular services when the infrastructure is available and to efficiently support local services even if a subset or all of the network nodes become dysfunctional due to public disaster or emergency situations. This paper reviews some of the key requirements, technology challenges, and solution approaches that must be in place in order to enable LTE networks and, in particular, D2D communications, to meet PPDR and NSPS-related requirements. In particular, we propose a clustering-procedure-based approach to the design of a system that integrates cellular and ad hoc operation modes depending on the availability of infrastructure nodes. System simulations demonstrate the viability of the proposed design. The proposed scheme is currently considered as a technology component of the evolving 5G concept developed by the European 5G research project METIS."
  },
  {
    "year": "2014",
    "abstract": "In recent decades, we have witnessed the evolution of biometric technology from the first pioneering works in face and voice recognition to the current state of development wherein a wide spectrum of highly accurate systems may be found, ranging from largely deployed modalities, such as fingerprint, face, or iris, to more marginal ones, such as signature or hand. This path of technological evolution has naturally led to a critical issue that has only started to be addressed recently: the resistance of this rapidly emerging technology to external attacks and, in particular, to spoofing. Spoofing, referred to by the term presentation attack in current standards, is a purely biometric vulnerability that is not shared with other IT security solutions. It refers to the ability to fool a biometric system into recognizing an illegitimate user as a genuine one by means of presenting a synthetic forged version of the original biometric trait to the sensor. The entire biometric community, including researchers, developers, standardizing bodies, and vendors, has thrown itself into the challenging task of proposing and developing efficient protection methods against this threat. The goal of this paper is to provide a comprehensive overview on the work that has been carried out over the last decade in the emerging field of antispoofing, with special attention to the mature and largely deployed face modality. The work covers theories, methodologies, state-of-the-art techniques, and evaluation databases and also aims at providing an outlook into the future of this very active field of research."
  },
  {
    "year": "2014",
    "abstract": "Large-scale data centers enable the new era of cloud computing and provide the core infrastructure to meet the computing and storage requirements for both enterprise information technology needs and cloud-based services. To support the ever-growing cloud computing needs, the number of servers in today's data centers are increasing exponentially, which in turn leads to enormous challenges in designing an efficient and cost-effective data center network. With data availability and security at stake, the issues with data center networks are more critical than ever. Motivated by these challenges and critical issues, many novel and creative research works have been proposed in recent years. In this paper, we investigate in data center networks and provide a general overview and analysis of the literature covering various research areas, including data center network interconnection architectures, network protocols for data center networks, and network resource sharing in multitenant cloud data centers. We start with an overview on data center networks and together with its requirements navigate the data center network designs. We then present the research literature related to the aforementioned research topics in the subsequent sections. Finally, we draw the conclusions."
  },
  {
    "year": "2014",
    "abstract": "Among the discovered knowledge, sequential-pattern mining is used to discover the frequent subsequences from a sequence database. Most research handles the static database in batch mode to discover the desired sequential patterns. In the past, the fast updated (FUP) and Fast UPdated 2 (FUP2) concepts were adopted to, respectively, maintain and update the discovered sequential patterns with sequence insertion and sequence deletion based on the designed FUP sequential pattern (FUSP)-tree structure. Based on the FUP or FUP2 concepts, original customer sequences are required to be rescanned if it is necessary to maintain and update the unpromising (small) sequences from the original database. In the past, pre-large concept was designed to keep the prelarge itemsets as the buffer to avoid the database rescan each time whether transaction insertion or deletion in the dynamic databases. In this paper, the prelarge concept is adopted to handle the discovered sequential patterns with sequence deletion. An FUSP tree is first built to keep only the frequent 1-sequences from the original database. The prelarge 1-sequences are also kept in a set for later maintenance approach. When some sequences are deleted from the original database, the proposed algorithm is then performed to divide the kept frequent 1-sequences and prelarge 1-sequences from the original database and the mined 1-sequences from the deleted customer sequences into three parts with nine cases. Each case is then processed by the designed algorithm to maintain and update the built FUSP tree. When the number of deleted customer sequences is smaller than the safety bound of the prelarge concept, the original customer sequences are unnecessary to be rescanned, but the sequential patterns can still be actually maintained and updated. Experiments are conducted to show the performance of the proposed algorithm in terms of execution time and the number of tree nodes."
  },
  {
    "year": "2014",
    "abstract": "A previous formulation for the application of information accounting to binary decision theory is extended to permit the quality of the decision to be quantitatively measured by evaluation of the underlying informational support. Both a single exemplar measure of information, separability, and its ensemble average equivalent, separation, are shown to measure the information support for decision quality (i.e., how well-informed is the decision), rather than the information support for decision adjudication (i.e., which hypothesis is the better choice) provided by predecision information measures. When compared to the traditional receiver operating characteristic, these measures present several functional advantages. They are scalar in nature, and may be directly optimized over secondary parameters, as well as being rigorously well posed and universally comparable. They incorporate the effects of all relevant decision components (prior information, observational information, and decision rule) in a unified manner while still being easily related to the predecision information measures of log likelihood ratio and generalized signal-to-noise ratio. They can be applied equally well to individual trials or composite averages, and evaluation does not require knowledge of the underlying truth. Compared to false alarm-oriented methods for assessing decision performance, their construction reduces sensitivity to tail effects in the underlying distributions."
  },
  {
    "year": "2014",
    "abstract": "With the evaluation and simulation of long-term evolution/4G cellular network and hot discussion about new technologies or network architecture for 5G, the appearance of simulation and evaluation guidelines for 5G is in urgent need. This paper analyzes the challenges of building a simulation platform for 5G considering the emerging new technologies and network architectures. Based on the overview of evaluation methodologies issued for 4G candidates, challenges in 5G evaluation are formulated. Additionally, a cloud-based two-level framework of system-level simulator is proposed to validate the candidate technologies and fulfill the promising technology performance identified for 5G."
  },
  {
    "year": "2014",
    "abstract": "A circularly polarized patch antenna for future fifth-generation mobile phones is presented in this paper. Miniaturization and beamwidth enhancement of a patch antenna are the two main areas to be discussed. By folding the edge of the radiating patch with loading slots, the size of the patch antenna is 44.8% smaller than a conventional half wavelength patch, which allows it to be accommodated inside handsets easily. Wide beamwidth is obtained by surrounding the patch with a dielectric substrate and supporting the antenna by a metallic block. A measured half power beamwidth of 124° is achieved. The impedance bandwidth of the antenna is over 10%, and the 3-dB axial ratio bandwidth is 3.05%. The proposed antenna covers a wide elevation angle and complete azimuth range. A parametric study of the effect of the metallic block and the surrounding dielectric substrate on the gain at a low elevation angle and the axial ratio of the proposed antenna are presented."
  },
  {
    "year": "2014",
    "abstract": "This paper focuses on energy efficiency aspects and related benefits of radio-access-network-as-a-service (RANaaS) implementation (using commodity hardware) as architectural evolution of LTE-advanced networks toward 5G infrastructure. RANaaS is a novel concept introduced recently, which enables the partial centralization of RAN functionalities depending on the actual needs as well as on network characteristics. In the view of future definition of 5G systems, this cloud-based design is an important solution in terms of efficient usage of network resources. The aim of this paper is to give a vision of the advantages of the RANaaS, to present its benefits in terms of energy efficiency and to propose a consistent system-level power model as a reference for assessing innovative functionalities toward 5G systems. The incremental benefits through the years are also discussed in perspective, by considering technological evolution of IT platforms and the increasing matching between their capabilities and the need for progressive virtualization of RAN functionalities. The description is complemented by an exemplary evaluation in terms of energy efficiency, analyzing the achievable gains associated with the RANaaS paradigm."
  },
  {
    "year": "2014",
    "abstract": "This paper describes the distortion effects often used in an electric guitar. Distortion is an added effect in an electric guitar, which compresses the peaks of the sound waves produced by the musical instrument, to produce a large number of added overtones, which here is done by rigging up a circuit in collaboration with the Arduino UNO circuit board. The digital potentiometer controlled by the Arduino (microcontroller) was an improvement and was able to produce satisfactory results, as compared with the analog potentiometer without the Arduino control. The complex circuitry of a three-stage distortion circuit with the analog potentiometer was replaced by a digital potentiometer controlled by a microcontroller, with better results. This variable-gating distortion pedal has an added advantage of being compact, light, and inexpensive."
  },
  {
    "year": "2014",
    "abstract": "We consider an M/M/1 multiple vacation queueing system with two types of server vacations. Type 1 vacation is taken after the server has exhaustively served all the customers in the system, where the number of customers served is at least one. Type 2 vacation is taken when the server returns from a vacation and finds no customer waiting. Each type of vacation can be interrupted when the number of customers in the system reaches two predefined thresholds, where each vacation type has a different threshold. It is assumed that service times and vacation durations are exponentially distributed with different means. We present a steady-state solution of the system under two vacation interruption policies."
  },
  {
    "year": "2014",
    "abstract": "There has been long-standing controversy, both among scientists and in the public, about whether children absorb more radio frequency (RF) energy in their heads than adults when using a mobile telephone. This review summarizes the current understanding of this issue, and some of the complexities in comparing the absorption of RF energy in different individuals from use of mobile phones. The discussion is limited to dosimetric issues, i.e., possible age-related differences in absorption of RF energy in the heads of mobile phone users. For most metrics of exposure, in particular those relevant to assessing the compliance of handsets with regulatory limits, there is no clear evidence for age-related differences in exposure. For two metrics of exposure, there is a clear evidence that age can play a factor: 1) the local specific absorption rate (SAR), in particular anatomically defined locations within the brain, will vary with head size and hence with age and 2) the SAR, in particular tissues, (e.g., bone marrow in the skull) can vary with age due to age-related differences in the dielectric properties of tissue. However, these differences involve SAR levels that are below the 1-g or 10-g peak spatial SAR (psSAR averaged over 1 or 10 g of tissue) and have no significance for compliance assessment. Age-related differences observed in worst case simulations such as presently considered are difficult to generalize to human populations under real-world exposure conditions due to many variables that determine SAR during realistic usages."
  },
  {
    "year": "2014",
    "abstract": "This paper relates to a genuine wrist pulse oximeter, which is a noninvasive medical device that can measure the pulse rate and oxygen saturation level in a person’s blood. The device is novel due to its innovative design. It is a new type of reflective oximeter, which has a concave structure for housing the optical source and sensor. The neo-reflective sensor module of the device is designed to send the sensor data to a nearby intelligent mobile phone using wireless data transmission. The pulse oximeter has been developed and calibrated, and the calibration curve was analyzed. The innovative design of this pulse oximeter would enable the user to wear the low-cost device on one wrist continuously throughout the day, without the inconvenience of a conventional finger pulse oximeter."
  },
  {
    "year": "2014",
    "abstract": "Ballistic resistance testing is conducted in the Department of Defense (DoD) to estimate the probability that a projectile will perforate the armor of a system under test. Ballistic resistance testing routinely employs sensitivity experiment techniques where sequential test designs are used to estimate a particular quantile of the probability of perforation. Statistical procedures used to estimate the ballistic resistance of armor in the DoD have remained relatively unchanged for decades. In the current fiscal atmosphere of sequestration and budget deficits, efficiency is critical for test and evaluation. In this paper, we review and compare sequential methods, estimators, and stopping criteria used in the DoD to those found in literature. Using Monte Carlo simulation, we find that the three-phase optimal design, a probit model, and a break separation stopping criteria are most accurate and efficient at estimating V50, while the three-phase optimal design or Robbins-Monroe-Joseph method should be used to estimate V10."
  },
  {
    "year": "2014",
    "abstract": "Nonsmall cell lung cancer is a prevalent disease. It is diagnosed and treated with the help of computed tomography (CT) scans. In this paper, we apply radiomics to select 3-D features from CT images of the lung toward providing prognostic information. Focusing on cases of the adenocarcinoma nonsmall cell lung cancer tumor subtype from a larger data set, we show that classifiers can be built to predict survival time. This is the first known result to make such predictions from CT scans of lung cancer. We compare classifiers and feature selection approaches. The best accuracy when predicting survival was 77.5% using a decision tree in a leave-one-out cross validation and was obtained after selecting five features per fold from 219."
  },
  {
    "year": "2014",
    "abstract": "To utilize the synergy between computed tomography (CT) and magnetic resonance imaging (MRI) data sets from an object at the same time, an edge-guided dual-modality image reconstruction approach is proposed. The key is to establish a knowledge-based connection between these two data sets for the tight fusion of different imaging modalities. Our scheme consists of four inter-related elements: 1) segmentation; 2) initial guess generation; 3) CT image reconstruction; and 4) MRI image reconstruction. Our experiments show that, aided by the image obtained from one imaging modality, even with highly under-sampled data, we can better reconstruct the image of the other modality. This approach can be potentially useful for a simultaneous CT-MRI system."
  },
  {
    "year": "2014",
    "abstract": "Clipping is one of the simplest peak-to-average power ratio reduction schemes for orthogonal frequency division multiplexing (OFDM). Deliberately clipping the transmission signal degrades system performance, and clipping mitigation is required at the receiver for information restoration. In this paper, we acknowledge the sparse nature of the clipping signal and propose a low-complexity Bayesian clipping estimation scheme. The proposed scheme utilizes a priori information about the sparsity rate and noise variance for enhanced recovery. At the same time, the proposed scheme is robust against inaccurate estimates of the clipping signal statistics. The undistorted phase property of the clipped signal, as well as the clipping likelihood, is utilized for enhanced reconstruction. Furthermore, motivated by the nature of modern OFDM-based communication systems, we extend our clipping reconstruction approach to multiple antenna receivers and multi-user OFDM.We also address the problem of channel estimation from pilots contaminated by the clipping distortion. Numerical findings are presented that depict favorable results for the proposed scheme compared to the established sparse reconstruction schemes."
  },
  {
    "year": "2014",
    "abstract": "Liposomal iodine nanoparticles (LINPs) have a long half-life and provide an excellent intravascular contrast. The nanoparticles can be functionalized as molecular probes for biological targets to facilitate numerous preclinical studies for translation toward diagnosis and therapy of various human diseases. Iodine has a K-edge at 33 keV due to the photoelectric absorption of photons, which emit X-ray fluorescence at 28 keV with a fluorescence yield of 0.88. Detections of the characteristic X-rays can be used for the imaging of iodine concentration distribution in an object. In this paper, we propose an X-ray fluorescence computed tomography method for reconstruction of a LINPs distribution over a region of interest (ROI) in a small animal. X-rays are focused onto a submillimeter focal spot utilizing a polycapillary lens, generating a pair of X-ray cones in the animal. This focused beam irradiates LINPs, the most strongly at the focal spot. Then, the focal spot can be scanned over an ROI in the object to produce X-ray fluorescence signals. From measured fluorescence data, a reliable image reconstruction can be achieved with a high spatial resolution. Numerical simulation studies are performed to demonstrate the superior imaging performance of this methodology."
  },
  {
    "year": "2014",
    "abstract": "Physical layer (PHY) security is recently regarded as a promising technique to improve the security performance of wireless communication networks. Current developments in PHY security are often based on the assumption of perfect channel state information (CSI). In this paper, both security and reliability performance for the downlink cloud radio access network with optimal remote radio heads (RRHs) node selection are investigated in a practical scenario by considering channel estimation (CE) errors. In particular, a three-phase transmission scheme is proposed and the linear minimum mean-square error (MMSE) estimation method is utilized to obtain the CSI. Based on the CSI estimates and the statistics of CE errors, the outage probability and intercept probability are derived in closed-form expression to evaluate the security and reliability performance, respectively. In addition, two possible cases (with or without intercepting signals from baseband unit) are considered for the eavesdropper. It is found that the suggested optimal RRHs selection scheme outperforms the nonselection scheme, and that the increasing number of RRHs can lower the outage probability as well as the intercept probability. It is also shown that there exists an optimal training number to minimize the sum of the outage probability and intercept probability. Finally, simulation results are provided to corroborate our proposed studies."
  },
  {
    "year": "2014",
    "abstract": "Distributed X-ray sources open the way to innovative system concepts in X-ray and computed tomography. They offer promising opportunities in terms of system performance, but they pose unique challenges in terms of source and system technologies. Several academic and industrial teams have proposed a variety of concepts and developed some unique prototypes. We present a broad review of multisource systems. We also discuss X-ray source components and challenges. We close with our perspective on the future prospects of multisource imaging."
  },
  {
    "year": "2014",
    "abstract": "It is now possible to manipulate individual molecules using a nanopore to read DNA and proteins, or write DNA by inserting mini-genes into cells. Furthermore, development of these methodologies will kick open the door to new biology and chemistry that has been logistically intractable previously. Nanopore technology will place molecular and sub-molecular analysis within the reach of the typical bench-top scientist or clinical lab-no longer limited to genomics or mass spectrometry specialists. Moreover, the prospects for synthetic biology-using nanopores to program or reprogram cells-are promising as well, but have been examined only at the level of a single cell, so far."
  },
  {
    "year": "2014",
    "abstract": "We present here a paper on the potential use of game engines and graphic technologies for 3-D ray-based technologies used to simulate multipath channels. Our approach harnesses the power of video game development engines to provide an urban 3-D ray-based model for exploration and analysis of multipath channels for a wide frequency range in different complex outdoor and indoor scenarios. Game technologies offer a variety of options for exploiting the capabilities of graphical processing units and provide high performance in computing time with accurate results for channel modeling in current and future wireless technologies. We show the usefulness of this approach using our 3-D ray-based system in different applications."
  },
  {
    "year": "2014",
    "abstract": "Bit-error rate measurements for ON-OFF keying modulation at multigigabit per second rates over a V-band wireless link are presented. Serial data-rates from 2.5 to 20 Gb/s were studied for a 231-1 bit random sequence. Error-free data transfer over a 0.3-m link was achieved at up to 10 Gb/s. Acceptable bit-error rates, <; 10-5and 10-3, were measured at up to 1.5 m for 10- and 15-Gb/s data-rate, respectively. The performance was achieved using a transmitter that consists of an integrated wavelet generator, whereas the receiver was built from off-the-shelf waveguide components. The results demonstrate that very high data-rates may be achieved using binary modulation and short symbols generated in an efficient V-band transmitter. The system is benchmarked against state-of-the-art transceiver systems with multigigabit per second data-rates."
  },
  {
    "year": "2014",
    "abstract": "With the rapid advancement of mobile devices, people have become more attached to them than ever. This rapid growth combined with millions of applications (apps) make smart phones a favorite means of communication among users. In general, the available contents on smart phones, apps, and Web, come in two versions: (1) free content that is monetized via advertisements (ads) and (2) paid content that is monetized by user subscription fees. However, the resources, namely, energy, bandwidth, and processing power, on-board are limited, and the existence of ads in Web sites and free apps can significantly increase the usage of these resources. These issues necessitate a good understanding of the mobile advertising eco-system and how such limited resources can be efficiently used. In this paper, we present the results of a novel Web browsing technique that adapts the Web pages delivered to smart phone, based on the smart phone's current battery level and the network type. Web pages are adapted by controlling the amount of ads to be displayed. Validation tests confirm that the system can extend smart phone battery life by up to 30% and save wireless bandwidth up to ~44%."
  },
  {
    "year": "2014",
    "abstract": "A cloud radio access network (Cloud-RAN) is a new cellular technology that brings baseband processing units for a set of base stations into a central server retaining only the radio front-ends at the cell sites. This new architecture opens up opportunities for algorithms that require centralized processing. However, efficient implementation of the algorithms presents a number of challenges the most critical being latency, fronthaul capacity, and resource control. In this paper, we propose a software-defined radio-based architecture that addresses these problems and can be implemented on a cloud of general purpose computing platforms. We also present the practical implementation of Cloud-RAN running on an off-the-shelf server to validate the flexibility of the architecture. The implementation is able to realize various cellular networks, including heterogeneous networks, distribute-antenna systems, and transmission schemes, such as transmit antenna selection and open-loop transmit diversity."
  },
  {
    "year": "2014",
    "abstract": "The use of large-size antenna arrays to implement pencil-beam forming techniques is becoming a key asset to cope with the very high throughput density requirements and high path-loss of future millimeter-wave (mm-wave) gigabit-wireless applications. Suboptimal beamforming (BF) strategies based on search over discrete set of beams (steering vectors) are proposed and implemented in present standards and applications. The potential of fully adaptive advanced BF strategies that will become possible in the future, thanks to the availability of accurate localization and powerful distributed computing, is evaluated in this paper through system simulation. After validation and calibration against mm-wave directional indoor channel measurements, a 3-D ray tracing model is used as a propagation-prediction engine to evaluate performance in a number of simple, reference cases. Ray tracing itself, however, is proposed and evaluated as a real-time prediction tool to assist future BF techniques."
  },
  {
    "year": "2014",
    "abstract": "The performance analysis of a graphics processing unit (GPU) is important for analyzing and fine tuning current and future graphics processors as well as for comparing the performance of different architectures. In this paper, we present an analytical model to calculate the total time it takes for a GPU to retire one frame on a given benchmark. The model also estimates the total retirement time for the same frame on a different GPU using regression estimation model. The model consists of two stages. The first stage entails establishing the measured baseline for a specific frame on a given graphics card, and the second stage entails adjusting the measured baseline and estimating the time it takes to process all draw calls for the same frame on a different graphics card. The model considers the impact of pipeline bottlenecks to process a specific frame, estimates the minimum time it takes to process that frame, and reparameterize the baseline for a different graphics card to calculate new frame retirement times at two different memory frequencies. We used Amdahl's law model to estimate frame retirement time for a different graphics card at higher memory frequencies based on the new adjusted measured baseline with error margin is <;5%."
  },
  {
    "year": "2014",
    "abstract": "The growing popularity and development of data mining technologies bring serious threat to the security of individual,'s sensitive information. An emerging research topic in data mining, known as privacy-preserving data mining (PPDM), has been extensively studied in recent years. The basic idea of PPDM is to modify the data in such a way so as to perform data mining algorithms effectively without compromising the security of sensitive information contained in the data. Current studies of PPDM mainly focus on how to reduce the privacy risk brought by data mining operations, while in fact, unwanted disclosure of sensitive information may also happen in the process of data collecting, data publishing, and information (i.e., the data mining results) delivering. In this paper, we view the privacy issues related to data mining from a wider perspective and investigate various approaches that can help to protect sensitive information. In particular, we identify four different types of users involved in data mining applications, namely, data provider, data collector, data miner, and decision maker. For each type of user, we discuss his privacy concerns and the methods that can be adopted to protect sensitive information. We briefly introduce the basics of related research topics, review state-of-the-art approaches, and present some preliminary thoughts on future research directions. Besides exploring the privacy-preserving approaches for each type of user, we also review the game theoretical approaches, which are proposed for analyzing the interactions among different users in a data mining scenario, each of whom has his own valuation on the sensitive information. By differentiating the responsibilities of different users with respect to security of sensitive information, we would like to provide some useful insights into the study of PPDM."
  },
  {
    "year": "2014",
    "abstract": "The follow me cloud (FMC) concept enables service mobility, wherein not only content/data, but also services follow their respective users. The FMC allows mobile users to be always connected via the optimal data anchor and mobility gateways to access their data and services from optimal data centers. The FMC was initially designed to support user mobility, particularly in 3rd Generation Partnership Project (3GPP) networks. In this paper, FMC is further tailored to support mobile users connected from other network types, such as public WiFi or Asymetric Digital Subscriber Line (ADSL) fixed networks. Indeed, this paper presents an implementation of FMC based on local/identifier separation protocol (LISP), whereby the main goal is to render FMC independent from the underlying technology. To simplify further the deployment, all FMC entities (including LISP entities) are virtualized considering the network function virtualization principle."
  },
  {
    "year": "2014",
    "abstract": "Reluctance accelerators are used to apply linear forces to ferromagnetic projectiles via solenoids. Efficiency increases for a single-stage reluctance accelerator were produced by manipulating the input current pulse supplied by a discharging capacitor. The development of a theoretical model allowed for the calculation of optimized pulse shapes. A digital pulsewidth modulated switching method was used to control the current pulse shape using an Arduino Uno microcontroller, which supplied signals to the gate of a MOSFET transistor that controlled the current to the system solenoid. An efficiency increase of 5.7% was obtained for a reluctance accelerator with an optimized current pulse shape in comparison to a capacitor discharge with no pulse shaping."
  },
  {
    "year": "2014",
    "abstract": "In this paper, a novel graphene-based multiple-input multiple-output (MIMO) concept is proposed for high-rate nanoscale wireless communications between transceivers, which are nano/micrometers apart from each other. In particular, the proposed MIMO architecture considers exploiting a deep-subwavelength propagation channel made of graphene. This allows us to increase the number of transmitted symbol streams, while using a deep-subwavelength arrangement of individual plasmonic nanotransmit/receive elements in which the spacing between the transmitters and/or the receivers is tens of times smaller than the wavelength. This exclusive benefit is achieved with the aid of the phenomenon of graphene plasmons, where graphene offers the extremely confined and low-loss plasmon propagation. Hence, the proposed graphene-based MIMO system is capable of combating the fundamental limitations imposed on the classic MIMO configuration. We also present a novel graphene-specific channel adaptation technique, where the chemical potential of the graphene channel is varied to improve the power of the received signals."
  },
  {
    "year": "2014",
    "abstract": "The cells in an organism emit different amounts of proteins according to their clinical state (healthy/pathological, for instance). The resulting proteomic profile can be used for early detection, diagnosis, and therapy planning. In this paper, we study the classification of a proteomic sample from the point of view of an inverse problem with a joint Bayesian solution, called inversion-classification. We propose a hierarchical physical forward model and present encouraging results from both simulation and clinical data."
  },
  {
    "year": "2014",
    "abstract": "This paper proposes a service-specific network virtualization to address the tremendous increase in the signaling processing load in the evolved packet core and IP multimedia subsystem of a fifth-generation mobile communication system. The proposal creates several virtual networks that are composed of functions specialized for particular services on a mobile communication network and efficiently forwards a sequence of signaling messages to the appropriate virtual networks. Using a prototype system, this paper verifies the overheads costs of the proposal that are incurred during the inspection of packet application headers needed to appropriately forward signaling messages as well as the overheads incurred when replicating state information from one virtual network to another. This paper shows that the proposal can reduce the signaling processing load by∼25% under certain assumptions."
  },
  {
    "year": "2014",
    "abstract": "The early success of wireless sensor networks has led to a new generation of increasingly sophisticated sensor network applications, such as HP’s CeNSE. These applications demand high network throughput that easily exceeds the capability of the low-power 802.15.4 radios most commonly used in today’s sensor nodes. To address this issue, this paper investigates an energy-efficient approach to supplementing an 802.15.4-based wireless sensor network with high bandwidth, high power, longer range radios, such as 802.11. Exploiting a key observation that the high-bandwidth radio achieves low energy consumption per bit of transmitted data due to its inherent transmission efficiency, we propose a hybrid network architecture that utilizes an optimal density of dual-radio (802.15.4 and 802.11) nodes to augment a sensor network having only 802.15.4 radios. We present a cross-layer mathematical model to calculate this optimal density, which strikes a delicate balance between the low energy consumption per transmitted bit of the high-bandwidth radio and low sleep power of the 802.15.4. Experimental results obtained using a wireless sensor network testbed reveal that our architecture improves the average energy per bit, time elapsed before the first node drains its battery, time elapsed before half of the nodes drain their batteries, and end-to-end delay by significant margins compared with a network having only 802.15.4."
  },
  {
    "year": "2014",
    "abstract": "This paper presents an overview of the cloud radio access network (C-RAN), which is a key enabler for future mobile networks in order to meet the explosive capacity demand of mobile traffic, and reduce the capital and operating expenditure burden faced by operators. We start by reviewing the requirements of future mobile networks, called 5G, followed by a discussion on emerging network concepts for 5G network architecture. Then, an overview of C-RAN and related works are presented. As a significant scenario of a 5G system, the ultra dense network deployment based on C-RAN is discussed with focuses on flexible backhauling, automated network organization, and advanced mobility management. Another import feature of a 5G system is the long-term coexistence of multiple radio access technologies (multi-RATs). Therefore, we present some directions and preliminary thoughts for future C-RAN-supporting Multi-RATs, including joint resource allocation, mobility management, as well as traffic steering and service mapping."
  },
  {
    "year": "2014",
    "abstract": "Machine type communications (MTCs) enable the communications of machines (devices) to machines over mobile networks. Besides simplifying our daily lives, the MTC business represents a promising market for mobile operators to increase their revenues. However, before a complete deployment of MTC over mobile networks, there is need to update the specifications of mobile networks in order to cope with the expected high number (massive deployment) of MTC devices. Indeed, large scale deployment of MTC devices represents an important challenge as a high number of MTC devices, simultaneously connecting to the mobile network, may cause congestion and system overload, which can degrade the network performance and even result in network node failures. Several activities have been led by 3GPP to alleviate system overload introduced by MTC. Most of the devised approaches represent only incremental solutions. Unlike these solutions, we devise a complete new architectural vision to support MTC in mobile networks. This vision relies on the marriage of mobile networks and cloud computing, specifically exploiting recent advances in network function virtualization (NFV). The aim of the proposed vision, namely LightEPC, is: 1) to orchestrate the on-demand creation of cloud-based lightweight mobile core networks dedicated for MTC and 2) to simplify the network attach procedure for MTC devices by creating only one NFV MTC function that groups all the usual procedures. By doing so, LightEPC is able to create and scale instances of NFV MTC functions on demand and in an elastic manner to cope with any sudden increase in traffic generated by MTC devices. To evaluate LightEPC, some preliminary analysis were conducted and the obtained analytical results indicate the ability of LightEPC in alleviating congestion and scaling up fast with massive numbers of MTC devices in mobile networks. Finally, a real-life implementation of LightEPC on top of cloud platform is discussed."
  },
  {
    "year": "2014",
    "abstract": "Regenerative braking is one of the most promising and environmentally friendly technologies used in electric and hybrid electric vehicles to improve energy efficiency and vehicle stability. This paper presents a systematic data-driven process for detecting and diagnosing faults in the regenerative braking system of hybrid electric vehicles. The diagnostic process involves signal processing and statistical techniques for feature extraction, data reduction for implementation in memory-constrained electronic control units, and variety of fault classification methodologies to isolate faults in the regenerative braking system. The results demonstrate that highly accurate fault diagnosis is possible with the classification methodologies. The process can be employed for fault analysis in a wide variety of systems, ranging from automobiles to buildings to aerospace systems."
  },
  {
    "year": "2014",
    "abstract": "In current computed tomography (CT) architecture, both X-ray tubes and X-ray detectors are rotated mechanically around an object to collect a sufficient number of projections. This architecture has been shown to not be fast enough for patients with high or irregular heart rates. Furthermore, both X-ray beams and detectors of the current architecture are made wide enough, so that the entire object is covered in the lateral direction without data truncation. Although novel acquisition protocols have recently been developed to reduce a radiation exposure, the high radiation dose from CT imaging remains a heightened public concern (especially for cardiac CT). The current CT architecture is a major bottleneck to further increase the temporal resolution and reduce the radiation dose. To overcome these problems, we present an innovative stationary-sources rotating-detectors CT (SSRD-CT) architecture based on the three stationary distributed X-ray sources and three smaller rotating X-ray detectors. Each distributed X-ray source has ~ 100 distinctive X-ray focal spots, and each detector has a narrower width compared with the conventional CT detectors. The SSRD-CT will have a field-of-view of 200 mm in diameter at isocenter, which is large enough to image many internal organs, including hearts. X-rays from the distributed sources are activated electronically to simulate the mechanical spinning of conventional single-beam X-ray sources with a high speed. The activation of individual X-ray beam will be synchronized to the corresponding rotating detector at the opposite end. Three source-detector chains can work in parallel to acquire three projections simultaneously and improve temporal resolution. Lower full-body radiation dose is expected for the proposed SSRD-CT because X-rays are restricted to irradiate a local smaller region. Taken together, the proposed SSRD-CT architecture will enable ≤50-ms temporal resolution and reduce radiation dose significantly."
  },
  {
    "year": "2014",
    "abstract": "Hurricanes regularly cause widespread and prolonged power outages along the U.S. coastline. These power outages have significant impacts on other infrastructure dependent on electric power and on the population living in the impacted area. Efficient and effective emergency response planning within power utilities, other utilities dependent on electric power, private companies, and local, state, and federal government agencies benefit from accurate estimates of the extent and spatial distribution of power outages in advance of an approaching hurricane. A number of models have been developed for predicting power outages in advance of a hurricane, but these have been specific to a given utility service area, limiting their use to support wider emergency response planning. In this paper, we describe the development of a hurricane power outage prediction model applicable along the full U.S. coastline using only publicly available data, we demonstrate the use of the model for Hurricane Sandy, and we use the model to estimate what the impacts of a number of historic storms, including Typhoon Haiyan, would be on current U.S. energy infrastructure."
  },
  {
    "year": "2014",
    "abstract": "In binary classification, two-way confusion matrices, with corresponding measures, such as sensitivity and specificity, have become so ubiquitous that those who review results may not realize there are other and more realistic ways to visualize data. This is, particularly, true when risk and reward considerations are important. The approach suggested here proposes that classification need not offer a conclusion on every instance within a data set. If an algorithm finds instances (e.g., patient cases in a medical data set) in which attributes pertaining to a patient's disease offer zero to nil information, there should be no classification offered. From the physician's perspective, disclosure of nil information should be welcome because it might prevent potentially harmful treatment. It follows from this that the developer of a classifier can provide summary results amendable for helping the consumer decide whether or not it is prudent to pass or act (commission versus omission). It is not always about balancing sensitivity and specificity in all cases, but optimizing action on some cases. The explanation is centered on John Kelly's link of gambling with Shannon information theory. In addition, Graham's margin of safety, Bernoulli's utiles, and Hippocratic Oath are important. An example problem is provided using a Netherlands Cancer Institute breast cancer data set. Recurrence score, a popular molecular-based assay for breast cancer prognosis, was found to have an uninformative zone. The uninformative subset had been grouped with positive results to garner higher sensitivity. Yet, because of a positive result, patients might be advised to undergo potentially harmful treatment in the absence of useful information."
  },
  {
    "year": "2014",
    "abstract": "This paper considers a downlink cloud radio access network (C-RAN) in which all the base-stations (BSs) are connected to a central computing cloud via digital backhaul links with finite capacities. Each user is associated with a user-centric cluster of BSs; the central processor shares the user's data with the BSs in the cluster, which then cooperatively serve the user through joint beamforming. Under this setup, this paper investigates the user scheduling, BS clustering, and beamforming design problem from a network utility maximization perspective. Differing from previous works, this paper explicitly considers the per-BS backhaul capacity constraints. We formulate the network utility maximization problem for the downlink C-RAN under two different models depending on whether the BS clustering for each user is dynamic or static over different user scheduling time slots. In the former case, the user-centric BS cluster is dynamically optimized for each scheduled user along with the beamforming vector in each time-frequency slot, whereas in the latter case, the user-centric BS cluster is fixed for each user and we jointly optimize the user scheduling and the beamforming vector to account for the backhaul constraints. In both cases, the nonconvex per-BS backhaul constraints are approximated using the reweighted ℓ1-norm technique. This approximation allows us to reformulate the per-BS backhaul constraints into weighted per-BS power constraints and solve the weighted sum rate maximization problem through a generalized weighted minimum mean square error approach. This paper shows that the proposed dynamic clustering algorithm can achieve significant performance gain over existing naive clustering schemes. This paper also proposes two heuristic static clustering schemes that can already achieve a substantial portion of the gain."
  },
  {
    "year": "2014",
    "abstract": "In this paper, a new dense dielectric (DD) patch array antenna prototype operating at 28 GHz for future fifth generation (5G) cellular networks is presented. This array antenna is proposed and designed with a standard printed circuit board process to be suitable for integration with radio frequency/microwave circuitry. The proposed structure employs four circular-shaped DD patch radiator antenna elements fed by a 1-to-4 Wilkinson power divider. To improve the array radiation characteristics, a ground structure based on a compact uniplanar electromagnetic bandgap unit cell has been used. The DD patch shows better radiation and total efficiencies compared with the metallic patch radiator. For further gain improvement, a dielectric layer of a superstrate is applied above the array antenna. The measured impedance bandwidth of the proposed array antenna ranges from 27 to beyond 32 GHz for a reflection coefficient (S11) of less than −10 dB. The proposed design exhibits stable radiation patterns over the whole frequency band of interest, with a total realized gain more than 16 dBi. Due to the remarkable performance of the proposed array, it can be considered as a good candidate for 5G communication applications."
  },
  {
    "year": "2014",
    "abstract": "Latent fingerprint has been used as evidence in the court of law for over 100 years. However, even today, a completely automated latent fingerprint system has not been achieved. Researchers have identified several important challenges in latent fingerprint recognition: 1) low information content; 2) presence of background noise and nonlinear ridge distortion; 3) need for an established scientific procedure for matching latent fingerprints; and 4) lack of publicly available latent fingerprint databases. The process of automatic latent fingerprint matching is divided into five definite stages, and this paper discusses the existing algorithms, limitations, and future research directions in each of the stages."
  },
  {
    "year": "2014",
    "abstract": "Cellular networks are a central part of today’s communication infrastructure. The global roll-out of 4G long-term evolution is underway, ideally enabling ubiquitous broadband Internet access. Mobile network operators, however, are currently facing an exponentially increasing demand for network capacity, necessitating densification of cellular base stations (keywords: small cells and heterogeneous networks) and causing a strongly deteriorated interference environment. Coordination among transmitters and receivers to mitigate and/or exploit interference is hence seen as a main path toward 5G mobile networks. We provide an overview of existing coordinated beamforming strategies for interference mitigation in broadcast and interference channels. To gain insight into their ergodic behavior in terms of signal to interference and noise ratio as well as achievable transmission rate, we focus on a simplified but representative scenario with two transmitters that serve two users. This analysis provides guidelines for selecting the best performing method depending on the particular transmission situation."
  },
  {
    "year": "2014",
    "abstract": "Heuristic methods or evolutionary algorithms (such as genetic algorithms and genetic programs) are common approaches applied in financial applications, such as trading systems. Determining the best time to buy or sell stocks in a stock market, and thereby maximizing profit with low risks, is an important issue in financial research. Recent studies have used trading rules based on technique analysis to address this problem. This method can determine trading times by analyzing the value of technical indicators. In other words, we can make trading rules by finding the trading value of technique indicators. An example of a trading rule would be, if one technical indicator’s value achieves the setting value, then either buy or sell. A combination of trading rules would become a trading strategy. The process of making trading strategies can be formulated as a combinational optimization problem. In this paper, we propose a novel method for applying a trading system. First, the proposed method uses the quantum-inspired Tabu search algorithm to find the optimal composition and combination of trading strategies. Second, this method uses a sliding window to avoid the major problem of over-fitting. The experiment results of earning money show much better performance than other approaches, and the proposed method outperforms the buy and hold method (which is a benchmark in this field)."
  },
  {
    "year": "2014",
    "abstract": "This paper studies the semantics of models for discrete physical phenomena, such as rigid body collisions and switching in electronic circuits. This paper combines generalized functions (specifically the Dirac delta function), superdense time, modal models, and constructive semantics to get a rich, flexible, efficient, and rigorous approach to modeling such systems. It shows that many physical scenarios that have been problematic for modeling techniques manifest as nonconstructive models, and that constructive versions of some of the models properly reflect uncertainty in the behavior of the physical systems that plausibly arise from the principles of the underlying physics. This paper argues that these modeling difficulties are not reasonably solved by more detailed continuous models of the underlying physical phenomena. Such more detailed models simply shift the uncertainty to other aspects of the model. Since such detailed models come with a high computational cost, there is little justification in using them unless the goal of modeling is specifically to understand these more detailed physical processes. All models in this paper are implemented in the Ptolemy II modeling and simulation environment and made available online."
  },
  {
    "year": "2014",
    "abstract": "One of the principal issues of alternative combustion modes for diesel engines (such as HCCI, PCCI, and LTC) is caused by the imbalances in the distribution of air and EGR across the cylinders, which affects the combustion process and ultimately cause significant differences in the pressure trace and indicated torque for each cylinder. In principle, a cylinder-by-cylinder control approach could compensate for air, residuals, and temperature imbalance. However, in order to fully benefit from closed-loop combustion control, it is necessary to obtain feedback signals from each engine cylinder to reconstruct the pressure trace. Therefore, cylinder imbalance is an issue that can be detected in a laboratory environment, wherein each engine cylinder is instrumented with a dedicated pressure transducer. This paper describes the framework and preliminary results of a model-based estimation approach to predict the individual pressure traces in a multicylinder engine relying on a very restricted sensor set, namely, a crankshaft speed sensor, a single production-grade pressure sensor. The objective of the estimator is to reconstruct the complete pressure trace during an engine cycle with sufficient accuracy to allow for detection of cylinder to cylinder imbalances. Starting from a model of the engine crankshaft dynamics, an adaptive sliding mode observer is designed to estimate the cylinder pressure from the crankshaft speed fluctuation measurement. The results obtained by the estimator are compared with experimental data obtained on a four-cylinder diesel engine."
  },
  {
    "year": "2014",
    "abstract": "In wireless communication, compressed vision information may suffer from kinds of degradation, which dramatically influences the final visual quality. In this paper, a compressed vision information restoration method is proposed based on two explored vision priors: 1) the cloud prior and 2) the local prior. The cloud prior can be obtained from the nature images set in the cloud, and fields of experts is used to formulate the statistical character of the nature image contents as a high order Markov random field. The local prior is achieved from the degraded image itself, and K-SVD is adopted to model the sparse and redundant representation characters of nature images. These priors are effectively comprised in the proposed vision information restoration method. The relation between the quantization parameter and the optimal configuration of the prior models is further analyzed. In addition, an enhanced quantization constrained projection algorithm is proposed to refine the high frequency components. We extend this paper to compressed video restoration for H.264/AVC and the experiment results demonstrate that the proposed scheme can reproduce higher quality images compared with conventional H.264/AVC."
  },
  {
    "year": "2014",
    "abstract": "Carbon nanotube (CNT)-based multibeam X-ray tubes provide an array of individually controllable X-ray focal spots. The CNT tube allows for flexible placement and distribution of X-ray focal spots in a system. Using a CNT tube, a computed tomography (CT) system with a noncircular geometry and a nonrotating gantry can be created. The noncircular CT geometry can be optimized around a specific imaging problem, utilizing the flexibility of CNT multibeam X-ray tubes to achieve the optimal focal spot distribution for the design constraints of the problem. Iterative reconstruction algorithms provide flexible CT reconstruction to accommodate the noncircular geometry. Compressed sensing-based iterative reconstruction algorithms apply a sparsity constraint to the reconstructed images that can partially account for missing angular coverage due to the noncircular geometry. In this paper, we present a laboratory prototype CT system that uses CNT multibeam X-ray tubes; a rectangular, nonrotating imaging geometry; and an accelerated compressed sensing-based iterative reconstruction algorithm. We apply a total variation minimization as our sparsity constraint. We present the advanced CNT multibeam tubes and show the stability and flexibility of these new tubes. We also present the unique imaging geometry and discuss the design constraints that influenced the specific system design. The reconstruction method is presented along with an overview of the acceleration of the algorithm to near real-time reconstruction. We demonstrate that the prototype reconstructed images have image quality comparable with a conventional CT system. The prototype is optimized for airport checkpoint baggage screening, but the concepts developed may apply to other application-specific CT imaging systems."
  },
  {
    "year": "2014",
    "abstract": "Fluid accumulation inside the lungs, known as cardiac pulmonary edema, is one of the main early symptoms of congestive heart failure (CHF). That accumulation causes significant changes in the electrical properties of the lung tissues, which in turn can be detected using microwave techniques. To that end, the design and implementation of an automated ultrahigh-frequency microwave-based system for CHF detection and monitoring is presented. The hardware of the system consists of a wideband folded antenna attached to a fully automated vertical scanning platform, compact microwave transceiver, and laptop. The system includes software in the form of operational control, signal processing, and visualizing algorithms. To detect CHF, the system is designed to vertically scan the rear side of the human torso in a monostatic radar approach. The collected data from the scanning is then visualized in the time domain using the inverse Fourier transform. These images show the intensity of the reflected signals from different parts of the torso. Using a differential based detection technique, a threshold is defined to differentiate between healthy and unhealthy cases. This paper includes details of developing the automated platform, designing the antenna with the required properties imposed by the system, developing a signal processing algorithm, and introducing differential detection technique besides investigating miscellaneous probable CHF cases."
  },
  {
    "year": "2014",
    "abstract": "Smartphones and tablets are finding their way into healthcare delivery to the extent that mobile health (mHealth) has become an identifiable field within eHealth. In prior work, a mobile app to document chronic wounds and wound care, specifically pressure ulcers (bedsores) was developed for Android smartphones and tablets. One feature of the mobile app allowed users to take images of the wound using the smartphone or tablet’s integrated camera. In a user trial with nurses at a personal care home, this feature emerged as a key benefit of the mobile app. This paper developed image analysis algorithms that facilitate noncontact measurements of irregularly shaped images (e.g., wounds), where the image is taken with a sole smartphone or tablet camera. The image analysis relies on the sensors integrated in the smartphone or tablet with no auxiliary or add-on instrumentation on the device. Three approaches to image analysis were developed and evaluated: 1) computing depth using autofocus data; 2) a custom sensor fusion of inertial sensors and feature tracking in a video stream; and 3) a custom pinch/zoom approach. The pinch/zoom approach demonstrated the strongest potential and thus developed into a fully functional prototype complete with a measurement mechanism. While image analysis is a very well developed field, this paper contributes to image analysis applications and implementation in mHealth, specifically for wound care."
  },
  {
    "year": "2014",
    "abstract": "The exponential growth of mobile data in macronetworks has driven the evolution of communications systems toward spectrally efficient, energy efficient, and fast local area communications. It is a well-known fact that the best way to increase capacity in a unit area is to introduce smaller cells. Local area communications are currently mainly driven by the IEEE 802.11 WLAN family being cheap and energy efficient with a low number of users per access point. For the future high user density scenarios, following the 802.11 HEW study group, the 802.11ax project has been initiated to improve the WLAN system performance. The 3GPP LTE-advanced (LTE-A) also includes new methods for pico and femto cell’s interference management functionalities for small cell communications. The main problem with LTE-A is, however, that the physical layer numerology is still optimized for macrocells and not for local area communications. Furthermore, the overall complexity and the overheads of the control plane and reference symbols are too large for spectrally and energy efficient local area communications. In this paper, we provide first an overview of WLAN 802.11ac and LTE/LTE-A, discuss the pros and cons of both technology areas, and then derive a new flexible TDD-based radio interface parametrization for 5G local area communications combining the best practices of both WiFi and LTE-A technologies. We justify the system design based on local area propagation characteristics and expected traffic distributions and derive targets for future local area concepts. We concentrate on initial physical layer design and discuss how it maps to higher layer improvements. This paper shows that the new design can significantly reduce the latency of the system, and offer increased sleeping opportunities on both base station and user equipment sides leading to enhanced power savings. In addition, through careful design of the control overhead, we are able to improve the channel utilization when compared w..."
  },
  {
    "year": "2014",
    "abstract": "Large-scale, or massive, multiple-input multiple-output (MIMO) systems are typified by the number of antennas contributing to a communication link. This type of link can consist of single nodes with a large number of antennas or a large number of cooperating nodes—each contributing a small number of antennas. Such massive systems naturally lead to link topologies that are not often considered in studies of smaller scale cooperative MIMO scenarios. For this system to be economically practical, each node participating in the massive link likely has limited transmit power capability, and therefore properly limiting the per-node transmit power must be incorporated into the signal processing algorithm. This paper develops a generalized multiuser massive MIMO (G4M) optimization algorithm for colocated or cooperative signaling, subject to any sum-, per-antenna, or per-node power constraint, and that can also accommodate nonlinear precoding and detection and any number of antennas. Using the G4M algorithm, a number of topologies unique to cooperative massive MIMO are described, demonstrating the facility this algorithm provides in optimizing the performance of multiuser massive links with atypical topologies."
  },
  {
    "year": "2014",
    "abstract": "This paper proposes a novel graph-based multicell scheduling framework to efficiently mitigate downlink intercell interference in OFDMA-based small cell networks. We define a graph-based optimization framework based on interference condition between any two users in the network assuming they are served on similar resources. Furthermore, we prove that the proposed framework obtains a tight lower bound for conventional weighted sum-rate maximization problem in practical scenarios. Thereafter, we decompose the optimization problem into dynamic graph-partitioning-based subproblems across different subchannels and provide an optimal solution using branch-and-cut approach. Subsequently, due to high complexity of the solution, we propose heuristic algorithms that display near optimal performance. At the final stage, we apply cluster-based resource allocation per subchannel to find candidate users with maximum total weighted sum-rate. A case study on networked small cells is also presented with simulation results showing a significant improvement over the state-of-the-art multicell scheduling benchmarks in terms of outage probability as well as average cell throughput."
  },
  {
    "year": "2014",
    "abstract": "The virtual machine (VM) is the most basic unit for virtualization and resource allocation. The study of VM power metering is the key to reducing the power consumption of data centers. In this paper, we make a comprehensive investigation in issues regarding VM power metering, including server models, sampling, VM power metering methods, and the accuracy of the methods. We will review many up-to-date power metering methods in this paper, and analyze their efficiencies, as well as evaluate their performance. Open research issues, such as VM service billing, power budgeting, and energy saving scheduling, are discussed, with an objective to spark new research interests in this field."
  },
  {
    "year": "2014",
    "abstract": "X-ray luminescence and X-ray fluorescence computed tomography (CT) are two emerging technologies in X-ray imaging that provide functional and molecular imaging capability. Both emission-type tomographic imaging modalities use external X-rays to stimulate secondary emissions, either light or secondary X-rays, which are then acquired for tomographic reconstruction. These modalities surpass the limits of sensitivity in current X-ray imaging and have the potential of enabling X-ray imaging to extract molecular imaging information. These new modalities also promise to break through the spatial resolution limits of other in vivo molecular imaging modalities. This paper reviews the development of X-ray luminescence and X-ray fluorescence CT and their relative merits. The discussion includes current problems and future research directions and the role of these modalities in future molecular imaging applications."
  },
  {
    "year": "2014",
    "abstract": "Continuous electrowetting (CEW) is demonstrated to be an effective actuation mechanism for reconfigurable radio frequency (RF) devices that use non-toxic liquid-metal tuning elements. Previous research has shown CEW is an efficient means of electrically inducing motion in a liquid-metal slug, but precise control of the slug’s position within fluidic channels has not been demonstrated. Here, the precise positioning of liquid-metal slugs is achieved using CEW actuation in conjunction with channels designed to minimize the liquid-metal surface energy at discrete locations. This approach leverages the high surface tension of liquid metal to control its resting position with submillimeter accuracy. The CEW actuation and fluidic channel design were optimized to create reconfigurable RF devices. In addition, solutions for the reliable actuation of a gallium-based, non-toxic liquid-metal alloy (Galinstan) are presented that mitigate the tendency of the alloy to form a surface oxide layer capable of wetting to the channel walls, inhibiting motion. A reconfigurable slot antenna utilizing these techniques to achieve a 15.2% tunable frequency bandwidth is demonstrated."
  },
  {
    "year": "2014",
    "abstract": "Data center networks (DCNs) for 5G are expected to support a large number of different bandwidth-hungry applications with exploding data, such as real-time search and data analysis. As a result, significant challenges are imposed to identify the cause of link congestion between any pair of switch ports that may severely damage the overall network performance. Generally, it is expected that the granularity of the flow monitoring to diagnose network congestion in 5G DCNs needs to be down to the flow level on a physical port of a switch in real time with high-estimation accuracy, low-computational complexity, and good scalability. In this paper, motivated by a comprehensive study of a real DCN trace, we propose two sketch-based algorithms, calledα-conservative update (CU) and P(d)-CU, based on the existing CU approach.α-CU adds no extra implementation cost to the traditional CU, but successfully trades off the achieved error with time complexity. P(d)-CU fully considers the amount of skew for different types of network services to aggregate traffic statistics of each type of network traffic at an individual, horizontally partitioned sketch. We also introduce a way to produce the real-time moving average of the reported results. By theoretical analysis and sufficient experimental results on a real DCN trace, we extensively evaluate the proposed and existing algorithms on their error performance, recall, space cost, and time complexity."
  },
  {
    "year": "2014",
    "abstract": "The massive multiple-input multiple-output (MIMO) system has drawn increasing attention recently as it is expected to boost the system throughput and result in lower costs. Previous studies mainly focus on time division duplexing (TDD) systems, which are more amenable to practical implementations due to channel reciprocity. However, there are many frequency division duplexing (FDD) systems deployed worldwide. Consequently, it is of great importance to investigate the design and performance of FDD massive MIMO systems. To reduce the overhead of channel estimation in FDD systems, a two-stage precoding scheme was recently proposed to decompose the precoding procedure into intergroup precoding and intragroup precoding. The problem of user grouping and scheduling thus arises. In this paper, we first propose three novel similarity measures for user grouping based on weighted likelihood, subspace projection, and Fubini-Study, respectively, as well as two novel clustering methods, including hierarchical and K-medoids clustering. We then propose a dynamic user scheduling scheme to further enhance the system throughput once the user groups are formed. The load balancing problem is considered when few users are active and solved with an effective algorithm. The efficacy of the proposed schemes are validated with theoretical analysis and simulations."
  },
  {
    "year": "2014",
    "abstract": "This paper presents the latest progress on cloud RAN (C-RAN) in the areas of centralization and virtualization. A C-RAN system centralizes the baseband processing resources into a pool and virtualizes soft base-band units on demand. The major challenges for C-RAN including front-haul and virtualization are analyzed with potential solutions proposed. Extensive field trials verify the viability of various front-haul solutions, including common public radio interface compression, single fiber bidirection and wavelength-division multiplexing. In addition, C-RANs facilitation of coordinated multipoint (CoMP) implementation is demonstrated with 50%-100% uplink CoMP gain observed in field trials. Finally, a test bed is established based on general purpose platform with assisted accelerators. It is demonstrated that this test bed can support multi-RAT, i.e., Time-Division Duplexing Long Term Evolution, Frequency-Division Duplexing Long Term Evolution, and Global System for Mobile Communications efficiently and presents similar performance to traditional systems."
  },
  {
    "year": "2014",
    "abstract": "Wireless networks have evolved from 1G to 4G networks, allowing smart devices to become important tools in daily life. The 5G network is a revolutionary technology that can change consumers’ Internet use habits, as it creates a truly wireless environment. It is faster, with better quality, and is more secure. Most importantly, users can truly use network services anytime, anywhere. With increasing demand, the use of bandwidth and frequency spectrum resources is beyond expectations. This paper found that the frequency spectrum and network information have considerable relevance; thus, spectrum utilization and channel flow interactions should be simultaneously considered. We considered that software defined radio (SDR) and software defined networks (SDNs) are the best solution. We propose a cross-layer architecture combining SDR and SDN characteristics. As the simulation evaluation results suggest, the proposed architecture can effectively use the frequency spectrum and considerably enhance network performance. Based on the results, suggestions are proposed for follow-up studies on the proposed architecture."
  },
  {
    "year": "2014",
    "abstract": "We present a UWB and spread spectrum communications method based on the idea of time compression where a sampled message signal is transmitted at a higher sampling rate. Robustness is achieved by dividing the signal into overlapping segments, transmitting each segment fast enough so that the segments no longer overlap, receiving these segments and reconstructing the message by overlap-adding the segments. A key feature of this scheme is that an exact sample rate match is not required to recover the signal. This method is implemented in a custom wideband software defined radio, with good results in the presence of interference and multipath. This method, referred to as time compression overlap-add (TC-OLA), represents a new concept and design approach and an advance in fundamental technology of the air interface physical layer that may be relevant to 5G wireless technologies."
  },
  {
    "year": "2014",
    "abstract": "One of the goals of neuromorphic engineering is to imitate the brain’s ability to recognize and count the number of individual objects as entities based on the global consistency of the information from the population of activated tactile (or visual) sensory neurons whatever the objects’ shapes are. To achieve this flexibility, it may be worth examining an unconventional algorithm such as topological methods. Here, we propose a fully parallelized algorithm for a shape-invariant touch counter for 2-D pixels. The number of touches is counted by the Euler integral, a generalized integral, in which a connected component counter (Betti number) for the binary image was used as elemental module. Through examples of touches, we demonstrate transparently how the proposed circuit architecture embodies the Euler integral in the form of recurrent neural networks for iterative vector operations. Our parallelization can lead the way to Field-Programmable Gate Array or Digital Signal Processor implementations of topological algorithms with scalability to high resolutions of pixels."
  },
  {
    "year": "2014",
    "abstract": "Face recognition is an interesting and a challenging problem that has been widely studied in the field of pattern recognition and computer vision. It has many applications such as biometric authentication, video surveillance, and others. In the past decade, several methods for face recognition were proposed. However, these methods suffer from pose and illumination variations. In order to address these problems, this paper proposes a novel methodology to recognize the face images. Since image gradients are invariant to illumination and pose variations, the proposed approach uses gradient orientation to handle these effects. The Schur decomposition is used for matrix decomposition and then Schurvalues and Schurvectors are extracted for subspace projection. We call this subspace projection of face features as Schurfaces, which is numerically stable and have the ability of handling defective matrices. The Hausdorff distance is used with the nearest neighbor classifier to measure the similarity between different faces. Experiments are conducted with Yale face database and ORL face database. The results show that the proposed approach is highly discriminant and achieves a promising accuracy for face recognition than the state-of-the-art approaches."
  },
  {
    "year": "2014",
    "abstract": "Pervasive computing and Internet of Things (IoTs) paradigms have created a huge potential for new business. To fully realize this potential, there is a need for a common way to abstract the heterogeneity of devices so that their functionality can be represented as a virtual computing platform. To this end, we present novel semantic level interoperability architecture for pervasive computing and IoTs. There are two main principles in the proposed architecture. First, information and capabilities of devices are represented with semantic web knowledge representation technologies and interaction with devices and the physical world is achieved by accessing and modifying their virtual representations. Second, global IoT is divided into numerous local smart spaces managed by a semantic information broker (SIB) that provides a means to monitor and update the virtual representation of the physical world. An integral part of the architecture is a resolution infrastructure that provides a means to resolve the network address of a SIB either using a physical object identifier as a pointer to information or by searching SIBs matching a specification represented with SPARQL. We present several reference implementations and applications that we have developed to evaluate the architecture in practice. The evaluation also includes performance studies that, together with the applications, demonstrate the suitability of the architecture to real-life IoT scenarios. In addition, to validate that the proposed architecture conforms to the common IoT-A architecture reference model (ARM), we map the central components of the architecture to the IoT-ARM."
  },
  {
    "year": "2014",
    "abstract": "Wireless systems have become more and more advanced in terms of handling the statistical properties of wireless channels. For example, the 4G long term evolution (LTE) system takes advantage of multiport antennas [multiple-input multiple-output (MIMO) technology] and orthogonal frequency division multiplexing (OFDM) to improve the detection probability of single bitstream by diversity in the spatial and frequency domains, respectively. The 4G system also supports transmission of two bitstreams by appropriate signal processing of the MIMO subchannels. The reverberation chamber emulates according to previous works rich isotropic multipath (RIMP) and has proven to be very useful for characterizing smart phones for LTE systems. The measured throughput can be accurately modeled by the simple digital threshold receiver, accounting accurately for both the MIMO and OFDM functions. The throughput is equivalent to the probability of detection (PoD) of the transmitted bitstream. The purpose of this paper is to introduce a systematic approach to include the statistical properties of the user and his or her terminal, when characterizing the performance. The user statistics will have a larger effect in environments with stronger line-of-sight (LOS), because the angle of arrival and the polarization of the LOS contribution vary due to the user’s orientation and practices. These variations are stochastic, and therefore, we introduce the term random-LOS to describe this. This paper elaborates on the characterization of an example antenna in both RIMP and random-LOS. The chosen antenna is a wideband microbase transceiver station (BTS) antenna. We show how to characterize the micro-BTS by the PoD of one and two bitstreams in both RIMP and random-LOS, by considering the user randomly located and oriented within the angular coverage sector. We limit the treatment to a wall-mounted BTS antenna, and assume a desired hemispherical coverage. The angular coverages of both one and two bitstre..."
  },
  {
    "year": "2014",
    "abstract": "A new regularization technique for graph Laplacians arising from triangular meshes of closed and open structures is presented. The new technique is based on the analysis of graph Laplacian spectrally equivalent operators in terms of Sobolev norms and on the appropriate selection of operators of opposite differential strength to achieve a multiplicative regularization. In addition, a new 3-D/2-D nested regularization strategy is presented to deal with open geometries. Numerical results show the advantages of the proposed regularization as well as its effectiveness when used in spectral partitioning applications."
  },
  {
    "year": "2014",
    "abstract": "The increasing use of smartphones, tablets, and other mobile devices poses a significant challenge in providing effective online security. CAPTCHAs, tests for distinguishing human and computer users, have traditionally been popular; however, they face particular difficulties in a modern mobile environment because most of them rely on keyboard input and have language dependencies. This paper proposes a novel image-based CAPTCHA that combines the touch-based input methods favored by mobile devices with genetically optimized face detection tests to provide a solution that is simple for humans to solve, ready for worldwide use, and provides a high level of security by being resilient to automated computer attacks. In extensive testing involving over 2600 users and 40000 CAPTCHA tests, fgCAPTCHA demonstrates a very high human success rate while ensuring a 0% attack rate using three well-known face detection algorithms."
  },
  {
    "year": "2014",
    "abstract": "Event-based mobile social networks (MSNs) are a special type of MSN that has an immanently temporal common feature, which allows any smart phone user to create events to share group messaging, locations, photos, and insights among participants. The emergence of Internet of Things and event-based social applications integrated with context-awareness ability can be helpful in planning and organizing social events like meetings, conferences, and tradeshows. This paper first provides review of the event-based social networks and the basic principles and architecture of event-based MSNs. Next, event-based MSNs with smartphone contained technology elements, such as context-aware mobility and multimedia sharing, are presented. By combining the feature of context-aware mobility with multimedia sharing in event-based MSNs, event organizers, and planners with the service providers optimize their capability to recognize value for the multimedia services they deliver. The unique features of the current event-based MSNs give rise to the major technology trends to watch for designing applications. These mobile applications and their main features are described. At the end, discussions on the evaluation of the event-based mobile applications based on their main features are presented. Some open research issues and challenges in this important area of research are also outlined."
  },
  {
    "year": "2014",
    "abstract": "Deep learning is currently an extremely active research area in machine learning and pattern recognition society. It has gained huge successes in a broad area of applications such as speech recognition, computer vision, and natural language processing. With the sheer size of data available today, big data brings big opportunities and transformative potential for various sectors; on the other hand, it also presents unprecedented challenges to harnessing data and information. As the data keeps getting bigger, deep learning is coming to play a key role in providing big data predictive analytics solutions. In this paper, we provide a brief overview of deep learning, and highlight current research efforts and the challenges to big data, as well as the future trends."
  },
  {
    "year": "2014",
    "abstract": "The compressive sensing (CS) theory shows that real signals can be exactly recovered from very few samplings. Inspired by the CS theory, the interior problem in computed tomography is proved uniquely solvable by minimizing the region-of-interest’s total variation if the imaging object is piecewise constant or polynomial. This is called CS-based interior tomography. However, the CS-based algorithms require high computational cost due to their iterative nature. In this paper, a graphics processing unit (GPU)-based parallel computing technique is applied to accelerate the CS-based interior reconstruction for practical application in both fan-beam and cone-beam geometries. Our results show that the CS-based interior tomography is able to reconstruct excellent volumetric images with GPU acceleration in a few minutes."
  },
  {
    "year": "2014",
    "abstract": "The formation control technique called cluster space control promotes simplified specification and monitoring of the motion of mobile multirobot systems of limited size. Previous paper has established the conceptual foundation of this approach and has experimentally verified and validated its use for various systems implementing kinematic controllers. In this paper, we briefly review the definition of the cluster space framework and introduce a new cluster space dynamic model. This model represents the dynamics of the formation as a whole as a function of the dynamics of the member robots. Given this model, generalized cluster space forces can be applied to the formation, and a Jacobian transpose controller can be implemented to transform cluster space compensation forces into robot-level forces to be applied to the robots in the formation. Then, a nonlinear model-based partition controller is proposed. This controller cancels out the formation dynamics and effectively decouples the cluster space variables. Computer simulations and experimental results using three autonomous surface vessels and four land rovers show the effectiveness of the approach. Finally, sensitivity to errors in the estimation of cluster model parameters is analyzed."
  },
  {
    "year": "2014",
    "abstract": "Source code revision history visualization tools have been around for over two decades. Yet, they have not become a mainstream tool in a typical programmer’s toolbox and are not typically available in intergraded development environments. So, do they really work? And if they do, what would it take to put them to work? This paper seeks to answer these two questions through experiments, surveys, and interviews. A source code history visualization tool named TeamWATCH was implemented to visualize subversion code repositories. Two comparative controlled experiments were conducted to evaluate the effectiveness of TeamWATCH. The experimental results showed that the subjects using TeamWATCH spent less time than subjects using the command-line subversion client and TortoiseSVN in answering the same set of questions regarding source code revision history. In addition, surveys and interviews were conducted to identify obstacles in adopting source code history visualization tools. Collectively, the results show that source code history visualization tools do bring value to programmers. Key obstacles to wider adoption in practice include nontrivial overhead in using the tools and perceived complexity in visualization."
  },
  {
    "year": "2014",
    "abstract": "Theℓ1regularization problem has been widely used to solve the sparsity constrained problems. To enhance the sparsity constraint for better imaging performance, a promising direction is to use theℓpnorm (0<p<1) and solve theℓpminimization problem. Very recently, Xu et al. developed an analytic solution for theℓ1/2regularization via an iterative thresholding operation, which is also referred to as half-threshold filtering. In this paper, we design a simultaneous algebraic reconstruction technique (SART)-type half-threshold filtering framework to solve the computed tomography (CT) reconstruction problem. In the medical imaging filed, the discrete gradient transform (DGT) is widely used to define the sparsity. However, the DGT is noninvertible and it cannot be applied to half-threshold filtering for CT reconstruction. To demonstrate the utility of the proposed SART-type half-threshold filtering framework, an emphasis of this paper is to construct a pseudoinverse transforms for DGT. The proposed algorithms are evaluated with numerical and physical phantom data sets. Our results show that the SART-type half-threshold filtering algorithms have great potential to improve the reconstructed image quality from few and noisy projections. They are complementary to the counterparts of the state-of-the-art soft-threshold filtering and hard-threshold filtering."
  },
  {
    "year": "2014",
    "abstract": "Self-organizing networks act autonomously for the sake of achieving the best possible performance. The attainable routing depends on a delicate balance of diverse and often conflicting quality-of-service requirements. Finding the optimal solution typically becomes an nonolynomial-hard problem, as the network size increases in terms of the number of nodes. Moreover, the employment of user-defined utility functions for the aggregation of the different objective functions often leads to suboptimal solutions. On the other hand, Pareto optimality is capable of amalgamating the different design objectives by providing an element of elitism. Although there is a plethora of bioinspired algorithms that attempt to address this optimization problem, they often fail to generate all the points constituting the optimal Pareto front. As a remedy, we propose an optimal multiobjective quantum-assisted algorithm, namely the nondominated quantum optimization algorithm (NDQO), which evaluates the legitimate routes using the concept of Pareto optimality at a reduced complexity. We then compare the performance of the NDQO algorithm to the state-of-the-art evolutionary algorithms, demonstrating that the NDQO algorithm achieves a near-optimal performance. Furthermore, we analytically derive the upper and lower bounds of the NDQO algorithmic complexity, which is of the order ofO(N)andO(NN−−√)in the best and worst case scenario, respectively. This corresponds to a substantial complexity reduction of the NDQO from the order ofO(N2)imposed by the brute-force method."
  },
  {
    "year": "2014",
    "abstract": "Recent technological advancements have led to a deluge of data from distinctive domains (e.g., health care and scientific sensors, user-generated data, Internet and financial companies, and supply chain systems) over the past two decades. The term big data was coined to capture the meaning of this emerging trend. In addition to its sheer volume, big data also exhibits other unique characteristics as compared with traditional data. For instance, big data is commonly unstructured and require more real-time analysis. This development calls for new system architectures for data acquisition, transmission, storage, and large-scale data processing mechanisms. In this paper, we present a literature survey and system tutorial for big data analytics platforms, aiming to provide an overall picture for nonexpert readers and instill a do-it-yourself spirit for advanced audiences to customize their own big-data solutions. First, we present the definition of big data and discuss big data challenges. Next, we present a systematic framework to decompose big data systems into four sequential modules, namely data generation, data acquisition, data storage, and data analytics. These four modules form a big data value chain. Following that, we present a detailed survey of numerous approaches and mechanisms from research and industry communities. In addition, we present the prevalent Hadoop framework for addressing big data challenges. Finally, we outline several evaluation benchmarks and potential research directions for big data systems."
  },
  {
    "year": "2014",
    "abstract": "Effectively confronting device and circuit parameter variations to maintain or improve the design of high performance and energy efficient systems while satisfying historical standards for reliability and lower costs is increasingly challenging with the scaling of technology. In this paper, we develop methods for robust and resilient six-transistor-cell static random access memory (6T-SRAM) designs that mitigate the effects of device and circuit parameter variations. Our interdisciplinary effort involves: 1) using our own recently developed VAR-TX model [1] to illustrate the impact of interdie (also known as die-to-die, D2D) and intradie (also know as within-die, WID) process and operation variations—namely threshold voltage (Vth), gate length (L), and supply voltage (Vdd)—on future different 16-nm architectures and 2) using modified versions of other well-received models to illustrate the impact of variability due to temperature, negative bias temperature instability, aging, and so forth, on existing and next-generation technology nodes. Our goal in combining modeling techniques is to help minimize all major types of variability and to consequently predict and optimize speed and yield for the next generation 6T-SRAMs."
  },
  {
    "year": "2014",
    "abstract": "In this paper, we introduce a novel but intuitive scheme to recover multiple signals of interest (SoI) from multiple emitters in signal collection applications such as signal intelligence, electronic intelligence, and communications intelligence. We consider a case where the SoIs form a heavy interference environment. The scheme, which is referred to as reference-based successive interference cancellation (RSIC), involves a combination of strategic receiver placement and signal processing techniques. The scheme works by placing a network of cooperative receivers where each receiver catches its own SoI (despite multiple interferences). The first receiver demodulates the initial SoI (called a reference signal) and forwards it to the second receiver. The second receiver collects a received signal containing the second SoI but is interfered with by the initial SoI, which is a problem called co-channel interference in cellular communications. Unfortunately, the amplitude scaling of the interference is unknown in the second receiver and therefore has to be estimated via least squares error. It turns out that the estimation requires a priori knowledge of the second SoI, which is the very signal it tries to demodulate, thereby yielding a Catch-22 problem. We propose using an initial guess on the second SoI to form an amplitude estimate such that the interference is subtracted (cancelled) from the collected measurement at the second receiver. The procedure is applied to a third receiver (or multiple receivers) until the last of the desired SoI is separated from all of the co-channel interferences. The RSIC scheme performs well. Using quaternary phase shift keying as example modulation, we present major symbol error rate (SER) performance improvements with the use of RSIC over the highly degraded SER of receivers that are heavily interfered and do not employ any cancellation technique."
  },
  {
    "year": "2014",
    "abstract": "This paper presents a novel approach that employs classification to enhance the accuracy of the stereo matching problem. First, the images are treated in order to improve their pixel to pixel correspondence and reduce illumination differences. After that, stereo matching is addressed using different methods with emphasis on local ones like the sum of absolute distances and normalized cross correlation. Other state-of-the-art approaches are also considered. Then, and for every pixel, different features are computed from the input stereo image and the initially found depth map. Afterward, boosting and neural networks, as classification methods, are used to handle occlusion and enhance stereo matching by finding the erroneous disparity values. These values are then corrected through a completion stage. The accuracy of the proposed implementation improves on the problem in an efficient manner. A timing analysis of the method is provided to validate the real time performance. This paper further clarifies some of the possible developments based on various discussions."
  },
  {
    "year": "2014",
    "abstract": "Wireless sensor networks (WSNs) have been proliferating due to their wide applications in both military and commercial use. However, one critical challenge to WSNs implementation is source location privacy. In this paper, we propose a novel tree-based diversionary routing scheme for preserving source location privacy using hide and seek strategy to create diversionary or decoy routes along the path to the sink from the real source, where the end of each diversionary route is a decoy (fake source node), which periodically emits fake events. Meanwhile, the proposed scheme is able to maximize the network lifetime of WSNs. The main idea is that the lifetime of WSNs depends on the nodes with high energy consumption or hotspot, and then the proposed scheme minimizes energy consumption in hotspot and creates redundancy diversionary routes in nonhotspot regions with abundant energy. Hence, it achieves not only privacy preservation, but also network lifetime maximization. Furthermore, we systematically analyze the energy consumption in WSNs, and provide guidance on the number of diversionary routes, which can be created in different regions away from the sink. In addition, we identify a novel attack against phantom routing, which is widely used for source location privacy preservation, namely, direction-oriented attack. We also perform a comprehensive analysis on how the direction-oriented attack can be defeated by the proposed scheme. Theoretical and experimental results show that our scheme is very effective to improve the privacy protection while maximizing the network lifetime."
  },
  {
    "year": "2014",
    "abstract": "This paper describes the most important topics on automation modernization of electrical repowering system of oil refinery in Mexico with IEC-61850. A strategic automation approach for protection, metering, and control is presented, as well as a description on how to automate the electrical equipment in order to reach a thorough communication level according to the standard IEC-61850, including station, bay, and, process buses and their interoperability concepts. A proposal for process bus, considering several scenarios with merging units interface, is presented. On the other hand, the repowering electrical system have several energy sources, with different conditions and challenges to solve, one of them is the integration of equipment with IEC 61850 protocols, other scenario is the system synchronization. Furthermore, the benefits on improvement in power quality and a scheme to avoid the high costs of maintenance are discussed. With the upgrading of the electrical system, the electrical demand is considered cover for all refining processes, including the new production plant of ultralow sulfur clean gasoline."
  },
  {
    "year": "2014",
    "abstract": "From an engineering point-of-view, cognitive control is inspired by the prefrontal cortex of the human brain; cognitive control may therefore be viewed as the overarching function of a cognitive dynamic system. In this paper, we describe a new way of thinking about cognitive control that embodies two basic components: learning and planning, both of which are based on two notions: 1) two-state model of the environment and the perceptor and 2) perception-action cycle, which is a distinctive characteristic of the cognitive dynamic system. Most importantly, it is shown that the cognitive control learning algorithm is a special form of Bellman's dynamic programming. Distinctive properties of the new algorithm include the following: 1) optimality of performance; 2) algorithmic convergence to optimal policy; and 3) linear law of complexity measured in terms of the number of actions taken by the cognitive controller on the environment. To validate these intrinsic properties of the algorithm, a computational experiment is presented, which involves a cognitive tracking radar that is known to closely mimic the visual brain. The experiment illustrates two different scenarios: 1) the impact of planning on learning curves of the new cognitive controller and 2) comparison of the learning curves of three different controllers, based on dynamic optimization, traditionalQ-learning, and the new algorithm. The latter two algorithms are based on the two-state model, and they both involve the use of planning."
  },
  {
    "year": "2014",
    "abstract": "A number of impedance-based fault location algorithms have been developed for estimating the distance to faults in a transmission network. Each algorithm has specific input data requirements and makes certain assumptions that may or may not hold true in a particular fault location scenario. Without a detailed understanding of the principle of each fault-locating method, choosing the most suitable fault location algorithm can be a challenging task. This paper, therefore, presents the theory of one-ended (simple reactance, Takagi, modified Takagi, Eriksson, and Novosel et al.) and two-ended (synchronized, unsynchronized, and current-only) impedance-based fault location algorithms and demonstrates their application in locating real-world faults. The theory details the formulation and input data requirement of each fault-locating algorithm and evaluates the sensitivity of each to the following error sources: 1) load; 2) remote infeed; 3) fault resistance; 4) mutual coupling; 5) inaccurate line impedances; 6) DC offset and CT saturation; 7) three-terminal lines; and 8) tapped radial lines. From the theoretical analysis and field data testing, the following criteria are recommended for choosing the most suitable fault-locating algorithm: 1) data availability and 2) fault location application scenario. Another objective of this paper is to assess what additional information can be gleaned from waveforms recorded by intelligent electronic devices (IEDs) during a fault. Actual fault event data captured in utility networks is exploited to gain valuable feedback about the transmission network upstream from the IED device, and estimate the value of fault resistance."
  },
  {
    "year": "2014",
    "abstract": "Gaussian Processes (GPs) are Bayesian nonparametric models that are becoming more and more popular for their superior capabilities to capture highly nonlinear data relationships in various tasks, such as dimensionality reduction, time series analysis, novelty detection, as well as classical regression and classification tasks. In this paper, we investigate the feasibility and applicability of GP models for music genre classification and music emotion estimation. These are two of the main tasks in the music information retrieval (MIR) field. So far, the support vector machine (SVM) has been the dominant model used in MIR systems. Like SVM, GP models are based on kernel functions and Gram matrices; but, in contrast, they produce truly probabilistic outputs with an explicit degree of prediction uncertainty. In addition, there exist algorithms for GP hyperparameter learning-something the SVM framework lacks. In this paper, we built two systems, one for music genre classification and another for music emotion estimation using both SVM and GP models, and compared their performances on two databases of similar size. In all cases, the music audio signal was processed in the same way, and the effects of different feature extraction methods and their various combinations were also investigated. The evaluation experiments clearly showed that in both music genre classification and music emotion estimation tasks the GP performed consistently better than the SVM. The GP achieved a 13.6% relative genre classification error reduction and up to an 11% absolute increase of the coefficient of determination in the emotion estimation task."
  },
  {
    "year": "2014",
    "abstract": "Low-complexity suboptimal multiuser detectors (MUDs) are widely used in multiple access communication systems for separating users, since the computational complexity of the maximum likelihood (ML) detector is potentially excessive for practical implementation. Quantum computing may be invoked in the detection procedure, by exploiting its inherent parallelism for approaching the ML MUDs performance at a substantially reduced number of cost function evaluations. In this contribution, we propose a soft-output (SO) quantum-assisted MUD achieving a near-ML performance and compare it to the corresponding SO ant colony optimization MUD. We investigate rank deficient direct-sequence spreading (DSS) and slow subcarrier-hopping aided (SSCH) spatial division multiple access orthogonal frequency division multiplexing systems, where the number of users to be detected is higher than the number of receive antenna elements used. We show that for a given complexity budget, the proposed SO-Dürr-Høyer algorithm (DHA) QMUD achieves a better performance. We also propose an adaptive hybrid SO-ML/SO-DHA MUD, which adapts itself to the number of users equipped with the same spreading sequence and transmitting on the same subcarrier. Finally, we propose a DSS-based uniform SSCH scheme, which improves the system's performance by 0.5 dB at a BER of 10−5, despite reducing the complexity required by the MUDs employed."
  },
  {
    "year": "2014",
    "abstract": "A better understanding of human cognitive ability-demand gap (ADG) is critical in designing assistive technology solution that is accurate and adaptive over a wide range of human-agent interaction. The main goal is to design systems that can adapt with the user’s abilities and needs over a range of cognitive tasks. It will also enable the system to provide feedback consistent with the situation. However, the latent structure and relationship between human ability to respond to cognitive task (demand on human by the agent) remains unknown. Robust modeling of cognitive ADG will be a paradigm shift from the current trends in assistive technology design. The key idea is to estimate the gap, based on human-agent cognitive task interaction. In particular, latent response model was adopted to quantify the gap. First, we used one parameter Rasch model and extended Rasch model (rating scale model, partial credit model) with dichotomous and polytomous responses, respectively. Residues between expected and observed ability scores were considered as gap parameter in case of dichotomous response. In extended Rasch modeling, response latitudes are considered as an indicator of the gap. Additionally, we performed model fitting, standard error measurement, kernel density estimation, and differential item functioning to test the suitability of Rasch model. Empirical analyses on a number of data set show that proposed analytical method can model the cognitive ADG from dichotomous and polytomous responses. In dichotomous case, the model better fits for mixed responses (combination of easy, medium, and hard) data set rather than monotonic (e.g., only easy) data. Results show that Rasch model can be reliably used to estimate cognitive gap with different cognitive task types."
  },
  {
    "year": "2014",
    "abstract": "The introduction of new mobile communication standards, enabling the ever growing amount of data transmitted in mobile communication networks, continuously increases the complexity of control processing within radio frequency (RF) transceivers. Since this complexity cannot be handled by traditional approaches, this paper focuses on the partitioning of RF transceiver systems and on the implementation of application-specific components to introduce an advanced multiprocessor system-on-chip interface and control architecture which is able to fulfill the requirements of future RF transceiver integrations. The proposed framework demonstrates a high degree of scalability, flexibility, and reusability. Consequently, the time to market for products can be reduced and fast adaptations to the requirements of the market are feasible. In addition, the developed application-specific components achieve improved or at least equivalent performance results compared with common architectures while the silicon area can be reduced. This characteristic has positive effects on the costs as well as on the power consumption of the RF transceiver."
  },
  {
    "year": "2014",
    "abstract": "Domain Name System (DNS) injection is a censorship method for blocking access to blacklisted domain names. The method uses deep packet inspection on all DNS queries passing through the network and injects spoofed responses. Compared with other blocking mechanisms, DNS injection impacts uninvolved third-parties if their traffic is routed through a censored network. In this paper, we look for large deployments of DNS injection, measured from vantage points outside of the censored networks. DNS injection is known to be used in China since it leaked unintentionally into foreign networks. We find that DNS injection is also used in Iran and can be observed by sending DNS queries to Iranian networks. In mid 2013, the Iranian DNS filter was temporarily suspended for some names, which correlated with media coverage of political debates in Iran about blocking social media. Spoofed responses from China and Iran can be detected passively by the IP address returned. We propose an algorithm to obtain these addresses remotely. After testing 255002 open resolvers outside of China, we determined that 6% are potentially affected by Chinese DNS injection when querying top-level domains outside of China. This is essentially the result of one top-level domain name server for which an anycast instance is hosted in China."
  },
  {
    "year": "2014",
    "abstract": "This paper concerns the soft-in soft-out detection in a coded communication system, where the transmitted symbols are discrete valued, and the exact a posteriori probability (APP) detection often involves prohibitive complexity. By using the properties of Gaussian functions, an approximate approach to the APP detection is devised with the idea that, in the computation of the APP of each symbol, the remaining symbols are distinguished based on their contributions to the APP of the concerned symbol, and the symbols with less contributions are approximated as (continuous) Gaussian variables [hence the name partial Gaussian approximation (PGA)] to reduce the computational complexity. The connection between the PGA detector and the reduced dimension maximum a posteriori detector (RDMAP) is investigated. It is shown that, PGA is equivalent to RDMAP, but it has a complexity much lower than that of RDMAP, i.e., PGA can be regarded as an efficient implementation of RDMAP. In addition, the application of PGA in intersymbol interference (ISI) channel equalization is also investigated. We show that PGA allows further significant complexity reduction by exploiting the circulant structure of the system transfer matrix, which makes PGA very attractive in handling severe ISI channels with large memory length."
  },
  {
    "year": "2014",
    "abstract": "With the increase in age, there are changes in skeletal structure, muscle mass, and body fat. For recognizing faces with age variations, researchers have generally focused on the skeletal structure and muscle mass. However, the effect of change in body fat has not been studied with respect to face recognition. In this paper, we incorporate weight information to improve the performance of face recognition with age variations. The proposed algorithm utilizes neural network and random decision forest to encode age variations across different weight categories. The results are reported on the WhoIsIt database prepared by the authors containing 1109 images from 110 individuals with age and weight variations. The comparison with existing state-of-the-art algorithms and commercial system on WhoIsIt and FG-Net databases shows that the proposed algorithm outperforms existing algorithms significantly."
  },
  {
    "year": "2014",
    "abstract": "In recent years, multilayer photonic bandgap structures comprising stacks of alternating layers of positive and negative index have been proposed for a variety of applications, such as perfect imaging, filters, sensors, coatings for tailored emittance, absorptance, etc. Following a brief review of the history of negative index materials, the performance of such stacks is reviewed, with emphasis on analysis of plane wave and beam propagation, and possible applications in sensing. First, the use of the transfer matrix method to analyze plane wave propagation in such structures to determine the transmittance and reflectance is developed. Examples of cases where the Bragg bandgap and the so-called zero <n> gap can be used for possible applications in sensing are illustrated. Next, the transfer matrix approach is extended to simulate the spatial evolution of a collection of propagating and nonpropagating TE and TM plane waves (or plane wave spectra) incident on such multilayer structures. The use of the complex Poynting theorem in checking the computations, as well as monitoring powers and the stored electric or magnetic energy in any section of the multilayer stack, is illustrated, along with its use in designing alternating positive and negative index structures with optimal gain to compensate for losses in the negative index material. Finally, the robustness of PIM-NIM stacks with respect to randomness in the dimensions of the PIM-NIM structure is examined. This should be useful in determining the performance of such structures when they are physically fabricated."
  },
  {
    "year": "2014",
    "abstract": "Provides a listing of current committee members and society officers."
  },
  {
    "year": "2014",
    "abstract": "In new product development, time to market (TTM) is critical for the success and profitability of next generation products. When these products include sophisticated electronics encased in 3D packaging with complex geometries and intricate detail, TTM can be compromised—resulting in lost opportunity. The use of advanced 3D printing technology enhanced with component placement and electrical interconnect deposition can provide electronic prototypes that now can be rapidly fabricated in comparable time frames as traditional 2D bread-boarded prototypes; however, these 3D prototypes include the advantage of being embedded within more appropriate shapes in order to authentically prototype products earlier in the development cycle. The fabrication freedom offered by 3D printing techniques, such as stereolithography and fused deposition modeling have recently been explored in the context of 3D electronics integration—referred to as 3D structural electronics or 3D printed electronics. Enhanced 3D printing may eventually be employed to manufacture end-use parts and thus offer unit-level customization with local manufacturing; however, until the materials and dimensional accuracies improve (an eventuality), 3D printing technologies can be employed to reduce development times by providing advanced geometrically appropriate electronic prototypes. This paper describes the development process used to design a novelty six-sided gaming die. The die includes a microprocessor and accelerometer, which together detect motion and upon halting, identify the top surface through gravity and illuminate light-emitting diodes for a striking effect. By applying 3D printing of structural electronics to expedite prototyping, the development cycle was reduced from weeks to hours."
  },
  {
    "year": "2014",
    "abstract": "Spinning plasma toroids, or spinning spheromaks, are reported as forming in partial atmosphere during high-power electric arc experiments. They are a new class of spheromaks because they are observed to be stable in partial atmosphere with no confining external toroidal magnetic fields, and are observed to endure for more than 600 ms. Included in this paper is a model that explains these stable plasma toroids (spheromaks); they are hollow plasma toroids with a thin outer shell of electrons and ions that all travel in parallel paths orthogonal to the toroid circumference—in effect, spiraling around the toroid. These toroids include sufficient ions to neutralize the space charge of the electrons. This model leads to the name Electron Spiral Toroid Spheromak (ESTS). The discovery of this new class of spheromaks resulted from work to explain ball lightning. A comparison is made between the experimental observations of spheromaks in partial atmosphere and reported ball lightning observations; strong similarities are reported. The ESTS is also found to have a high ion density of>1019ions/cm3without needing any external toroidal magnetic field for containment, compared, for example, to tokamaks, with ion density limits of∼1015ions/cm3. This high ion density is a defining characteristic and opens the potential to be useful in applications. The ESTS is a field reversed configuration plasma toroid."
  },
  {
    "year": "2014",
    "abstract": "The existing worst case response-time analysis for controller area network (CAN) with nodes implementing priority and First In First Out (FIFO) queues does not support mixed messages. It assumes that a message is queued for transmission either periodically or sporadically. However, a message can also be queued both periodically and sporadically using mixed transmission mode implemented by several higher level protocols for CAN that are used in the automotive industry. We extend the existing analysis for CAN to support any higher level protocol for CAN that uses periodic, sporadic, and mixed transmission of messages in the systems where some nodes implement priority queues, whereas others implement FIFO queues. In order to provide a proof of concept, we implement the extended analysis in a free tool, conduct an automotive-application case study, and perform comparative evaluation of the extended analysis with the existing analysis."
  },
  {
    "year": "2014",
    "abstract": "Over time, the traditional single-objective job shop scheduling method has grown increasingly incapable of meeting the requirements of contemporary business models; thus, a multiobjective scheduling solution is required. Because of changing orders, understanding the schedule and output is a constant challenge when using a traditional manual schedule, particularly among manufacturers that produce various products. The multiobjective optimization genetic algorithm (MOGA) is a relatively superior method of solving multiobjective optimization problems; therefore, we used a MOGA to solve flexible job-shop problems for a middle-scale screw manufacturer in Taiwan. For solving the problems of incorrect jobs assign and diversity problem of traditional genetic algorithm (GA) caused by encoding method when applying traditional GA in the flexible manufacturing environment, a refined GA was proposed. Two-phase test has performed for proposed approach, using a classical benchmark of distributed and flexible jobs-shop scheduling problem, and 80 set of work orders, the empirical results indicated that the proposed model yielded substantial savings, regardless of the total order completion time, machine retooling rate, and average machine load rate."
  },
  {
    "year": "2014",
    "abstract": "In this paper, we study control problems that can be directly applied to controlling the rotational motion of eye and head. We model eye and head as a sphere, or ellipsoid, rotating about its center, or about its south pole, where the axes of rotation are physiologically constrained, as was proposed originally by Listing and Donders. The Donders' constraint is either derived from Fick gimbals or from observed rotation data of adult human head. The movement dynamics is derived onSO(3)or on a suitable submanifold ofSO(3)after describing a Lagrangian. Using two forms of parametrization, the axis-angle and Tait– Bryan, the motion dynamics is described as an Euler– Lagrange's equation, which is written together with an externally applied control torque. Using the control system, so obtained, we propose a class of optimal control problem that minimizes the norm of the applied external torque vector. Our control objective is to point the eye or head, toward a stationary point target, also called the regulation problem. The optimal control problem has also been analyzed by writing the dynamical system as a Newton– Euler's equation using angular velocity as part of the state variables. In this approach, explicit parametrization ofSO(3)is not required. Finally, in the appendix, we describe a recently introduced potential control problem to address the regulation problem."
  },
  {
    "year": "2014",
    "abstract": "GooFit is a thread-parallel, GPU-friendly function evaluation library, nominally designed for use with the maximum likelihood fitting program MINUIT. In this use case, it provides highly parallel calculations of normalization intergrals and log (likelihood) sums. A key feature of the design is its use of the Thrust library to manage all parallel kernel launches. This allows GooFit to execute on any architecture for which Thrust has a backend, currently, including CUDA for nVidia GPUs and OpenMP for single- and multicore CPUs. Running on an nVidia C2050, GooFit executes 300 times more quickly for a complex high energy physics problem than does the prior (algorithmically equivalent) code running on a single CPU core. The design and implementation choices, discussed in detail, can help to guide developers of other highly parallel, compute-intensive libraries."
  },
  {
    "year": "2014",
    "abstract": "Agent-based modeling has become a viable alternative and complement-to-traditional analysis methods for studying complex social environments. In this paper, we survey the role of agent-based modeling within hospital settings, where agent-based models investigate patient flow and other operational issues as well as the dynamics of infection spread within hospitals or hospital units. While there is a rich history of simulation and modeling of hospitals and hospital units, relatively little work exists, which applies agent-based models to this context."
  },
  {
    "year": "2014",
    "abstract": "We investigate a quantum coding for quantum communication over a partially degradable (PD) quantum channel. For a PD channel, the degraded environment state can be expressed from the channel output state up to a degrading map. PD channels can be restricted to the set of optical channels, which allows for the parties to exploit the benefits in experimental quantum communications. We show that for a PD channel, the partial degradability property leads to higher quantum data rates in comparison with those of a degradable channel. The PD property is particularly convenient for quantum communications and allows one to implement the experimental quantum protocols with higher performance. We define a coding scheme for PD channels and give the achievable rates of quantum communication."
  },
  {
    "year": "2014",
    "abstract": "The dynamic nature of vehicular networks with their fast changing topology poses several challenges to setup communication between vehicles. Packet collisions are considered to be the main source of data loss in contention-based vehicular networks. Retransmission of collided packets is done several times until an acknowledgment of successful reception is received or the maximum number of retries is reached. The retransmission delay is drawn randomly from an interval, called the backoff interval. A good choice of the backoff interval reduces the number of collisions and the waiting periods of data packets, which increases the throughput and decreases the energy consumption. An optimal backoff interval could be obtained if global network information spread in the network in a short time. However, this is practically not achievable which motivates the efficient utilization of local information to approach the optimal performance. In this paper, we propose a localized adaptive strategy that calculates the backoff interval for unicast applications in vehicular networks. The new strategy uses fuzzy logic to adapt the backoff interval to the fast changing vehicular environment using only local information. We present four schemes of that strategy that differ in their behavior and the number of inputs. We compare the proposed schemes with other known schemes, binary exponential backoff, backoff algorithm, and an optimal scheme, in terms of throughput, fairness, and energy consumption. Results show that by proper tuning of the fuzzy parameters and rules, one of the proposed schemes outperform the other schemes, and approach the optimal results."
  },
  {
    "year": "2014",
    "abstract": "In this paper, we propose an application specific instrument (ASIN)-based ultrawideband (UWB) radar system for sludge monitoring from scattering signatures from the bottom of industrial oil tanks. The method is validated by successful estimation of sludge volume in oil tanks using simulated and real data. First, as a demonstration of the conventional system, image reconstruction algorithms are used for tank-bottom sludge profile imaging for symmetrical and asymmetrical sludge profiles, where the setup is modeled in finite difference time domain method with reduced dimensions of the tank. A 3-D imaging algorithm is used for the 3-D simulation of real life targets. To get the volume of the sludge, ASIN-based UWB radar system is then applied and its effectiveness is demonstrated. In this framework, to get information about the sludge at the bottom of industrial tank, first, a scheme is proposed to differentiate between two sets of data which correspond to two different set of volumes. This method is validated using a commercial UWB kit, in which, practical experiments were performed. The data obtained is visualized using multidimensional scaling procedure and analyzed. Then, regression analysis using radial basis function artificial neuron network is performed, so that given a particular data, it can be predicted that, to which volume it best corresponds."
  },
  {
    "year": "2014",
    "abstract": "The Editor-in-Chief would like to acknowledge the work of the following Associate Editors for IEEE Access."
  },
  {
    "year": "2014",
    "abstract": "This paper presents a systematic methodology to develop compact MOSFET models for process variability-aware VLSI circuit design. Process variability in scaled CMOS technologies severely impacts the functionality, yield, and reliability of advanced integrated circuit devices, circuits, and systems. Therefore, variability-aware circuit design techniques are required for realistic assessment of the impact of random and systematic process variability in advanced VLSI circuit performance. However, variability-aware circuit design requires compact MOSFET variability models for computer analysis of the impact of process variability in VLSI circuit design. This paper describes a generalized methodology to determine the major set of device parameters sensitive to random and systematic process variability in nanoscale MOSFET devices, map each variability-sensitive device parameter to the corresponding compact model parameter of the target compact model, and generate statistical compact MOSFET models for variability-aware VLSI circuit design."
  },
  {
    "year": "2014",
    "abstract": "Deep belief nets (DBNs) with restricted Boltzmann machines (RBMs) as the building block have recently attracted wide attention due to their great performance in various applications. The learning of a DBN starts with pretraining a series of the RBMs followed by fine-tuning the whole net using backpropagation. Generally, the sequential implementation of both RBMs and backpropagation algorithm takes significant amount of computational time to process massive data sets. The emerging big data learning requires distributed computing for the DBNs. In this paper, we present a distributed learning paradigm for the RBMs and the backpropagation algorithm using MapReduce, a popular parallel programming model. Thus, the DBNs can be trained in a distributed way by stacking a series of distributed RBMs for pretraining and a distributed backpropagation for fine-tuning. Through validation on the benchmark data sets of various practical problems, the experimental results demonstrate that the distributed RBMs and DBNs are amenable to large-scale data with a good performance in terms of accuracy and efficiency."
  },
  {
    "year": "2014",
    "abstract": "Recent years have seen increasing popularity of storing and managing personal multimedia data using online services. Preserving confidentiality of online personal data while offering efficient functionalities thus becomes an important and pressing research issue. In this paper, we study the problem of content-based search of image data archived online while preserving content confidentiality. The problem has different settings from those typically considered in the secure computation literature, as it deals with data in rank-ordered search, and has a different security-efficiency requirement. Secure computation techniques, such as homomorphic encryption, can potentially be used in this application, at a cost of high computational and communication complexity. Alternatively, efficient techniques based on randomizing visual feature and search indexes have been proposed recently to enable similarity comparison between encrypted images. This paper focuses on comparing these two major paradigms of techniques, namely, homomorphic encryption-based techniques and feature/index randomization-based techniques, for confidentiality-preserving image search. We develop novel and systematic metrics to quantitatively evaluate security strength in this unique type of data and applications. We compare these two paradigms of techniques in terms of their search performance, security strength, and computational efficiency. The insights obtained through this paper and comparison will help design practical algorithms appropriate for privacy-aware cloud multimedia systems."
  },
  {
    "year": "2014",
    "abstract": "What is it inside the colorfully wrapped present? You pick it up to ear level and listen, shake it, and then listen again. To you, the basic principle of active sensing is quite clear—first absorb, then, if there is no movement or sound, shake it, and then reabsorb. We propose an extremely basic hypothesis for the active sensing of haptic interaction with dynamical systems. Our hypothesis asserts that in order to improve the efficiency of extracting information from a probed system, the sensor should act according to the following basic principle: if the probed system is passive, the sensor should be active; conversely, when the probed system is active, the sensor should be passive. We proved the proposed principle for interaction with a second-order mechanical system with the goal to enhance classification performance between two possible sine power sources. We showed that the addition of an active power source to a passive testing sensor leads to decreased sensitivity in the amplitude and frequency of the tested power source. Further, an extension of this principle is provided, presenting the conditions for reduced sensitivity to spring and damper parameters. To test its applicability for a linear system in a noisy environment, a computer simulation was performed demonstrating that classification performance improved by following the proposed principle. At last, ten subjects probed an active virtual system under either active or passive conditions. A comparison of the mean just-noticeable difference of both conditions indicated significantly better sensitivity was obtained by following the principle."
  },
  {
    "year": "2014",
    "abstract": "For three decades, sudden acceleration (SA) incidents have been reported, where automobiles accelerate without warning. These incidents are often diagnosed as no fault found. Investigators, who follow the line of diagnostic reasoning from the 1989 National Highway Traffic Safety Administration (NHTSA) SA report, tend to conclude that SAs are caused by driver pedal error. This paper reviews the diagnostic process in the NHTSA report and finds that: 1) it assumes that an intermittent electronic malfunction should be reproducible either through in-vehicle or laboratory bench tests without saying why and 2) the consequence of this assumption, for which there appears to be no forensic precedent, is to recategorize possible intermittent electronic failures as proven to be nonelectronic. Showing that the supposedly inescapable conclusions of the NHTSA report concerning electronic malfunctions are without foundation opens the way for this paper to discuss electronic intermittency as a potential factor in SA incidents. It then reports a simple practical experiment that shows how mechanically induced electrical contact intermittencies can generate false speed signals that an automobile speed control system may accept as true and that do not trigger any diagnostic fault codes. Since the generation of accurate speed signals is essential for the proper functioning of a number of other automobile safety-critical control systems, the apparent ease with which false speed signals can be generated by vibration of a poor electrical contact is obviously a matter of general concern. Various ways of reducing the likelihood of SAs are discussed, including electrical contact improvements to reduce the likelihood of generating false speed signals, improved battery maintenance, and the incorporation of an independent fail-safe that reduces engine power in an emergency, such as a kill switch."
  },
  {
    "year": "2014",
    "abstract": "The results of successful hacking attacks against commercially available cybersecurity protection tools that had been touted as secure are distilled into a set of concepts that are applicable to many protection planning scenarios. The concepts, which explain why trust in those systems was misplaced, provides a framework for both analyzing known exploits and also evaluating proposed protection systems for predicting likely potential vulnerabilities. The concepts are: 1) differentiating security threats into distinct classes; 2) a five layer model of computing systems; 3) a payload versus protection paradigm; and 4) the nine Ds of cybersecurity, which present practical defensive tactics in an easily remembered scheme. An eavesdropping risk, inherent in many smartphones and notebook computers, is described to motivate improved practices and demonstrate real-world application of the concepts to predicting new vulnerabilities. Additionally, the use of the nine Ds is demonstrated as analysis tool that permits ranking of the expected effectiveness of some potential countermeasures."
  },
  {
    "year": "2014",
    "abstract": "Standardized turbo codes (TCs) use recursive systematic convolutional transducers of rateb/(b+d), having a single feedback polynomial(b+dRSCT). In this paper, we investigate the realizability of theb+dRSCTset through two single shift register canonical forms (SSRCFs), called, in the theory of linear systems, constructibility, and controllability. The two investigated SSRCF are the adaptations, for the implementation ofb+dRSCT, of the better-known canonical forms controller (constructibility) and observer (controllability). Constructibility is the implementation form actually used for convolutional transducers in TCs. This paper shows that anyb+1RSCTcan be implemented in a unique SSRCF observer. As a result, we build a function,ξ:H→G, which has as definition domain the set of encoders in SSRCF constructibility, denoted byH, and as codomain a subset of encoders in SSRCF observer, denoted byG. By proving the noninjectivity and nonsurjectivity properties of the functionξ, we prove thatHis redundant and incomplete in comparison withG, i.e., the SSRCF observer is more efficient than the SSRCF constructibility for the implementation ofb+1RSCT. We show that the redundancy of the setHis dependent on the memorymand on the number of inputsbof the consideredb+1RSCT. In addition, the difference betweenGandξ(H)contains encoders with very good performance, when used in a TC structure. This difference is consistent form≈b>1. The results on the realizability of theb+1RSCTallowed us some considerations onb+dRSCT, withb,d>1, as well, for which we proposed the SSRCF controllability. These results could be useful in the design of TC based on exhaustive search. So, the proposed implementation form permits the design of new..."
  },
  {
    "year": "2014",
    "abstract": "The large variety of network traffic sets many challenges in modeling the essential aspects of network traffic flows. Analyzing and collecting features for the model creation process from the network traffic traces is a time-consuming and error-prone task. Automating these procedures are a challenge. The research problem discussed in this paper concentrates on the analysis and collection of features from the network traffic traces for the model development process, by automating the analysis and collection. The proposed system of this paper, called MGtoolV2, supports the model development process through the automation of collection and analysis in the actual model creation procedures. The model development process aims to enhance the development of a model by reducing the development cost and time. The proposed tool automatically creates large sets of models according to the network traffic traces and minimizes the errors of manual modeling. The experiments conducted with MGtoolV2 indicate that the tool is able to create the models from the traffic traces cost effectively. MGtoolV2 is able to unify similarities between packets, to create very detailed models describing specific information, and to raise the abstraction level of the created models. The research is based on the constructive method of the related publications and technologies, and the results are established from the testing, validation, and analysis of the implemented MGtoolV2."
  },
  {
    "year": "2014",
    "abstract": "Provides a listing of current staff, committee members and society officers."
  },
  {
    "year": "2014",
    "abstract": "Software development is facing new challenges as a result of evolution toward integration and collaboration-based service engineering, which embody high degrees of dynamism both at design time and run-time. Short times-to-market require cost reduction by maximizing software reuse. Openness for new innovations presumes a flexible development platform and fast software engineering practices. User satisfaction assumes situation-based applications of high quality. The main contribution of this paper is the piecemeal service engineering (PSE) approach developed for and tested in application development for smart spaces. The intent of PSE is to maximize the reuse of existing knowledge of business and design practices and existing technical assets in the development of new smart-space applications. Business knowledge is mostly informal and domain-dependent, but architectural knowledge is generic, at least semiformal, and represented in principles, ontologies, patterns, and rules that together form a reusable architectural knowledge base for fast smart-space application development. The PSE facilitates the incremental development of intelligent applications by supporting abstraction, aggregation, and adaptability in smart-space development."
  },
  {
    "year": "2014",
    "abstract": "This paper is about how the SP theory of intelligence and its realization in the SP machine may, with advantage, be applied to the management and analysis of big data. The SP system—introduced in this paper and fully described elsewhere—may help to overcome the problem of variety in big data; it has potential as a universal framework for the representation and processing of diverse kinds of knowledge, helping to reduce the diversity of formalisms and formats for knowledge, and the different ways in which they are processed. It has strengths in the unsupervised learning or discovery of structure in data, in pattern recognition, in the parsing and production of natural language, in several kinds of reasoning, and more. It lends itself to the analysis of streaming data, helping to overcome the problem of velocity in big data. Central in the workings of the system is lossless compression of information: making big data smaller and reducing problems of storage and management. There is potential for substantial economies in the transmission of data, for big cuts in the use of energy in computing, for faster processing, and for smaller and lighter computers. The system provides a handle on the problem of veracity in big data, with potential to assist in the management of errors and uncertainties in data. It lends itself to the visualization of knowledge structures and inferential processes. A high-parallel, open-source version of the SP machine would provide a means for researchers everywhere to explore what can be done with the system and to create new versions of it."
  },
  {
    "year": "2014",
    "abstract": "The quantum capacity of degradable quantum channels has been proven to be additive. On the other hand, there is no general rule for the behavior of quantum capacity for antidegradable quantum channels. We introduce the set of partially degradable (PD) quantum channels to answer the question of additivity of quantum capacity for a well-separable subset of antidegradable channels. A quantum channel is PD if the channel output can be used to simulate the degraded environment state. The PD channels could exist both in the degradable, antidegradable and conjugate degradable family. We define the term partial simulation, which is a clear benefit that arises from the structure of the complementary channel of a PD channel. We prove that the quantum capacity of an arbitrary dimensional PD channel is additive. We also demonstrate that better quantum data rates can be achieved over a PD channel in comparison with standard (non-PD) channels. Our results indicate that the partial degradability property can be exploited and yet still hold many benefits for quantum communications."
  },
  {
    "year": "2014",
    "abstract": "Silane crosslinked polyethylene cable insulation occasionally fails to meet the aging requirements given in technical standards. The purpose of this paper is to investigate this phenomena and establish whether the safety margin of aging tests can be increased by changes in manufacture or test procedures. Using a number of cable types with different compositions and dimensions, the evolution of the absolute values of tensile strength and elongation at break upon aging was obtained. The results show that the major changes in mechanical properties happen within the first 24-48 h. This finding is valid both for ethylene vinylsilane copolymers and for grafted silane systems. In general, the effect is more pronounced for the 100°C compatibility test. Statistical analysis shows that insulation crosslinked in a hot waterbath will exhibit this behavior to a lesser extent, thus increasing the safety margin in aging tests, compared with ambient curing. This paper demonstrates that preconditioning at 70°C has no significant impact on aging properties. In addition, only small variations in mechanical properties were seen when changing the process parameters. It is concluded that further crosslinking is the principal cause of the phenomena under investigation."
  },
  {
    "year": "2014",
    "abstract": "The use of computer-based and online education systems has made new data available that can describe the temporal and process-level progression of learning. To date, machine learning research has not considered the impacts of these properties on the machine learning prediction task in educational settings. Machine learning algorithms may have applications in supporting targeted intervention approaches. The goals of this paper are to: 1) determine the impact of process-level information on machine learning prediction results and 2) establish the effect of type of machine learning algorithm used on prediction results. Data were collected from a university level course in human factors engineering (n=35), which included both traditional classroom assessment and computer-based assessment methods. A set of common regression and classification algorithms were applied to the data to predict final course score. The overall prediction accuracy as well as the chronological progression of prediction accuracy was analyzed for each algorithm. Simple machine learning algorithms (linear regression, logistic regression) had comparable performance with more complex methods (support vector machines, artificial neural networks). Process-level information was not useful in post-hoc predictions, but contributed significantly to allowing for accurate predictions to be made earlier in the course. Process level information provides useful prediction features for development of targeted intervention techniques, as it allows more accurate predictions to be made earlier in the course. For small course data sets, the prediction accuracy and simplicity of linear regression and logistic regression make these methods preferable to more complex algorithms."
  },
  {
    "year": "2014",
    "abstract": "Generating synthetic data traffic, which statistically resembles its recorded counterpart is one of the main goals of network traffic modeling. Equivalently, one or several random processes shall be created, exhibiting multiple prescribed statistical measures. In this paper, we present a framework enabling the joint representation of distributions, autocorrelations and cross-correlations of multiple processes. This is achieved by so called transformed Gaussian autoregressive moving-average models. They constitute an analytically tractable framework, which allows for the separation of the fitting problems into subproblems for individual measures. Accordingly, known fitting techniques and algorithms can be deployed for the respective solution. The proposed framework exhibits promising properties: 1) relevant statistical properties such as heavy tails and long-range dependences are manageable; 2) the resulting models are parsimonious; 3) the fitting procedure is fully automatic; and 4) the complexity of generating synthetic traffic is very low. We evaluate the framework with traced traffic, i.e., aggregated traffic, online gaming, and video streaming. The queueing responses of synthetic and recorded traffic exhibit identical statistics. This paper provides guidance for high-quality modeling of network traffic. It proposes a unifying framework, validates several fitting algorithms, and suggests combinations of algorithms suited best for specific traffic types."
  },
  {
    "year": "2014",
    "abstract": "We have developed a personal mobility vehicle (PMV) with four driven wheels that is capable of negotiating obstacles with a leg motion mechanism. When obstacles are encountered, wheels are lifted, moved ahead in a stepping-like motion, and lowered back down, thereby allowing the PMV to advance further. In our previous paper, we discussed the principle of the gait algorithm used by our PMV, in which wheels are utilized as legs to negotiate obstacles. In the original algorithm, when the wheels encountered terrain that might require leg motion to traverse, the system determined whether such motion was applicable and, if it was, orchestrated a series of leg motions. However, there were terrains that could not be negotiated using the original algorithm. In this paper, we propose an improved gait algorithm, in which when the vehicle encounters terrain intractable by leg motion with its current posture, the vehicle changes its posture until it can traverse that terrain. We verified the effectiveness of the improved gait algorithm through a variety of mobility tests with a passenger. In addition, we present numerical data on the range of terrain topologies that could be negotiated by the proposed algorithm."
  },
  {
    "year": "2014",
    "abstract": "Emerging opportunities for open data based business have been recognized around the world. Open data can provide new business opportunities for actors that provide data, for actors that consume data, and for actors that develop innovative services and applications around the data. Open data based business requires business models and a collaborative environment—called an ecosystem—to support businesses based on open data, services, and applications. This paper outlines the open data ecosystem (ODE) from the business viewpoint and then defines the requirements of such an ecosystem. The outline and requirements are based on the state-of-the-art knowledge explored from the literature and the state of the practice on data-based business in the industry collected through interviews. The interviews revealed several motives and advantages of the ODE. However, there are also obstacles that should be carefully considered and solved. This paper defines the actors of the ODE and their roles in the ecosystem as well as the business model elements and services that are needed in open data based business. According to the interviews, the interest in open data and open data ecosystems is high at this moment. However, further research work is required to establish and validate the ODE in the near future."
  },
  {
    "year": "2014",
    "abstract": "Motion planning is a fundamental research area in robotics. Sampling-based methods offer an efficient solution for what is otherwise a rather challenging dilemma of path planning. Consequently, these methods have been extended further away from basic robot planning into further difficult scenarios and diverse applications. A comprehensive survey of the growing body of work in sampling-based planning is given here. Simulations are executed to evaluate some of the proposed planners and highlight some of the implementation details that are often left unspecified. An emphasis is placed on contemporary research directions in this field. We address planners that tackle current issues in robotics. For instance, real-life kinodynamic planning, optimal planning, replanning in dynamic environments, and planning under uncertainty are discussed. The aim of this paper is to survey the state of the art in motion planning and to assess selected planners, examine implementation details and above all shed a light on the current challenges in motion planning and the promising approaches that will potentially overcome those problems."
  }
]