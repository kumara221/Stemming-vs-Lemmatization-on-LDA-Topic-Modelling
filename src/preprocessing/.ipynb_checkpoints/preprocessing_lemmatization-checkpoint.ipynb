{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff61e472",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b81d114",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_path = './../../data/raw/abstracts_only.json'\n",
    "clean_path = './../../data/cleaned/cleaned_abstracts_lemmatization.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba3eac06",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(raw_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f52f9728",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "\n",
    "if 'abstract' not in df.columns:\n",
    "    raise KeyError(\"'abstract' column not found in JSON.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2da6c0ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Technical progress in the open-source self rep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abstractive dialogue summarization is a challe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The electromagnetic (EM) properties of two-com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>To fulfil the tight area and memory constraint...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Enterprises exist in a competitive manufacturi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            abstract\n",
       "0  Technical progress in the open-source self rep...\n",
       "1  Abstractive dialogue summarization is a challe...\n",
       "2  The electromagnetic (EM) properties of two-com...\n",
       "3  To fulfil the tight area and memory constraint...\n",
       "4  Enterprises exist in a competitive manufacturi..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2669a8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1834b08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9b2b249",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    doc = nlp(text.lower())\n",
    "    tokens = [\n",
    "        token.lemma_ for token in doc\n",
    "        if token.is_alpha and token.lemma_ not in stop_words\n",
    "    ]\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eeec4998",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_abstract'] = df['abstract'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11da8d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Before and After Preprocessing ---\n",
      "\n",
      "Data 1:\n",
      "Raw       : Technical progress in the open-source self replicating rapid prototyper (RepRap) community has enabled a distributed form of additive manufacturing to expand rapidly using polymer-based materials. However, the lack of an open-source metal alternative and the high capital costs and slow throughput of proprietary commercialized metal 3-D printers has severely restricted their deployment. The applications of commercialized metal 3-D printers are limited to only rapid prototyping and expensive finished products. This severely restricts the access of the technology for small and medium enterprises, the developing world and for use in laboratories. This paper reports on the development of a<$2000open-source metal 3-D printer. The metal 3-D printer is controlled with an open-source micro-controller and is a combination of a low-cost commercial gas-metal arc welder and a derivative of the Rostock, a deltabot RepRap. The bill of materials, electrical and mechanical design schematics, and basic construction and operating procedures are provided. A preliminary technical analysis of the properties of the 3-D printer and the resultant steel products are performed. The results of printing customized functional metal parts are discussed and conclusions are drawn about the potential for the technology and the future work necessary for the mass distribution of this technology.\n",
      "Cleaned   : technical progress open source self replicate rapid prototyper reprap community enable distribute form additive manufacturing expand rapidly use polymer base material however lack open source metal alternative high capital cost slow throughput proprietary commercialize metal printer severely restrict deployment application commercialize metal printer limit rapid prototyping expensive finished product severely restrict access technology small medium enterprise develop world use laboratory paper report development source metal printer metal printer control open source micro controller combination low cost commercial gas metal arc welder derivative rostock deltabot reprap bill material electrical mechanical design schematic basic construction operating procedure provide preliminary technical analysis property printer resultant steel product perform result print customize functional metal part discuss conclusion draw potential technology future work necessary mass distribution technology\n",
      "\n",
      "Data 2:\n",
      "Raw       : Abstractive dialogue summarization is a challenging task for several reasons. First, most of the key information in a conversation is scattered across utterances through multi-party interactions with different textual styles. Second, dialogues are often informal structures, wherein different individuals express personal perspectives, unlike text summarization, tasks that usually target formal documents such as news articles. To address these issues, we focused on the association between utterances from individual speakers and unique syntactic structures. Speakers have unique textual styles that can contain linguistic information, such as voiceprint. To do this, we used ad-hoc analysis to explore speakersâ€™ text styles and constructed a syntax-aware model by leveraging linguistic information (i.e., POS tagging), which alleviates the above issues by inherently distinguishing utterances from individual speakers. Our approach allows for both data and model-centric investigation. Also, we employed multi-task learning of both syntax-aware information and dialogue summarization. To the best of our knowledge, our approach is the first method to apply multi-task learning to the dialogue summarization task. Experiments on a SAMSum corpus (a large-scale dialogue summarization corpus) demonstrated that our method improved upon the vanilla model. Consequently, we found that our efforts of syntax-aware approach have been reflected by the model.\n",
      "Cleaned   : abstractive dialogue summarization challenging task several reason first key information conversation scatter across utterance multi party interaction different textual style second dialogue often informal structure wherein different individual express personal perspective unlike text summarization task usually target formal document news article address issue focus association utterance individual speaker unique syntactic structure speaker unique textual style contain linguistic information voiceprint use ad hoc analysis explore speaker text style construct syntax aware model leverage linguistic information pos tagging alleviate issue inherently distinguish utterance individual speaker approach allow datum model centric investigation also employ multi task learning syntax aware information dialogue summarization good knowledge approach first method apply multi task learning dialogue summarization task experiment samsum corpus large scale dialogue summarization corpus demonstrate method improve upon vanilla model consequently find effort syntax aware approach reflect model\n",
      "\n",
      "Data 3:\n",
      "Raw       : The electromagnetic (EM) properties of two-component mixtures involving many disordered regularly and irregularly shaped crystals are studied. The effective relative permittivities are calculated utilizing the time-domain finite integration technique. The effective permittivity of disordered mixtures deviates from established mixing theories especially in cases of high permittivity contrast between inclusions and matrix material, and is strongly correlated to the cross-sectional area of the inclusion crystals. Electric energy density localizes at the edges and corners of inclusions in a manner independent of inclusion shape and influenced by EM propagation direction and surrounding inclusions. For mixtures with both disordered irregular and more organized cube inclusions, energy localization increases as the EM signal travels through the mixture before decreasing due to attenuation of the propagating EM signal. With a large number of inclusion crystals (here in the hundreds), it is found that the impact on effective permittivity from differences in individual inclusion shapes is negligible.\n",
      "Cleaned   : electromagnetic em property two component mixture involve many disorder regularly irregularly shape crystal study effective relative permittivitie calculate utilize time domain finite integration technique effective permittivity disorder mixture deviate established mix theory especially case high permittivity contrast inclusion matrix material strongly correlate cross sectional area inclusion crystal electric energy density localize edge corner inclusion manner independent inclusion shape influence em propagation direction surround inclusion mixture disorder irregular organized cube inclusion energy localization increase em signal travel mixture decrease due attenuation propagate em signal large number inclusion crystal hundred find impact effective permittivity difference individual inclusion shape negligible\n",
      "\n",
      "Data 4:\n",
      "Raw       : To fulfil the tight area and memory constraints in IoT applications, the design of efficient Convolutional Neural Network (CNN) hardware becomes crucial. Quantization of CNN is one of the promising approach that allows the compression of large CNN into a much smaller one, which is very suitable for IoT applications. Among various proposed quantization schemes, Power-of-two (PoT) quantization enables efficient hardware implementation and small memory consumption for CNN accelerators, but requires retraining of CNN to retain its accuracy. This paper proposes a two-level post-training static quantization technique (DoubleQ) that combines the 8-bit and PoT weight quantization. The CNN weight is first quantized to 8-bit (level one), then further quantized to PoT (level two). This allows multiplication to be carried out using shifters, by expressing the weights in their PoT exponent form. DoubleQ also reduces the memory storage requirement for CNN, as only the exponent of the weights is needed for storage. However, DoubleQ trades the accuracy of the network for reduced memory storage. To recover the accuracy, a selection process (DoubleQExt) was proposed to strategically select some of the less informative layers in the network to be quantized with PoT at the second level. On ResNet-20, the proposed DoubleQ can reduce the memory consumption by 37.50% with 7.28% accuracy degradation compared to 8-bit quantization. By applying DoubleQExt, the accuracy is only degraded by 1.19% compared to 8-bit version while achieving a memory reduction of 23.05%. This result is also 1% more accurate than the state-of-the-art work (SegLog). The proposed DoubleQExt also allows flexible configuration to trade off the memory consumption with better accuracy, which is not found in the other state-of-the-art works. With the proposed two-level weight quantization, one can achieve a more efficient hardware architecture for CNN with minimal impact to the accuracy, which is crucial for IoT applicati...\n",
      "Cleaned   : fulfil tight area memory constraint iot application design efficient convolutional neural network cnn hardware become crucial quantization cnn one promising approach allow compression large cnn much small one suitable iot application among various propose quantization scheme power two pot quantization enable efficient hardware implementation small memory consumption cnn accelerator require retrain cnn retain accuracy paper propose two level post training static quantization technique doubleq combine bit pot weight quantization cnn weight first quantize bit level one far quantize pot level two allow multiplication carry use shifter express weight pot exponent form doubleq also reduce memory storage requirement cnn exponent weight need storage however doubleq trade accuracy network reduce memory storage recover accuracy selection process doubleqext propose strategically select less informative layer network quantize pot second level propose doubleq reduce memory consumption accuracy degradation compare bit quantization apply doubleqext accuracy degrade compare bit version achieve memory reduction result also accurate state art work seglog propose doubleqext also allow flexible configuration trade memory consumption well accuracy find state art work propose two level weight quantization one achieve efficient hardware architecture cnn minimal impact accuracy crucial iot applicati\n",
      "\n",
      "Data 5:\n",
      "Raw       : Enterprises exist in a competitive manufacturing environment. To reduce production costs and effectively use production capacity to improve competitiveness, a hybrid production system is necessary. The flexible job shop (FJS) is a hybrid production system, and the FJS problem (FJSP) has drawn considerable attention in the past few decades. This paper examined the FJSP and, like previous studies, aimed to minimize the total order completion time (makespan). We developed a novel method that involves encoding feasible solutions in the genes of the initial chromosomes of a genetic algorithm (GA) and embedding the Taguchi method behind mating to increase the effectiveness of the GA. Two numerical experiments were conducted for evaluating the performance of the proposed algorithm relative to that of the Brandimarte MK1â€“MK10 benchmarks. The first experiment involved comparing the proposed algorithm and the traditional GA. The second experiment entailed comparing the proposed algorithm with those presented in previous studies. The results demonstrate that the proposed algorithm is superior to those reported in previous studies (except for that of Zhang et al.: the results in experiment MK7 were superior to those of Zhang, the results in experiments MK6 and MK10 were slightly inferior to those of Zhang, and the results were equivalent in other experiments) and effectively overcomes the encoding problem that occurs when a GA is used to solve the FJSP.\n",
      "Cleaned   : enterprise exist competitive manufacturing environment reduce production cost effectively use production capacity improve competitiveness hybrid production system necessary flexible job shop fjs hybrid production system fjs problem fjsp draw considerable attention past decade paper examine fjsp like previous study aim minimize total order completion time makespan develop novel method involve encode feasible solution gene initial chromosome genetic algorithm ga embed taguchi method behind mate increase effectiveness ga two numerical experiment conduct evaluate performance propose algorithm relative brandimarte benchmark first experiment involve compare propose algorithm traditional ga second experiment entail compare propose algorithm present previous study result demonstrate propose algorithm superior report previous study except zhang et al result experiment superior zhang result experiment slightly inferior zhang result equivalent experiment effectively overcome encoding problem occur ga use solve fjsp\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Before and After Preprocessing ---\")\n",
    "for i, row in df.head(5).iterrows():\n",
    "    print(f\"\\nData {i+1}:\")\n",
    "    print(\"Raw       :\", row['abstract'])\n",
    "    print(\"Cleaned   :\", row['cleaned_abstract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40ff1cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing finished, saved to: ./../../data/cleaned/cleaned_abstracts_lemmatization.json\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(os.path.dirname(clean_path), exist_ok=True)\n",
    "df[['cleaned_abstract']].to_json(clean_path, orient=\"records\", lines=False, indent=2)\n",
    "\n",
    "print(\"Preprocessing finished, saved to:\", clean_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a9f871",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
