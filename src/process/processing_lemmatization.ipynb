{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d58021fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from pathlib import Path\n",
    "import random\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "import math\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4474a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Folder input (preprocessed per tahun) & folder output\n",
    "INPUT_DIR = Path('./../../data/preprocessing/lemmatization')\n",
    "OUTPUT_DIR = Path('./../../output/lda_lemmatization')\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1738904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Inisialisasi Gibbs Sampling LDA (manual)\n",
    "def lda_gibbs(docs, K=3, alpha=0.1, beta=0.01, iterations=500):\n",
    "    # Bangun vocab & mapping\n",
    "    vocab   = sorted({w for doc in docs for w in doc})\n",
    "    W       = len(vocab)\n",
    "    word2id = {w:i for i,w in enumerate(vocab)}\n",
    "    D       = len(docs)\n",
    "\n",
    "    # Hitung prior + init matriks count\n",
    "    doc_topic  = np.zeros((D,K), dtype=int)   # Nd,k\n",
    "    topic_word = np.zeros((K,W), dtype=int)   # Nk,w\n",
    "    topic_sum  = np.zeros(K, dtype=int)       # Nk,.\n",
    "    Z          = []                           # assignments\n",
    "\n",
    "    # Inisialisasi acak\n",
    "    for d, doc in enumerate(docs):\n",
    "        z_d = []\n",
    "        for w in doc:\n",
    "            t = random.randrange(K)\n",
    "            z_d.append(t)\n",
    "            doc_topic[d,t]    += 1\n",
    "            topic_word[t,word2id[w]] += 1\n",
    "            topic_sum[t]      += 1\n",
    "        Z.append(z_d)\n",
    "\n",
    "    # Iterasi Gibbs Sampling\n",
    "    for it in range(iterations):\n",
    "        for d, doc in enumerate(docs):\n",
    "            for i, w in enumerate(doc):\n",
    "                t_old = Z[d][i]\n",
    "                wid   = word2id[w]\n",
    "\n",
    "                # decrement\n",
    "                doc_topic[d,t_old]      -= 1\n",
    "                topic_word[t_old,wid]   -= 1\n",
    "                topic_sum[t_old]        -= 1\n",
    "\n",
    "                # hitung p(z = k)\n",
    "                p_z = (doc_topic[d] + alpha) * \\\n",
    "                      (topic_word[:,wid] + beta) / \\\n",
    "                      (topic_sum + beta*W)\n",
    "\n",
    "                p_z = p_z / p_z.sum()\n",
    "                t_new = np.random.choice(K, p=p_z)\n",
    "\n",
    "                # increment\n",
    "                Z[d][i]                   = t_new\n",
    "                doc_topic[d,t_new]       += 1\n",
    "                topic_word[t_new,wid]    += 1\n",
    "                topic_sum[t_new]         += 1\n",
    "\n",
    "    # Ekstrak top-10 kata per topik\n",
    "    top_words = {\n",
    "        k: [vocab[i] for i in topic_word[k].argsort()[-10:][::-1]]\n",
    "        for k in range(K)\n",
    "    }\n",
    "    return top_words, vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d13b955d-0454-4030-935d-fb826e0596e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence(top_words, docs, epsilon=1e-12):\n",
    "    \"\"\"\n",
    "    Menghitung coherence score (UMass-like) secara manual.\n",
    "    top_words : dict of {topic_id: [word1, word2, ..., wordN]}\n",
    "    docs      : list of list of tokens\n",
    "    \"\"\"\n",
    "    # Hitung dokumen yang mengandung setiap kata\n",
    "    doc_freq = defaultdict(int)\n",
    "    for doc in docs:\n",
    "        unique_words = set(doc)\n",
    "        for word in unique_words:\n",
    "            doc_freq[word] += 1\n",
    "\n",
    "    # Hitung coherence tiap topik\n",
    "    coherence_scores = []\n",
    "\n",
    "    for topic, words in top_words.items():\n",
    "        score = 0.0\n",
    "        for i in range(1, len(words)):\n",
    "            for j in range(i):\n",
    "                w_i = words[i]\n",
    "                w_j = words[j]\n",
    "                # Hitung co-occurence di dokumen\n",
    "                D_wi = doc_freq.get(w_i, 0)\n",
    "                D_wi_wj = 0\n",
    "                for doc in docs:\n",
    "                    if w_i in doc and w_j in doc:\n",
    "                        D_wi_wj += 1\n",
    "\n",
    "                # UMass-like: log( (D(w_i, w_j) + epsilon) / D(w_j) )\n",
    "                score += math.log((D_wi_wj + epsilon) / (doc_freq.get(w_j, 1)))\n",
    "        coherence_scores.append(score)\n",
    "\n",
    "    # Rata-rata antar topik\n",
    "    return sum(coherence_scores) / len(coherence_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82b05e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "→ Memproses tahun 2013\n",
      "✔️ Coherence Score (UMass-like) untuk 2013: -112.1953\n",
      "⏱️ Waktu proses 2013: 18.73 detik\n",
      "\n",
      "→ Memproses tahun 2014\n",
      "✔️ Coherence Score (UMass-like) untuk 2014: -105.5897\n",
      "⏱️ Waktu proses 2014: 38.78 detik\n",
      "\n",
      "→ Memproses tahun 2015\n",
      "✔️ Coherence Score (UMass-like) untuk 2015: -77.1048\n",
      "⏱️ Waktu proses 2015: 75.01 detik\n",
      "\n",
      "→ Memproses tahun 2016\n",
      "✔️ Coherence Score (UMass-like) untuk 2016: -55.7656\n",
      "⏱️ Waktu proses 2016: 173.25 detik\n",
      "\n",
      "→ Memproses tahun 2017\n",
      "✔️ Coherence Score (UMass-like) untuk 2017: -48.7339\n",
      "⏱️ Waktu proses 2017: 907.14 detik\n",
      "\n",
      "→ Memproses tahun 2018\n",
      "✔️ Coherence Score (UMass-like) untuk 2018: -48.5206\n",
      "⏱️ Waktu proses 2018: 2086.15 detik\n",
      "\n",
      "→ Memproses tahun 2019\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 23\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     21\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 23\u001b[0m top_words, vocab \u001b[38;5;241m=\u001b[39m \u001b[43mlda_gibbs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m coherence \u001b[38;5;241m=\u001b[39m compute_coherence(top_words, docs)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✔️ Coherence Score (UMass-like) untuk \u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcoherence\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 44\u001b[0m, in \u001b[0;36mlda_gibbs\u001b[0;34m(docs, K, alpha, beta, iterations)\u001b[0m\n\u001b[1;32m     39\u001b[0m p_z \u001b[38;5;241m=\u001b[39m (doc_topic[d] \u001b[38;5;241m+\u001b[39m alpha) \u001b[38;5;241m*\u001b[39m \\\n\u001b[1;32m     40\u001b[0m       (topic_word[:,wid] \u001b[38;5;241m+\u001b[39m beta) \u001b[38;5;241m/\u001b[39m \\\n\u001b[1;32m     41\u001b[0m       (topic_sum \u001b[38;5;241m+\u001b[39m beta\u001b[38;5;241m*\u001b[39mW)\n\u001b[1;32m     43\u001b[0m p_z \u001b[38;5;241m=\u001b[39m p_z \u001b[38;5;241m/\u001b[39m p_z\u001b[38;5;241m.\u001b[39msum()\n\u001b[0;32m---> 44\u001b[0m t_new \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mp_z\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# increment\u001b[39;00m\n\u001b[1;32m     47\u001b[0m Z[d][i]                   \u001b[38;5;241m=\u001b[39m t_new\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 3. Loop per file JSON (per tahun)\n",
    "for path in sorted(INPUT_DIR.glob(\"preprocessed_abstracts_lemmatization_*.json\")):\n",
    "    year = path.stem.split(\"_\")[-1]\n",
    "    out_year_dir = OUTPUT_DIR / year\n",
    "    out_year_dir.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"\\n→ Memproses tahun {year}\")\n",
    "\n",
    "    # Baca list dict, ambil saja abstrak (string)\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        records = json.load(f)\n",
    "\n",
    "    # Tokenisasi: setiap string split on whitespace\n",
    "    docs = [ rec[\"abstract\"].split() \n",
    "             for rec in records \n",
    "             if rec.get(\"abstract\",\"\").strip() ]\n",
    "\n",
    "    if not docs:\n",
    "        print(f\"  ⚠️  Tidak ada abstrak valid untuk {year}, dilewati.\")\n",
    "        continue\n",
    "        \n",
    "    start_time = time.time()\n",
    "    \n",
    "    top_words, vocab = lda_gibbs(docs, K=3, iterations=100)\n",
    "\n",
    "    coherence = compute_coherence(top_words, docs)\n",
    "    print(f\"✔️ Coherence Score (UMass-like) untuk {year}: {coherence:.4f}\")\n",
    "\n",
    "    # 4a. Bar chart per topik\n",
    "    for k, words in top_words.items():\n",
    "        freqs = [sum(doc.count(w) for doc in docs) for w in words]\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.barh(words[::-1], freqs[::-1])\n",
    "        plt.title(f\"[{year}] Topik #{k+1}\")\n",
    "        plt.xlabel(\"Frekuensi\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(out_year_dir / f\"bar_topic{(k+1):02d}_{year}.png\")\n",
    "        plt.close()\n",
    "    \n",
    "    # 4b. Word Cloud per topik\n",
    "        wc = WordCloud(width=800, height=400, background_color='white')\n",
    "        wc.generate_from_frequencies(dict(zip(words, freqs)))\n",
    "        wc.to_file(out_year_dir / f\"wordcloud_topic{(k+1):02d}_{year}.png\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"⏱️ Waktu proses {year}: {end_time - start_time:.2f} detik\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93072be4-fca4-4150-b760-c8e5db770d7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9342dd-b4d8-4fb2-9892-9b550527e8fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (envtextminning)",
   "language": "python",
   "name": "envtextminning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
